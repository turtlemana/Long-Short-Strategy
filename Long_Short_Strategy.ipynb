{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Long Short Strategy",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMlPiEri/SXzqCnK9tpM08A",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/turtlemana/Long-Short-Strategy/blob/main/Long_Short_Strategy.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Long Short Strategy, using MLP and RandomForestClassifier\n",
        "\n",
        "\n",
        "\n",
        "*   Applying Random Search CV, or Grid CV to find optimal hyperparameter\n",
        "*   Using crsp_m_2000.csv CRSP file \n",
        "\n",
        "\n",
        "*   Training data: before 2010\n",
        "*   Testing data: after 2010\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "hh7eeri8OLzY"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 224
        },
        "id": "pM7Z74hC7Frx",
        "outputId": "7d890257-5c2f-43ed-a283-22233ae47d3b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "         date  permno     prc       ret  shrout     vol\n",
              "0  2000-01-31   80000  16.750 -0.007407  3532.0  1547.0\n",
              "1  2000-02-29   80000  16.375 -0.022388  3551.0  2394.0\n",
              "2  2000-03-31   80000  14.750 -0.099237  3558.0  2430.0\n",
              "3  2000-04-28   80000  14.250 -0.033898  3558.0  2122.0\n",
              "4  2000-05-31   80000  15.375  0.078947  3577.0  1841.0"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-01f81a3e-b9e9-4c3f-a014-63cd2b84884b\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>date</th>\n",
              "      <th>permno</th>\n",
              "      <th>prc</th>\n",
              "      <th>ret</th>\n",
              "      <th>shrout</th>\n",
              "      <th>vol</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2000-01-31</td>\n",
              "      <td>80000</td>\n",
              "      <td>16.750</td>\n",
              "      <td>-0.007407</td>\n",
              "      <td>3532.0</td>\n",
              "      <td>1547.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2000-02-29</td>\n",
              "      <td>80000</td>\n",
              "      <td>16.375</td>\n",
              "      <td>-0.022388</td>\n",
              "      <td>3551.0</td>\n",
              "      <td>2394.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2000-03-31</td>\n",
              "      <td>80000</td>\n",
              "      <td>14.750</td>\n",
              "      <td>-0.099237</td>\n",
              "      <td>3558.0</td>\n",
              "      <td>2430.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2000-04-28</td>\n",
              "      <td>80000</td>\n",
              "      <td>14.250</td>\n",
              "      <td>-0.033898</td>\n",
              "      <td>3558.0</td>\n",
              "      <td>2122.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2000-05-31</td>\n",
              "      <td>80000</td>\n",
              "      <td>15.375</td>\n",
              "      <td>0.078947</td>\n",
              "      <td>3577.0</td>\n",
              "      <td>1841.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-01f81a3e-b9e9-4c3f-a014-63cd2b84884b')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-01f81a3e-b9e9-4c3f-a014-63cd2b84884b button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-01f81a3e-b9e9-4c3f-a014-63cd2b84884b');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "crsp=pd.read_csv(\"/content/gdrive/My Drive/crsp_m_2000.csv\")\n",
        "\n",
        "crsp.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preprocessing\n",
        "\n",
        "\n",
        "\n",
        "*   Generating variable that which trading volumne relative to shares outstanding\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "*   Inspecting the data's description\n",
        "*   Dropping the missing value\n",
        "\n",
        "\n",
        "\n",
        "*   Winsorizing 1%\n",
        "\n"
      ],
      "metadata": {
        "id": "f6w2_85F9Xza"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "crsp['relvol'] = 0.1*crsp.vol/crsp.shrout # trading vol relative to shares outstanding\n",
        "crsp['date'] = pd.to_datetime(crsp.date) # string to Timestamp\n",
        "crsp['prc'] = crsp.prc.abs() # abs(prc)"
      ],
      "metadata": {
        "id": "IRnft5Ec7Q0B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "crsp.info()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XXx5UHUA9af4",
        "outputId": "137d2c57-65a7-4f1a-fe19-31014efbbfe0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 609247 entries, 0 to 609246\n",
            "Data columns (total 7 columns):\n",
            " #   Column  Non-Null Count   Dtype         \n",
            "---  ------  --------------   -----         \n",
            " 0   date    609247 non-null  datetime64[ns]\n",
            " 1   permno  609247 non-null  int64         \n",
            " 2   prc     593733 non-null  float64       \n",
            " 3   ret     595044 non-null  float64       \n",
            " 4   shrout  609247 non-null  float64       \n",
            " 5   vol     598363 non-null  float64       \n",
            " 6   relvol  598363 non-null  float64       \n",
            "dtypes: datetime64[ns](1), float64(5), int64(1)\n",
            "memory usage: 32.5 MB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "crsp.describe()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 300
        },
        "id": "FiU4P2Tr9lqr",
        "outputId": "8a4ce1ec-6c93-4564-db0d-d96fd30acc77"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "              permno            prc            ret        shrout  \\\n",
              "count  609247.000000  593733.000000  595044.000000  6.092470e+05   \n",
              "mean    87076.846204      20.088695       0.006790  6.777919e+04   \n",
              "std      3584.821611      55.504162       0.208501  1.761328e+05   \n",
              "min     80000.000000       0.007800      -1.000000  2.000000e+00   \n",
              "25%     84229.000000       3.950000      -0.077211  1.177700e+04   \n",
              "50%     87236.000000      11.030000       0.000000  2.705700e+04   \n",
              "75%     90081.000000      24.062500       0.073465  5.769050e+04   \n",
              "max     93436.000000    4736.000000      15.774193  6.433649e+06   \n",
              "\n",
              "                vol         relvol  \n",
              "count  5.983630e+05  598363.000000  \n",
              "mean   1.328973e+05       0.168159  \n",
              "std    4.585374e+05       0.747183  \n",
              "min    0.000000e+00       0.000000  \n",
              "25%    4.859000e+03       0.032650  \n",
              "50%    2.703100e+04       0.090839  \n",
              "75%    9.896500e+04       0.196360  \n",
              "max    3.875707e+07     262.246178  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-f3462f52-5c76-4b07-af04-d3430bd9d07b\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>permno</th>\n",
              "      <th>prc</th>\n",
              "      <th>ret</th>\n",
              "      <th>shrout</th>\n",
              "      <th>vol</th>\n",
              "      <th>relvol</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>609247.000000</td>\n",
              "      <td>593733.000000</td>\n",
              "      <td>595044.000000</td>\n",
              "      <td>6.092470e+05</td>\n",
              "      <td>5.983630e+05</td>\n",
              "      <td>598363.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>87076.846204</td>\n",
              "      <td>20.088695</td>\n",
              "      <td>0.006790</td>\n",
              "      <td>6.777919e+04</td>\n",
              "      <td>1.328973e+05</td>\n",
              "      <td>0.168159</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>3584.821611</td>\n",
              "      <td>55.504162</td>\n",
              "      <td>0.208501</td>\n",
              "      <td>1.761328e+05</td>\n",
              "      <td>4.585374e+05</td>\n",
              "      <td>0.747183</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>80000.000000</td>\n",
              "      <td>0.007800</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>2.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>84229.000000</td>\n",
              "      <td>3.950000</td>\n",
              "      <td>-0.077211</td>\n",
              "      <td>1.177700e+04</td>\n",
              "      <td>4.859000e+03</td>\n",
              "      <td>0.032650</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>87236.000000</td>\n",
              "      <td>11.030000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>2.705700e+04</td>\n",
              "      <td>2.703100e+04</td>\n",
              "      <td>0.090839</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>90081.000000</td>\n",
              "      <td>24.062500</td>\n",
              "      <td>0.073465</td>\n",
              "      <td>5.769050e+04</td>\n",
              "      <td>9.896500e+04</td>\n",
              "      <td>0.196360</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>93436.000000</td>\n",
              "      <td>4736.000000</td>\n",
              "      <td>15.774193</td>\n",
              "      <td>6.433649e+06</td>\n",
              "      <td>3.875707e+07</td>\n",
              "      <td>262.246178</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-f3462f52-5c76-4b07-af04-d3430bd9d07b')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-f3462f52-5c76-4b07-af04-d3430bd9d07b button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-f3462f52-5c76-4b07-af04-d3430bd9d07b');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "crsp=crsp.dropna()\n",
        "len(crsp)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8UBS1oPe9o8W",
        "outputId": "c76a688a-9852-4bb8-ed74-a250bfcda2ce"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "590656"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lb = crsp.quantile(q = 0.01)\n",
        "ub = crsp.quantile(q = 0.99)\n",
        "\n",
        "crsp = crsp[crsp.ret >= lb.ret]\n",
        "crsp = crsp[crsp.ret <= ub.ret]"
      ],
      "metadata": {
        "id": "TJi46SNi9vVe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(crsp)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aqyPKGXa96rD",
        "outputId": "738389eb-9533-4154-8810-17d4187821c1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "578864"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Generating Target Label\n",
        "\n",
        "\n",
        "\n",
        "*  Dividing 10 bundles of stocks for each date,\n",
        "*  Sorted by previous return\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Q48gvSqh-5p3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "crsp['label2'] = pd.concat([pd.qcut(-g, 10, labels=False) for k, g in crsp.groupby('date')['ret']])\n",
        "\n",
        "y = [i**2 for i in range(10)]\n"
      ],
      "metadata": {
        "id": "zr1mdT0I975V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "crsp.sort_values(['permno','date'], inplace=True)\n",
        "crsp['label'] = crsp.groupby('date')['ret'].apply(lambda x: pd.qcut(-x, 10, labels=False)) \n",
        "crsp['tgt_ret'] = crsp.groupby('permno')['ret'].shift(-1) \n",
        "crsp['tgt_label'] = crsp.groupby('permno')['label'].shift(-1) \n",
        "crsp.dropna(inplace=True) "
      ],
      "metadata": {
        "id": "7FJzt2Ce-8Oa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "crsp.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "7qEW_Zjd_UYe",
        "outputId": "ea01a215-4ae2-48ec-95ec-57bbf9d27747"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "        date  permno     prc       ret  shrout     vol    relvol  label2  \\\n",
              "0 2000-01-31   80000  16.750 -0.007407  3532.0  1547.0  0.043800       4   \n",
              "1 2000-02-29   80000  16.375 -0.022388  3551.0  2394.0  0.067418       6   \n",
              "2 2000-03-31   80000  14.750 -0.099237  3558.0  2430.0  0.068297       6   \n",
              "3 2000-04-28   80000  14.250 -0.033898  3558.0  2122.0  0.059640       3   \n",
              "4 2000-05-31   80000  15.375  0.078947  3577.0  1841.0  0.051468       1   \n",
              "\n",
              "   label   tgt_ret  tgt_label  \n",
              "0      4 -0.022388        6.0  \n",
              "1      6 -0.099237        6.0  \n",
              "2      6 -0.033898        3.0  \n",
              "3      3  0.078947        1.0  \n",
              "4      1  0.097561        3.0  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-9f8b5e80-1ecb-42e5-a879-c99ee7d49617\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>date</th>\n",
              "      <th>permno</th>\n",
              "      <th>prc</th>\n",
              "      <th>ret</th>\n",
              "      <th>shrout</th>\n",
              "      <th>vol</th>\n",
              "      <th>relvol</th>\n",
              "      <th>label2</th>\n",
              "      <th>label</th>\n",
              "      <th>tgt_ret</th>\n",
              "      <th>tgt_label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2000-01-31</td>\n",
              "      <td>80000</td>\n",
              "      <td>16.750</td>\n",
              "      <td>-0.007407</td>\n",
              "      <td>3532.0</td>\n",
              "      <td>1547.0</td>\n",
              "      <td>0.043800</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>-0.022388</td>\n",
              "      <td>6.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2000-02-29</td>\n",
              "      <td>80000</td>\n",
              "      <td>16.375</td>\n",
              "      <td>-0.022388</td>\n",
              "      <td>3551.0</td>\n",
              "      <td>2394.0</td>\n",
              "      <td>0.067418</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>-0.099237</td>\n",
              "      <td>6.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2000-03-31</td>\n",
              "      <td>80000</td>\n",
              "      <td>14.750</td>\n",
              "      <td>-0.099237</td>\n",
              "      <td>3558.0</td>\n",
              "      <td>2430.0</td>\n",
              "      <td>0.068297</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>-0.033898</td>\n",
              "      <td>3.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2000-04-28</td>\n",
              "      <td>80000</td>\n",
              "      <td>14.250</td>\n",
              "      <td>-0.033898</td>\n",
              "      <td>3558.0</td>\n",
              "      <td>2122.0</td>\n",
              "      <td>0.059640</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>0.078947</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2000-05-31</td>\n",
              "      <td>80000</td>\n",
              "      <td>15.375</td>\n",
              "      <td>0.078947</td>\n",
              "      <td>3577.0</td>\n",
              "      <td>1841.0</td>\n",
              "      <td>0.051468</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0.097561</td>\n",
              "      <td>3.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-9f8b5e80-1ecb-42e5-a879-c99ee7d49617')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-9f8b5e80-1ecb-42e5-a879-c99ee7d49617 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-9f8b5e80-1ecb-42e5-a879-c99ee7d49617');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Creating momentum variables\n",
        "\n",
        "\n",
        "*   Considering size by using prc and shrout features\n",
        "*   generating 3, 6, 12 months momentum variable using cumulative return\n",
        "\n"
      ],
      "metadata": {
        "id": "QoiFd98p_ooL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create features\n",
        "# add size\n",
        "import numpy as np\n",
        "crsp['size'] = np.log(crsp.prc * crsp.shrout)\n",
        "\n",
        "# add momentum factors\n",
        "# cumulative return\n",
        "crsp['cumret'] = crsp.groupby('permno')['ret'].apply(lambda x: (1+x).cumprod())\n",
        "gb = crsp.groupby('permno')\n",
        "numer = gb.shift(1)\n",
        "\n",
        "# 3-month momentum\n",
        "denom = gb.shift(3)\n",
        "crsp['mom3m'] = numer.cumret/denom.cumret - 1\n",
        "crsp['prd3m'] = 12*(numer.date.dt.year - denom.date.dt.year) \\\n",
        "                    + (numer.date.dt.month - denom.date.dt.month) + 1 # number of actual months\n",
        "\n",
        "# 6-month momentum\n",
        "denom = gb.shift(6)\n",
        "crsp['mom6m'] = numer.cumret/denom.cumret - 1\n",
        "crsp['prd6m'] = 12*(numer.date.dt.year - denom.date.dt.year) \\\n",
        "                    + (numer.date.dt.month - denom.date.dt.month) + 1 # number of actual months\n",
        "\n",
        "# 12-month momentum\n",
        "denom = gb.shift(12)\n",
        "crsp['mom12m'] = numer.cumret/denom.cumret - 1\n",
        "crsp['prd12m'] = 12*(numer.date.dt.year - denom.date.dt.year) \\\n",
        "                    + (numer.date.dt.month - denom.date.dt.month) + 1 # number of actual months\n",
        "\n",
        "crsp"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 661
        },
        "id": "1dI94AoVBoau",
        "outputId": "0e4936b4-544e-4c5c-c02b-2956bcbbde48"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "             date  permno         prc       ret    shrout        vol  \\\n",
              "0      2000-01-31   80000   16.750000 -0.007407    3532.0     1547.0   \n",
              "1      2000-02-29   80000   16.375000 -0.022388    3551.0     2394.0   \n",
              "2      2000-03-31   80000   14.750000 -0.099237    3558.0     2430.0   \n",
              "3      2000-04-28   80000   14.250000 -0.033898    3558.0     2122.0   \n",
              "4      2000-05-31   80000   15.375000  0.078947    3577.0     1841.0   \n",
              "...           ...     ...         ...       ...       ...        ...   \n",
              "609241 2018-07-31   93436  298.140015 -0.130660  170593.0  1723953.0   \n",
              "609242 2018-08-31   93436  301.660004  0.011806  170593.0  2773316.0   \n",
              "609243 2018-09-28   93436  264.769989 -0.122290  171578.0  1960767.0   \n",
              "609244 2018-10-31   93436  337.320007  0.274011  171733.0  2864231.0   \n",
              "609245 2018-11-30   93436  350.480011  0.039013  171733.0  1331285.0   \n",
              "\n",
              "          relvol  label2  label   tgt_ret  tgt_label       size    cumret  \\\n",
              "0       0.043800       4      4 -0.022388        6.0  10.988018  0.992593   \n",
              "1       0.067418       6      6 -0.099237        6.0  10.970740  0.970370   \n",
              "2       0.068297       6      6 -0.033898        3.0  10.868197  0.874074   \n",
              "3       0.059640       3      3  0.078947        1.0  10.833711  0.844444   \n",
              "4       0.051468       1      1  0.097561        3.0  10.915023  0.911111   \n",
              "...          ...     ...    ...       ...        ...        ...       ...   \n",
              "609241  1.010565       9      9  0.011806        5.0  17.744599  6.909528   \n",
              "609242  1.625692       5      5 -0.122290        8.0  17.756336  6.991105   \n",
              "609243  1.142785       8      8  0.274011        0.0  17.631655  6.136163   \n",
              "609244  1.667840       0      0  0.039013        3.0  17.874728  7.817542   \n",
              "609245  0.775206       3      3 -0.050445        2.0  17.913000  8.122531   \n",
              "\n",
              "           mom3m  prd3m     mom6m  prd6m    mom12m  prd12m  \n",
              "0            NaN    NaN       NaN    NaN       NaN     NaN  \n",
              "1            NaN    NaN       NaN    NaN       NaN     NaN  \n",
              "2            NaN    NaN       NaN    NaN       NaN     NaN  \n",
              "3      -0.119403    3.0       NaN    NaN       NaN     NaN  \n",
              "4      -0.129771    3.0       NaN    NaN       NaN     NaN  \n",
              "...          ...    ...       ...    ...       ...     ...  \n",
              "609241  0.166894    3.0 -0.032062    6.0  0.060222    12.0  \n",
              "609242  0.047097    3.0 -0.130939    6.0 -0.162293    12.0  \n",
              "609243 -0.120397    3.0  0.133506    6.0 -0.115626    12.0  \n",
              "609244 -0.111927    3.0 -0.099115    6.0 -0.201369    12.0  \n",
              "609245  0.118213    3.0  0.184701    6.0  0.092181    12.0  \n",
              "\n",
              "[572412 rows x 19 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-9096a5d4-b4c1-4a51-9531-dfa076ab3c40\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>date</th>\n",
              "      <th>permno</th>\n",
              "      <th>prc</th>\n",
              "      <th>ret</th>\n",
              "      <th>shrout</th>\n",
              "      <th>vol</th>\n",
              "      <th>relvol</th>\n",
              "      <th>label2</th>\n",
              "      <th>label</th>\n",
              "      <th>tgt_ret</th>\n",
              "      <th>tgt_label</th>\n",
              "      <th>size</th>\n",
              "      <th>cumret</th>\n",
              "      <th>mom3m</th>\n",
              "      <th>prd3m</th>\n",
              "      <th>mom6m</th>\n",
              "      <th>prd6m</th>\n",
              "      <th>mom12m</th>\n",
              "      <th>prd12m</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2000-01-31</td>\n",
              "      <td>80000</td>\n",
              "      <td>16.750000</td>\n",
              "      <td>-0.007407</td>\n",
              "      <td>3532.0</td>\n",
              "      <td>1547.0</td>\n",
              "      <td>0.043800</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>-0.022388</td>\n",
              "      <td>6.0</td>\n",
              "      <td>10.988018</td>\n",
              "      <td>0.992593</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2000-02-29</td>\n",
              "      <td>80000</td>\n",
              "      <td>16.375000</td>\n",
              "      <td>-0.022388</td>\n",
              "      <td>3551.0</td>\n",
              "      <td>2394.0</td>\n",
              "      <td>0.067418</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>-0.099237</td>\n",
              "      <td>6.0</td>\n",
              "      <td>10.970740</td>\n",
              "      <td>0.970370</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2000-03-31</td>\n",
              "      <td>80000</td>\n",
              "      <td>14.750000</td>\n",
              "      <td>-0.099237</td>\n",
              "      <td>3558.0</td>\n",
              "      <td>2430.0</td>\n",
              "      <td>0.068297</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>-0.033898</td>\n",
              "      <td>3.0</td>\n",
              "      <td>10.868197</td>\n",
              "      <td>0.874074</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2000-04-28</td>\n",
              "      <td>80000</td>\n",
              "      <td>14.250000</td>\n",
              "      <td>-0.033898</td>\n",
              "      <td>3558.0</td>\n",
              "      <td>2122.0</td>\n",
              "      <td>0.059640</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>0.078947</td>\n",
              "      <td>1.0</td>\n",
              "      <td>10.833711</td>\n",
              "      <td>0.844444</td>\n",
              "      <td>-0.119403</td>\n",
              "      <td>3.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2000-05-31</td>\n",
              "      <td>80000</td>\n",
              "      <td>15.375000</td>\n",
              "      <td>0.078947</td>\n",
              "      <td>3577.0</td>\n",
              "      <td>1841.0</td>\n",
              "      <td>0.051468</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0.097561</td>\n",
              "      <td>3.0</td>\n",
              "      <td>10.915023</td>\n",
              "      <td>0.911111</td>\n",
              "      <td>-0.129771</td>\n",
              "      <td>3.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>609241</th>\n",
              "      <td>2018-07-31</td>\n",
              "      <td>93436</td>\n",
              "      <td>298.140015</td>\n",
              "      <td>-0.130660</td>\n",
              "      <td>170593.0</td>\n",
              "      <td>1723953.0</td>\n",
              "      <td>1.010565</td>\n",
              "      <td>9</td>\n",
              "      <td>9</td>\n",
              "      <td>0.011806</td>\n",
              "      <td>5.0</td>\n",
              "      <td>17.744599</td>\n",
              "      <td>6.909528</td>\n",
              "      <td>0.166894</td>\n",
              "      <td>3.0</td>\n",
              "      <td>-0.032062</td>\n",
              "      <td>6.0</td>\n",
              "      <td>0.060222</td>\n",
              "      <td>12.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>609242</th>\n",
              "      <td>2018-08-31</td>\n",
              "      <td>93436</td>\n",
              "      <td>301.660004</td>\n",
              "      <td>0.011806</td>\n",
              "      <td>170593.0</td>\n",
              "      <td>2773316.0</td>\n",
              "      <td>1.625692</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>-0.122290</td>\n",
              "      <td>8.0</td>\n",
              "      <td>17.756336</td>\n",
              "      <td>6.991105</td>\n",
              "      <td>0.047097</td>\n",
              "      <td>3.0</td>\n",
              "      <td>-0.130939</td>\n",
              "      <td>6.0</td>\n",
              "      <td>-0.162293</td>\n",
              "      <td>12.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>609243</th>\n",
              "      <td>2018-09-28</td>\n",
              "      <td>93436</td>\n",
              "      <td>264.769989</td>\n",
              "      <td>-0.122290</td>\n",
              "      <td>171578.0</td>\n",
              "      <td>1960767.0</td>\n",
              "      <td>1.142785</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>0.274011</td>\n",
              "      <td>0.0</td>\n",
              "      <td>17.631655</td>\n",
              "      <td>6.136163</td>\n",
              "      <td>-0.120397</td>\n",
              "      <td>3.0</td>\n",
              "      <td>0.133506</td>\n",
              "      <td>6.0</td>\n",
              "      <td>-0.115626</td>\n",
              "      <td>12.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>609244</th>\n",
              "      <td>2018-10-31</td>\n",
              "      <td>93436</td>\n",
              "      <td>337.320007</td>\n",
              "      <td>0.274011</td>\n",
              "      <td>171733.0</td>\n",
              "      <td>2864231.0</td>\n",
              "      <td>1.667840</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.039013</td>\n",
              "      <td>3.0</td>\n",
              "      <td>17.874728</td>\n",
              "      <td>7.817542</td>\n",
              "      <td>-0.111927</td>\n",
              "      <td>3.0</td>\n",
              "      <td>-0.099115</td>\n",
              "      <td>6.0</td>\n",
              "      <td>-0.201369</td>\n",
              "      <td>12.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>609245</th>\n",
              "      <td>2018-11-30</td>\n",
              "      <td>93436</td>\n",
              "      <td>350.480011</td>\n",
              "      <td>0.039013</td>\n",
              "      <td>171733.0</td>\n",
              "      <td>1331285.0</td>\n",
              "      <td>0.775206</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>-0.050445</td>\n",
              "      <td>2.0</td>\n",
              "      <td>17.913000</td>\n",
              "      <td>8.122531</td>\n",
              "      <td>0.118213</td>\n",
              "      <td>3.0</td>\n",
              "      <td>0.184701</td>\n",
              "      <td>6.0</td>\n",
              "      <td>0.092181</td>\n",
              "      <td>12.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>572412 rows  19 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-9096a5d4-b4c1-4a51-9531-dfa076ab3c40')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-9096a5d4-b4c1-4a51-9531-dfa076ab3c40 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-9096a5d4-b4c1-4a51-9531-dfa076ab3c40');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# remove rows with missing months\n",
        "crsp = crsp[crsp.prd3m==3]\n",
        "crsp = crsp[crsp.prd6m==6]\n",
        "crsp = crsp[crsp.prd12m==12]\n",
        "\n",
        "crsp.drop(columns=['prd3m','prd6m','prd12m'], inplace=True)\n",
        "crsp"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 641
        },
        "id": "u1uteZbqDaID",
        "outputId": "e095ed26-ba69-45e4-aa24-e392057b5c27"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "             date  permno         prc       ret    shrout        vol  \\\n",
              "12     2001-01-31   80000   20.125000  0.025478    3564.0     1869.0   \n",
              "13     2001-02-28   80000   20.062500 -0.003106    3473.0      811.0   \n",
              "14     2001-03-30   80000   22.062500  0.099688    3466.0     2518.0   \n",
              "15     2001-04-30   80000   21.445000 -0.027989    3466.0     1936.0   \n",
              "16     2001-05-31   80000   21.660000  0.010026    3483.0      905.0   \n",
              "...           ...     ...         ...       ...       ...        ...   \n",
              "609241 2018-07-31   93436  298.140015 -0.130660  170593.0  1723953.0   \n",
              "609242 2018-08-31   93436  301.660004  0.011806  170593.0  2773316.0   \n",
              "609243 2018-09-28   93436  264.769989 -0.122290  171578.0  1960767.0   \n",
              "609244 2018-10-31   93436  337.320007  0.274011  171733.0  2864231.0   \n",
              "609245 2018-11-30   93436  350.480011  0.039013  171733.0  1331285.0   \n",
              "\n",
              "          relvol  label2  label   tgt_ret  tgt_label       size    cumret  \\\n",
              "12      0.052441       6      6 -0.003106        3.0  11.180602  1.192593   \n",
              "13      0.023352       3      3  0.099688        1.0  11.151626  1.188889   \n",
              "14      0.072649       1      1 -0.027989        6.0  11.244636  1.307407   \n",
              "15      0.055857       6      6  0.010026        5.0  11.216248  1.270815   \n",
              "16      0.025983       5      5 -0.021237        5.0  11.231117  1.283556   \n",
              "...          ...     ...    ...       ...        ...        ...       ...   \n",
              "609241  1.010565       9      9  0.011806        5.0  17.744599  6.909528   \n",
              "609242  1.625692       5      5 -0.122290        8.0  17.756336  6.991105   \n",
              "609243  1.142785       8      8  0.274011        0.0  17.631655  6.136163   \n",
              "609244  1.667840       0      0  0.039013        3.0  17.874728  7.817542   \n",
              "609245  0.775206       3      3 -0.050445        2.0  17.913000  8.122531   \n",
              "\n",
              "           mom3m     mom6m    mom12m  \n",
              "12      0.019481  0.226563  0.171642  \n",
              "13      0.080537  0.183824  0.229008  \n",
              "14      0.022293  0.163043  0.360169  \n",
              "15      0.096273  0.146104  0.548246  \n",
              "16      0.068910  0.151409  0.394797  \n",
              "...          ...       ...       ...  \n",
              "609241  0.166894 -0.032062  0.060222  \n",
              "609242  0.047097 -0.130939 -0.162293  \n",
              "609243 -0.120397  0.133506 -0.115626  \n",
              "609244 -0.111927 -0.099115 -0.201369  \n",
              "609245  0.118213  0.184701  0.092181  \n",
              "\n",
              "[443284 rows x 16 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-c19c7e86-93cc-45b6-a268-5a86084d3575\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>date</th>\n",
              "      <th>permno</th>\n",
              "      <th>prc</th>\n",
              "      <th>ret</th>\n",
              "      <th>shrout</th>\n",
              "      <th>vol</th>\n",
              "      <th>relvol</th>\n",
              "      <th>label2</th>\n",
              "      <th>label</th>\n",
              "      <th>tgt_ret</th>\n",
              "      <th>tgt_label</th>\n",
              "      <th>size</th>\n",
              "      <th>cumret</th>\n",
              "      <th>mom3m</th>\n",
              "      <th>mom6m</th>\n",
              "      <th>mom12m</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>2001-01-31</td>\n",
              "      <td>80000</td>\n",
              "      <td>20.125000</td>\n",
              "      <td>0.025478</td>\n",
              "      <td>3564.0</td>\n",
              "      <td>1869.0</td>\n",
              "      <td>0.052441</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>-0.003106</td>\n",
              "      <td>3.0</td>\n",
              "      <td>11.180602</td>\n",
              "      <td>1.192593</td>\n",
              "      <td>0.019481</td>\n",
              "      <td>0.226563</td>\n",
              "      <td>0.171642</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>2001-02-28</td>\n",
              "      <td>80000</td>\n",
              "      <td>20.062500</td>\n",
              "      <td>-0.003106</td>\n",
              "      <td>3473.0</td>\n",
              "      <td>811.0</td>\n",
              "      <td>0.023352</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>0.099688</td>\n",
              "      <td>1.0</td>\n",
              "      <td>11.151626</td>\n",
              "      <td>1.188889</td>\n",
              "      <td>0.080537</td>\n",
              "      <td>0.183824</td>\n",
              "      <td>0.229008</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>2001-03-30</td>\n",
              "      <td>80000</td>\n",
              "      <td>22.062500</td>\n",
              "      <td>0.099688</td>\n",
              "      <td>3466.0</td>\n",
              "      <td>2518.0</td>\n",
              "      <td>0.072649</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>-0.027989</td>\n",
              "      <td>6.0</td>\n",
              "      <td>11.244636</td>\n",
              "      <td>1.307407</td>\n",
              "      <td>0.022293</td>\n",
              "      <td>0.163043</td>\n",
              "      <td>0.360169</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>2001-04-30</td>\n",
              "      <td>80000</td>\n",
              "      <td>21.445000</td>\n",
              "      <td>-0.027989</td>\n",
              "      <td>3466.0</td>\n",
              "      <td>1936.0</td>\n",
              "      <td>0.055857</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>0.010026</td>\n",
              "      <td>5.0</td>\n",
              "      <td>11.216248</td>\n",
              "      <td>1.270815</td>\n",
              "      <td>0.096273</td>\n",
              "      <td>0.146104</td>\n",
              "      <td>0.548246</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>2001-05-31</td>\n",
              "      <td>80000</td>\n",
              "      <td>21.660000</td>\n",
              "      <td>0.010026</td>\n",
              "      <td>3483.0</td>\n",
              "      <td>905.0</td>\n",
              "      <td>0.025983</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>-0.021237</td>\n",
              "      <td>5.0</td>\n",
              "      <td>11.231117</td>\n",
              "      <td>1.283556</td>\n",
              "      <td>0.068910</td>\n",
              "      <td>0.151409</td>\n",
              "      <td>0.394797</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>609241</th>\n",
              "      <td>2018-07-31</td>\n",
              "      <td>93436</td>\n",
              "      <td>298.140015</td>\n",
              "      <td>-0.130660</td>\n",
              "      <td>170593.0</td>\n",
              "      <td>1723953.0</td>\n",
              "      <td>1.010565</td>\n",
              "      <td>9</td>\n",
              "      <td>9</td>\n",
              "      <td>0.011806</td>\n",
              "      <td>5.0</td>\n",
              "      <td>17.744599</td>\n",
              "      <td>6.909528</td>\n",
              "      <td>0.166894</td>\n",
              "      <td>-0.032062</td>\n",
              "      <td>0.060222</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>609242</th>\n",
              "      <td>2018-08-31</td>\n",
              "      <td>93436</td>\n",
              "      <td>301.660004</td>\n",
              "      <td>0.011806</td>\n",
              "      <td>170593.0</td>\n",
              "      <td>2773316.0</td>\n",
              "      <td>1.625692</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>-0.122290</td>\n",
              "      <td>8.0</td>\n",
              "      <td>17.756336</td>\n",
              "      <td>6.991105</td>\n",
              "      <td>0.047097</td>\n",
              "      <td>-0.130939</td>\n",
              "      <td>-0.162293</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>609243</th>\n",
              "      <td>2018-09-28</td>\n",
              "      <td>93436</td>\n",
              "      <td>264.769989</td>\n",
              "      <td>-0.122290</td>\n",
              "      <td>171578.0</td>\n",
              "      <td>1960767.0</td>\n",
              "      <td>1.142785</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>0.274011</td>\n",
              "      <td>0.0</td>\n",
              "      <td>17.631655</td>\n",
              "      <td>6.136163</td>\n",
              "      <td>-0.120397</td>\n",
              "      <td>0.133506</td>\n",
              "      <td>-0.115626</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>609244</th>\n",
              "      <td>2018-10-31</td>\n",
              "      <td>93436</td>\n",
              "      <td>337.320007</td>\n",
              "      <td>0.274011</td>\n",
              "      <td>171733.0</td>\n",
              "      <td>2864231.0</td>\n",
              "      <td>1.667840</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.039013</td>\n",
              "      <td>3.0</td>\n",
              "      <td>17.874728</td>\n",
              "      <td>7.817542</td>\n",
              "      <td>-0.111927</td>\n",
              "      <td>-0.099115</td>\n",
              "      <td>-0.201369</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>609245</th>\n",
              "      <td>2018-11-30</td>\n",
              "      <td>93436</td>\n",
              "      <td>350.480011</td>\n",
              "      <td>0.039013</td>\n",
              "      <td>171733.0</td>\n",
              "      <td>1331285.0</td>\n",
              "      <td>0.775206</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>-0.050445</td>\n",
              "      <td>2.0</td>\n",
              "      <td>17.913000</td>\n",
              "      <td>8.122531</td>\n",
              "      <td>0.118213</td>\n",
              "      <td>0.184701</td>\n",
              "      <td>0.092181</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>443284 rows  16 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-c19c7e86-93cc-45b6-a268-5a86084d3575')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-c19c7e86-93cc-45b6-a268-5a86084d3575 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-c19c7e86-93cc-45b6-a268-5a86084d3575');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "crsp[['tgt_ret', 'size', 'mom3m', 'mom6m', 'mom12m']].corr() # correlations of all pairs\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "hpg1GvSHD3Vg",
        "outputId": "e06b7d7c-6508-4bfa-edc0-b92ee679880d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "          tgt_ret      size     mom3m     mom6m    mom12m\n",
              "tgt_ret  1.000000  0.013780  0.032066  0.028684  0.035437\n",
              "size     0.013780  1.000000  0.108584  0.161473  0.208675\n",
              "mom3m    0.032066  0.108584  1.000000  0.623639  0.391253\n",
              "mom6m    0.028684  0.161473  0.623639  1.000000  0.637747\n",
              "mom12m   0.035437  0.208675  0.391253  0.637747  1.000000"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-ff9d7bff-93ee-4562-a588-aae01249251c\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>tgt_ret</th>\n",
              "      <th>size</th>\n",
              "      <th>mom3m</th>\n",
              "      <th>mom6m</th>\n",
              "      <th>mom12m</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>tgt_ret</th>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.013780</td>\n",
              "      <td>0.032066</td>\n",
              "      <td>0.028684</td>\n",
              "      <td>0.035437</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>size</th>\n",
              "      <td>0.013780</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.108584</td>\n",
              "      <td>0.161473</td>\n",
              "      <td>0.208675</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mom3m</th>\n",
              "      <td>0.032066</td>\n",
              "      <td>0.108584</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.623639</td>\n",
              "      <td>0.391253</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mom6m</th>\n",
              "      <td>0.028684</td>\n",
              "      <td>0.161473</td>\n",
              "      <td>0.623639</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.637747</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mom12m</th>\n",
              "      <td>0.035437</td>\n",
              "      <td>0.208675</td>\n",
              "      <td>0.391253</td>\n",
              "      <td>0.637747</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-ff9d7bff-93ee-4562-a588-aae01249251c')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-ff9d7bff-93ee-4562-a588-aae01249251c button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-ff9d7bff-93ee-4562-a588-aae01249251c');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "crsp.corr().tgt_ret.sort_values(ascending=False) # correlations between the target and the features."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UchYNbmtD9ZG",
        "outputId": "833d3cc5-e558-4187-b68f-e22648937ce6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tgt_ret      1.000000\n",
              "mom12m       0.035437\n",
              "mom3m        0.032066\n",
              "mom6m        0.028684\n",
              "size         0.013780\n",
              "ret          0.012303\n",
              "label2       0.010366\n",
              "label        0.010366\n",
              "prc          0.004117\n",
              "cumret       0.003351\n",
              "shrout      -0.001750\n",
              "vol         -0.007684\n",
              "permno      -0.014803\n",
              "relvol      -0.020155\n",
              "tgt_label   -0.826067\n",
              "Name: tgt_ret, dtype: float64"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.matshow(crsp.corr())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 293
        },
        "id": "mblqfx5REKu7",
        "outputId": "7bb00df5-1ca4-4e4e-cf45-e2fed59fe948"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7fedcfb82d90>"
            ]
          },
          "metadata": {},
          "execution_count": 15
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 288x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the filtered file\n",
        "crsp.to_csv('crsp_filtered.csv', index=False)"
      ],
      "metadata": {
        "id": "90Rmg1qXELmz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files \n",
        "files.download(\"crsp_filtered.csv\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "siFj13RUEZg2",
        "outputId": "a0facca2-4a02-4be8-a546-9eff919d452b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_59a4063d-1bc7-4e1f-89eb-dece16f5218d\", \"crsp_filtered.csv\", 94652706)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "crsp=pd.read_csv(\"/content/gdrive/My Drive/crsp_filtered.csv\")"
      ],
      "metadata": {
        "id": "FnemSsk3EtXr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "638e6644-3b67-41f7-90a9-2be7297d5e95"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "crsp.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "7lGTnq0eJV0o",
        "outputId": "ba536a11-14d7-4355-bcf6-78ca1b063fac"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "         date  permno      prc       ret  shrout     vol    relvol  label2  \\\n",
              "0  2001-01-31   80000  20.1250  0.025478  3564.0  1869.0  0.052441       6   \n",
              "1  2001-02-28   80000  20.0625 -0.003106  3473.0   811.0  0.023352       3   \n",
              "2  2001-03-30   80000  22.0625  0.099688  3466.0  2518.0  0.072649       1   \n",
              "3  2001-04-30   80000  21.4450 -0.027989  3466.0  1936.0  0.055857       6   \n",
              "4  2001-05-31   80000  21.6600  0.010026  3483.0   905.0  0.025983       5   \n",
              "\n",
              "   label   tgt_ret  tgt_label       size    cumret     mom3m     mom6m  \\\n",
              "0      6 -0.003106        3.0  11.180602  1.192593  0.019481  0.226563   \n",
              "1      3  0.099688        1.0  11.151626  1.188889  0.080537  0.183824   \n",
              "2      1 -0.027989        6.0  11.244636  1.307407  0.022293  0.163043   \n",
              "3      6  0.010026        5.0  11.216248  1.270815  0.096273  0.146104   \n",
              "4      5 -0.021237        5.0  11.231117  1.283556  0.068910  0.151409   \n",
              "\n",
              "     mom12m  \n",
              "0  0.171642  \n",
              "1  0.229008  \n",
              "2  0.360169  \n",
              "3  0.548246  \n",
              "4  0.394797  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-310fe89f-a501-410b-b7ee-80f09a2ae360\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>date</th>\n",
              "      <th>permno</th>\n",
              "      <th>prc</th>\n",
              "      <th>ret</th>\n",
              "      <th>shrout</th>\n",
              "      <th>vol</th>\n",
              "      <th>relvol</th>\n",
              "      <th>label2</th>\n",
              "      <th>label</th>\n",
              "      <th>tgt_ret</th>\n",
              "      <th>tgt_label</th>\n",
              "      <th>size</th>\n",
              "      <th>cumret</th>\n",
              "      <th>mom3m</th>\n",
              "      <th>mom6m</th>\n",
              "      <th>mom12m</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2001-01-31</td>\n",
              "      <td>80000</td>\n",
              "      <td>20.1250</td>\n",
              "      <td>0.025478</td>\n",
              "      <td>3564.0</td>\n",
              "      <td>1869.0</td>\n",
              "      <td>0.052441</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>-0.003106</td>\n",
              "      <td>3.0</td>\n",
              "      <td>11.180602</td>\n",
              "      <td>1.192593</td>\n",
              "      <td>0.019481</td>\n",
              "      <td>0.226563</td>\n",
              "      <td>0.171642</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2001-02-28</td>\n",
              "      <td>80000</td>\n",
              "      <td>20.0625</td>\n",
              "      <td>-0.003106</td>\n",
              "      <td>3473.0</td>\n",
              "      <td>811.0</td>\n",
              "      <td>0.023352</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>0.099688</td>\n",
              "      <td>1.0</td>\n",
              "      <td>11.151626</td>\n",
              "      <td>1.188889</td>\n",
              "      <td>0.080537</td>\n",
              "      <td>0.183824</td>\n",
              "      <td>0.229008</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2001-03-30</td>\n",
              "      <td>80000</td>\n",
              "      <td>22.0625</td>\n",
              "      <td>0.099688</td>\n",
              "      <td>3466.0</td>\n",
              "      <td>2518.0</td>\n",
              "      <td>0.072649</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>-0.027989</td>\n",
              "      <td>6.0</td>\n",
              "      <td>11.244636</td>\n",
              "      <td>1.307407</td>\n",
              "      <td>0.022293</td>\n",
              "      <td>0.163043</td>\n",
              "      <td>0.360169</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2001-04-30</td>\n",
              "      <td>80000</td>\n",
              "      <td>21.4450</td>\n",
              "      <td>-0.027989</td>\n",
              "      <td>3466.0</td>\n",
              "      <td>1936.0</td>\n",
              "      <td>0.055857</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>0.010026</td>\n",
              "      <td>5.0</td>\n",
              "      <td>11.216248</td>\n",
              "      <td>1.270815</td>\n",
              "      <td>0.096273</td>\n",
              "      <td>0.146104</td>\n",
              "      <td>0.548246</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2001-05-31</td>\n",
              "      <td>80000</td>\n",
              "      <td>21.6600</td>\n",
              "      <td>0.010026</td>\n",
              "      <td>3483.0</td>\n",
              "      <td>905.0</td>\n",
              "      <td>0.025983</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>-0.021237</td>\n",
              "      <td>5.0</td>\n",
              "      <td>11.231117</td>\n",
              "      <td>1.283556</td>\n",
              "      <td>0.068910</td>\n",
              "      <td>0.151409</td>\n",
              "      <td>0.394797</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-310fe89f-a501-410b-b7ee-80f09a2ae360')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-310fe89f-a501-410b-b7ee-80f09a2ae360 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-310fe89f-a501-410b-b7ee-80f09a2ae360');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Train, Test Split\n",
        "\n",
        "\n",
        "\n",
        "*   Splitting data into two bundles: before, and after 2010\n",
        "*   Train set: before 2010\n",
        "\n",
        "\n",
        "*   Test set: after 2010\n",
        "\n",
        "\n",
        "*   Independent variables: return, momentum 3/6/12\n",
        "*   Dependent variable: target label (10th rank)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "gBz5TKQJY2cv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "before_2010=crsp[crsp.date < '2010-01-01']\n",
        "after_2010=crsp[crsp.date >= '2010-01-01']\n",
        "\n",
        "from sklearn.model_selection import train_test_split \n",
        "x_train=before_2010[[\"ret\",'mom3m','mom6m','mom12m']].values\n",
        "x_test=after_2010[[\"ret\",'mom3m','mom6m','mom12m']].values\n",
        "y_train=before_2010['tgt_label'].values\n",
        "y_test=after_2010['tgt_label'].values"
      ],
      "metadata": {
        "id": "ePUIhSReJYHp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Classifier \n",
        "\n",
        "\n",
        "\n",
        "*   Random Forest Classifier\n",
        "\n",
        "      *   According to the Jounral of Risk and Financial Management article: A Random Forests Approach to Predicting Clean Energy\n",
        "Stock Prices, Decsion tree bagging and random forests predictions of stock price direction are more accurate than those obtained from logit models.\n",
        "      *  Tree bagging and random forests methods produce accuracy rates of between\n",
        "85% and 90% while logit models produce accuracy rates of between 55% and 60%. Tree bagging and\n",
        "random forests are easy to understand and estimate and are useful methods for forecasting the stock\n",
        "price direction of clean energy stocks. \n",
        "    * So it maybe a presumable approach, using Random Forest Classifier for our prediction\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    \n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "*   Multi Layer Perceptron\n",
        "\n",
        "    * Neural network will be our baseline model, serving as a comparison for Random Forest Classifier \n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "3NbcVtZWFrA4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import metrics"
      ],
      "metadata": {
        "id": "yZafpuQzL1JP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "46fN6MVqcJT-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "forest_clf=RandomForestClassifier(max_depth=None,max_features='auto',min_samples_leaf=1,min_samples_split=6,n_estimators=100)\n",
        "forest_clf.fit(x_train,y_train)\n",
        "forest_clf.score(x_test,y_test)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sppYZf6ZLbUe",
        "outputId": "26cdac33-c3c3-4818-c088-55f5b83c0bbc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.11892887362500622"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.neural_network import MLPClassifier\n",
        "clf = MLPClassifier(hidden_layer_sizes=(40,40), \n",
        "                    activation='relu',\n",
        "                    alpha=0,\n",
        "                    max_iter=100,\n",
        "                    verbose=True,\n",
        "                    early_stopping=True,\n",
        "                    validation_fraction=0.3,\n",
        "                    n_iter_no_change=5\n",
        "                   )\n",
        "clf = clf.fit(x_train, y_train)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OXR9bldnMd9F",
        "outputId": "38495501-b05a-42a2-d730-18b5551ecaee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1, loss = 2.27524995\n",
            "Validation score: 0.132466\n",
            "Iteration 2, loss = 2.26231920\n",
            "Validation score: 0.132383\n",
            "Iteration 3, loss = 2.25995752\n",
            "Validation score: 0.131476\n",
            "Iteration 4, loss = 2.25865699\n",
            "Validation score: 0.130884\n",
            "Iteration 5, loss = 2.25822674\n",
            "Validation score: 0.132741\n",
            "Iteration 6, loss = 2.25767026\n",
            "Validation score: 0.132480\n",
            "Iteration 7, loss = 2.25730815\n",
            "Validation score: 0.133044\n",
            "Iteration 8, loss = 2.25715448\n",
            "Validation score: 0.133731\n",
            "Iteration 9, loss = 2.25700297\n",
            "Validation score: 0.132383\n",
            "Iteration 10, loss = 2.25666478\n",
            "Validation score: 0.130788\n",
            "Iteration 11, loss = 2.25662677\n",
            "Validation score: 0.133938\n",
            "Iteration 12, loss = 2.25634635\n",
            "Validation score: 0.133745\n",
            "Iteration 13, loss = 2.25622541\n",
            "Validation score: 0.134171\n",
            "Iteration 14, loss = 2.25599369\n",
            "Validation score: 0.133717\n",
            "Iteration 15, loss = 2.25596639\n",
            "Validation score: 0.132232\n",
            "Iteration 16, loss = 2.25577438\n",
            "Validation score: 0.134185\n",
            "Iteration 17, loss = 2.25586318\n",
            "Validation score: 0.133621\n",
            "Iteration 18, loss = 2.25569271\n",
            "Validation score: 0.133993\n",
            "Iteration 19, loss = 2.25557664\n",
            "Validation score: 0.133319\n",
            "Validation score did not improve more than tol=0.000100 for 5 consecutive epochs. Stopping.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Random Search CV\n",
        "\n",
        "\n",
        "*   To find optimal hyper parameter\n",
        "*   Applying Randomized Search CV both Random Forest Classifier and MLP\n",
        "\n"
      ],
      "metadata": {
        "id": "LTiro55WhcTj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import RandomizedSearchCV \n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "import numpy as np\n",
        "np.random.seed(42)\n",
        "\n",
        "grid={\"n_estimators\":[100,200,500],\"max_depth\":[None,5,10,20,30],\"max_features\":[\"auto\",\"sqrt\"],\"min_samples_split\":[2,4,6],\"min_samples_leaf\":[4,6,8]}\n",
        "forest_clf=RandomForestClassifier(n_jobs=1)\n",
        "rs_forest_clf=RandomizedSearchCV(estimator=forest_clf,param_distributions=grid,n_iter=10,cv=5,verbose=2)\n",
        "rs_forest_clf.fit(x_train,y_train)\n",
        "rs_forest_clf.score(x_test,y_test)\n",
        "\n",
        "import pickle\n",
        "\n",
        "fpath = 'rs_forest_clf.pickle' # file name\n",
        "\n",
        "# Save\n",
        "pickle.dump(rs_forest_clf, open(fpath, 'wb'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n5z75tw4kQHv",
        "outputId": "af7d60f5-348f-4f8d-c19b-436e2942de8c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
            "[CV] END max_depth=10, max_features=auto, min_samples_leaf=4, min_samples_split=6, n_estimators=100; total time=  43.1s\n",
            "[CV] END max_depth=10, max_features=auto, min_samples_leaf=4, min_samples_split=6, n_estimators=100; total time=  43.0s\n",
            "[CV] END max_depth=10, max_features=auto, min_samples_leaf=4, min_samples_split=6, n_estimators=100; total time=  42.8s\n",
            "[CV] END max_depth=10, max_features=auto, min_samples_leaf=4, min_samples_split=6, n_estimators=100; total time=  42.1s\n",
            "[CV] END max_depth=10, max_features=auto, min_samples_leaf=4, min_samples_split=6, n_estimators=100; total time=  42.3s\n",
            "[CV] END max_depth=None, max_features=sqrt, min_samples_leaf=6, min_samples_split=2, n_estimators=100; total time= 1.4min\n",
            "[CV] END max_depth=None, max_features=sqrt, min_samples_leaf=6, min_samples_split=2, n_estimators=100; total time= 1.3min\n",
            "[CV] END max_depth=None, max_features=sqrt, min_samples_leaf=6, min_samples_split=2, n_estimators=100; total time= 1.3min\n",
            "[CV] END max_depth=None, max_features=sqrt, min_samples_leaf=6, min_samples_split=2, n_estimators=100; total time= 1.3min\n",
            "[CV] END max_depth=None, max_features=sqrt, min_samples_leaf=6, min_samples_split=2, n_estimators=100; total time= 1.3min\n",
            "[CV] END max_depth=30, max_features=sqrt, min_samples_leaf=6, min_samples_split=6, n_estimators=1200; total time=15.6min\n",
            "[CV] END max_depth=30, max_features=sqrt, min_samples_leaf=6, min_samples_split=6, n_estimators=1200; total time=16.0min\n",
            "[CV] END max_depth=30, max_features=sqrt, min_samples_leaf=6, min_samples_split=6, n_estimators=1200; total time=16.1min\n",
            "[CV] END max_depth=30, max_features=sqrt, min_samples_leaf=6, min_samples_split=6, n_estimators=1200; total time=16.9min\n",
            "[CV] END max_depth=30, max_features=sqrt, min_samples_leaf=6, min_samples_split=6, n_estimators=1200; total time=17.4min\n",
            "[CV] END max_depth=None, max_features=sqrt, min_samples_leaf=6, min_samples_split=6, n_estimators=200; total time= 3.0min\n",
            "[CV] END max_depth=None, max_features=sqrt, min_samples_leaf=6, min_samples_split=6, n_estimators=200; total time= 3.1min\n",
            "[CV] END max_depth=None, max_features=sqrt, min_samples_leaf=6, min_samples_split=6, n_estimators=200; total time= 3.0min\n",
            "[CV] END max_depth=None, max_features=sqrt, min_samples_leaf=6, min_samples_split=6, n_estimators=200; total time= 3.0min\n",
            "[CV] END max_depth=None, max_features=sqrt, min_samples_leaf=6, min_samples_split=6, n_estimators=200; total time= 3.0min\n",
            "[CV] END max_depth=30, max_features=auto, min_samples_leaf=8, min_samples_split=2, n_estimators=100; total time= 1.4min\n",
            "[CV] END max_depth=30, max_features=auto, min_samples_leaf=8, min_samples_split=2, n_estimators=100; total time= 1.4min\n",
            "[CV] END max_depth=30, max_features=auto, min_samples_leaf=8, min_samples_split=2, n_estimators=100; total time= 1.4min\n",
            "[CV] END max_depth=30, max_features=auto, min_samples_leaf=8, min_samples_split=2, n_estimators=100; total time= 1.4min\n",
            "[CV] END max_depth=30, max_features=auto, min_samples_leaf=8, min_samples_split=2, n_estimators=100; total time= 1.4min\n",
            "[CV] END max_depth=None, max_features=sqrt, min_samples_leaf=6, min_samples_split=2, n_estimators=1200; total time=17.6min\n",
            "[CV] END max_depth=None, max_features=sqrt, min_samples_leaf=6, min_samples_split=2, n_estimators=1200; total time=18.0min\n",
            "[CV] END max_depth=None, max_features=sqrt, min_samples_leaf=6, min_samples_split=2, n_estimators=1200; total time=18.4min\n",
            "[CV] END max_depth=None, max_features=sqrt, min_samples_leaf=6, min_samples_split=2, n_estimators=1200; total time=18.1min\n",
            "[CV] END max_depth=None, max_features=sqrt, min_samples_leaf=6, min_samples_split=2, n_estimators=1200; total time=18.0min\n",
            "[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=6, min_samples_split=4, n_estimators=500; total time= 4.0min\n",
            "[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=6, min_samples_split=4, n_estimators=500; total time= 3.7min\n",
            "[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=6, min_samples_split=4, n_estimators=500; total time= 3.9min\n",
            "[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=6, min_samples_split=4, n_estimators=500; total time= 3.9min\n",
            "[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=6, min_samples_split=4, n_estimators=500; total time= 3.8min\n",
            "[CV] END max_depth=20, max_features=sqrt, min_samples_leaf=8, min_samples_split=2, n_estimators=1000; total time=12.6min\n",
            "[CV] END max_depth=20, max_features=sqrt, min_samples_leaf=8, min_samples_split=2, n_estimators=1000; total time=12.5min\n",
            "[CV] END max_depth=20, max_features=sqrt, min_samples_leaf=8, min_samples_split=2, n_estimators=1000; total time=12.5min\n",
            "[CV] END max_depth=20, max_features=sqrt, min_samples_leaf=8, min_samples_split=2, n_estimators=1000; total time=12.5min\n",
            "[CV] END max_depth=20, max_features=sqrt, min_samples_leaf=8, min_samples_split=2, n_estimators=1000; total time=12.4min\n",
            "[CV] END max_depth=20, max_features=auto, min_samples_leaf=6, min_samples_split=2, n_estimators=10; total time=   7.3s\n",
            "[CV] END max_depth=20, max_features=auto, min_samples_leaf=6, min_samples_split=2, n_estimators=10; total time=   7.9s\n",
            "[CV] END max_depth=20, max_features=auto, min_samples_leaf=6, min_samples_split=2, n_estimators=10; total time=   7.1s\n",
            "[CV] END max_depth=20, max_features=auto, min_samples_leaf=6, min_samples_split=2, n_estimators=10; total time=   7.2s\n",
            "[CV] END max_depth=20, max_features=auto, min_samples_leaf=6, min_samples_split=2, n_estimators=10; total time=   7.2s\n",
            "[CV] END max_depth=20, max_features=sqrt, min_samples_leaf=4, min_samples_split=2, n_estimators=1200; total time=15.0min\n",
            "[CV] END max_depth=20, max_features=sqrt, min_samples_leaf=4, min_samples_split=2, n_estimators=1200; total time=15.3min\n",
            "[CV] END max_depth=20, max_features=sqrt, min_samples_leaf=4, min_samples_split=2, n_estimators=1200; total time=15.0min\n",
            "[CV] END max_depth=20, max_features=sqrt, min_samples_leaf=4, min_samples_split=2, n_estimators=1200; total time=15.1min\n",
            "[CV] END max_depth=20, max_features=sqrt, min_samples_leaf=4, min_samples_split=2, n_estimators=1200; total time=14.8min\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# in-sample (train set) accuracy\n",
        "print(clf.score(x_train, y_train)) \n",
        "\n",
        "# out-of-sample (test set) accuracy\n",
        "print(clf.score(x_test, y_test))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PXOSfYpUMgI7",
        "outputId": "0584f257-5d7d-4daa-c877-b2679cabb183"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.13636776221871982\n",
            "0.13547359514210344\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import RandomizedSearchCV \n",
        "grid={\"hidden_layer_sizes\":[(24,24),(32,32)],\"activation\":[\"relu\",\"tanh\"],\"solver\":['sgd','adam'],\"alpha\":[0,0.05],\"learning_rate\":['constant','adaptive'],\"validation_fraction\":[0.3],\"max_iter\":[100],\"verbose\":[True]}\n",
        "mlp_rs=MLPClassifier(max_iter=1)\n",
        "rs_clf=RandomizedSearchCV(estimator=mlp_rs,param_distributions=grid,n_iter=10,cv=5,verbose=2)\n",
        "rs_clf.fit(x_train,y_train)\n",
        "rs_clf.score(x_test,y_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "PEmKCJNoRD7R",
        "outputId": "b894b173-f6ed-41eb-a96a-3794e47815f3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
            "Iteration 1, loss = 2.28447618\n",
            "Iteration 2, loss = 2.26931668\n",
            "Iteration 3, loss = 2.26721870\n",
            "Iteration 4, loss = 2.26623653\n",
            "Iteration 5, loss = 2.26561806\n",
            "Iteration 6, loss = 2.26518820\n",
            "Iteration 7, loss = 2.26486339\n",
            "Iteration 8, loss = 2.26457818\n",
            "Iteration 9, loss = 2.26439776\n",
            "Iteration 10, loss = 2.26431158\n",
            "Iteration 11, loss = 2.26423095\n",
            "Iteration 12, loss = 2.26406438\n",
            "Iteration 13, loss = 2.26394213\n",
            "Iteration 14, loss = 2.26385011\n",
            "Iteration 15, loss = 2.26379789\n",
            "Iteration 16, loss = 2.26375493\n",
            "Iteration 17, loss = 2.26371208\n",
            "Iteration 18, loss = 2.26368366\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:699: UserWarning: Training interrupted by user.\n",
            "  warnings.warn(\"Training interrupted by user.\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END activation=relu, alpha=0.05, hidden_layer_sizes=(24, 24), learning_rate=constant, max_iter=100, solver=adam, validation_fraction=0.3, verbose=True; total time=  27.6s\n",
            "Iteration 1, loss = 2.28427430\n",
            "Iteration 2, loss = 2.27009886\n",
            "Iteration 3, loss = 2.26690459\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:699: UserWarning: Training interrupted by user.\n",
            "  warnings.warn(\"Training interrupted by user.\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END activation=relu, alpha=0.05, hidden_layer_sizes=(24, 24), learning_rate=constant, max_iter=100, solver=adam, validation_fraction=0.3, verbose=True; total time=   3.4s\n",
            "Iteration 1, loss = 2.28612692\n",
            "Iteration 2, loss = 2.27296657\n",
            "Iteration 3, loss = 2.27039193\n",
            "Iteration 4, loss = 2.26921620\n",
            "Iteration 5, loss = 2.26844731\n",
            "Iteration 6, loss = 2.26772241\n",
            "Iteration 7, loss = 2.26734144\n",
            "Iteration 8, loss = 2.26703920\n",
            "Iteration 9, loss = 2.26662206\n",
            "Iteration 10, loss = 2.26640689\n",
            "Iteration 11, loss = 2.26624015\n",
            "Iteration 12, loss = 2.26606823\n",
            "Iteration 13, loss = 2.26593290\n",
            "Iteration 14, loss = 2.26581825\n",
            "Iteration 15, loss = 2.26568187\n",
            "Iteration 16, loss = 2.26557149\n",
            "Iteration 17, loss = 2.26549330\n",
            "Iteration 18, loss = 2.26541415\n",
            "Iteration 19, loss = 2.26539602\n",
            "Iteration 20, loss = 2.26537636\n",
            "Iteration 21, loss = 2.26534025\n",
            "Iteration 22, loss = 2.26517064\n",
            "Iteration 23, loss = 2.26523647\n",
            "Iteration 24, loss = 2.26511521\n",
            "Iteration 25, loss = 2.26513106\n",
            "Iteration 26, loss = 2.26503654\n",
            "Iteration 27, loss = 2.26503931\n",
            "Iteration 28, loss = 2.26494296\n",
            "Iteration 29, loss = 2.26495722\n",
            "Iteration 30, loss = 2.26497211\n",
            "Iteration 31, loss = 2.26494176\n",
            "Iteration 32, loss = 2.26490398\n",
            "Iteration 33, loss = 2.26483273\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "[CV] END activation=relu, alpha=0.05, hidden_layer_sizes=(24, 24), learning_rate=constant, max_iter=100, solver=adam, validation_fraction=0.3, verbose=True; total time=  42.1s\n",
            "Iteration 1, loss = 2.28288338\n",
            "Iteration 2, loss = 2.26997806\n",
            "Iteration 3, loss = 2.26631242\n",
            "Iteration 4, loss = 2.26447125\n",
            "Iteration 5, loss = 2.26353938\n",
            "Iteration 6, loss = 2.26280287\n",
            "Iteration 7, loss = 2.26246020\n",
            "Iteration 8, loss = 2.26213512\n",
            "Iteration 9, loss = 2.26182252\n",
            "Iteration 10, loss = 2.26166318\n",
            "Iteration 11, loss = 2.26144627\n",
            "Iteration 12, loss = 2.26145997\n",
            "Iteration 13, loss = 2.26139781\n",
            "Iteration 14, loss = 2.26128599\n",
            "Iteration 15, loss = 2.26117665\n",
            "Iteration 16, loss = 2.26101656\n",
            "Iteration 17, loss = 2.26102039\n",
            "Iteration 18, loss = 2.26094221\n",
            "Iteration 19, loss = 2.26092981\n",
            "Iteration 20, loss = 2.26092726\n",
            "Iteration 21, loss = 2.26081887\n",
            "Iteration 22, loss = 2.26083178\n",
            "Iteration 23, loss = 2.26081603\n",
            "Iteration 24, loss = 2.26079123\n",
            "Iteration 25, loss = 2.26073001\n",
            "Iteration 26, loss = 2.26065056\n",
            "Iteration 27, loss = 2.26062795\n",
            "Iteration 28, loss = 2.26069354\n",
            "Iteration 29, loss = 2.26064509\n",
            "Iteration 30, loss = 2.26052237\n",
            "Iteration 31, loss = 2.26052397\n",
            "Iteration 32, loss = 2.26050340\n",
            "Iteration 33, loss = 2.26057613\n",
            "Iteration 34, loss = 2.26050551\n",
            "Iteration 35, loss = 2.26050530\n",
            "Iteration 36, loss = 2.26051495\n",
            "Iteration 37, loss = 2.26045928\n",
            "Iteration 38, loss = 2.26054596\n",
            "Iteration 39, loss = 2.26047862\n",
            "Iteration 40, loss = 2.26049456\n",
            "Iteration 41, loss = 2.26044929\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "[CV] END activation=relu, alpha=0.05, hidden_layer_sizes=(24, 24), learning_rate=constant, max_iter=100, solver=adam, validation_fraction=0.3, verbose=True; total time=  59.3s\n",
            "Iteration 1, loss = 2.28410156\n",
            "Iteration 2, loss = 2.26903731\n",
            "Iteration 3, loss = 2.26602502\n",
            "Iteration 4, loss = 2.26447538\n",
            "Iteration 5, loss = 2.26359160\n",
            "Iteration 6, loss = 2.26297345\n",
            "Iteration 7, loss = 2.26272581\n",
            "Iteration 8, loss = 2.26246913\n",
            "Iteration 9, loss = 2.26228564\n",
            "Iteration 10, loss = 2.26206807\n",
            "Iteration 11, loss = 2.26190763\n",
            "Iteration 12, loss = 2.26174346\n",
            "Iteration 13, loss = 2.26175220\n",
            "Iteration 14, loss = 2.26164049\n",
            "Iteration 15, loss = 2.26163761\n",
            "Iteration 16, loss = 2.26165163\n",
            "Iteration 17, loss = 2.26157568\n",
            "Iteration 18, loss = 2.26140040\n",
            "Iteration 19, loss = 2.26147635\n",
            "Iteration 20, loss = 2.26139070\n",
            "Iteration 21, loss = 2.26135947\n",
            "Iteration 22, loss = 2.26129366\n",
            "Iteration 23, loss = 2.26120638\n",
            "Iteration 24, loss = 2.26121172\n",
            "Iteration 25, loss = 2.26120789\n",
            "Iteration 26, loss = 2.26122879\n",
            "Iteration 27, loss = 2.26121000\n",
            "Iteration 28, loss = 2.26116257\n",
            "Iteration 29, loss = 2.26115159\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "[CV] END activation=relu, alpha=0.05, hidden_layer_sizes=(24, 24), learning_rate=constant, max_iter=100, solver=adam, validation_fraction=0.3, verbose=True; total time=  34.9s\n",
            "Iteration 1, loss = 2.31591893\n",
            "Iteration 2, loss = 2.30045778\n",
            "Iteration 3, loss = 2.29706006\n",
            "Iteration 4, loss = 2.29437940\n",
            "Iteration 5, loss = 2.29188691\n",
            "Iteration 6, loss = 2.28958137\n",
            "Iteration 7, loss = 2.28761391\n",
            "Iteration 8, loss = 2.28598678\n",
            "Iteration 9, loss = 2.28470167\n",
            "Iteration 10, loss = 2.28371761\n",
            "Iteration 11, loss = 2.28297027\n",
            "Iteration 12, loss = 2.28243077\n",
            "Iteration 13, loss = 2.28200106\n",
            "Iteration 14, loss = 2.28167733\n",
            "Iteration 15, loss = 2.28139761\n",
            "Iteration 16, loss = 2.28118580\n",
            "Iteration 17, loss = 2.28098541\n",
            "Iteration 18, loss = 2.28081905\n",
            "Iteration 19, loss = 2.28065408\n",
            "Iteration 20, loss = 2.28049578\n",
            "Iteration 21, loss = 2.28037259\n",
            "Iteration 22, loss = 2.28022322\n",
            "Iteration 23, loss = 2.28013113\n",
            "Iteration 24, loss = 2.27999686\n",
            "Iteration 25, loss = 2.27986599\n",
            "Iteration 26, loss = 2.27977977\n",
            "Iteration 27, loss = 2.27964913\n",
            "Iteration 28, loss = 2.27953330\n",
            "Iteration 29, loss = 2.27942728\n",
            "Iteration 30, loss = 2.27930493\n",
            "Iteration 31, loss = 2.27920801\n",
            "Iteration 32, loss = 2.27907110\n",
            "Iteration 33, loss = 2.27895397\n",
            "Iteration 34, loss = 2.27882921\n",
            "Iteration 35, loss = 2.27871402\n",
            "Iteration 36, loss = 2.27859084\n",
            "Iteration 37, loss = 2.27844974\n",
            "Iteration 38, loss = 2.27831834\n",
            "Iteration 39, loss = 2.27820072\n",
            "Iteration 40, loss = 2.27804721\n",
            "Iteration 41, loss = 2.27791661\n",
            "Iteration 42, loss = 2.27777878\n",
            "Iteration 43, loss = 2.27763919\n",
            "Iteration 44, loss = 2.27750649\n",
            "Iteration 45, loss = 2.27736293\n",
            "Iteration 46, loss = 2.27721345\n",
            "Iteration 47, loss = 2.27706587\n",
            "Iteration 48, loss = 2.27692819\n",
            "Iteration 49, loss = 2.27679243\n",
            "Iteration 50, loss = 2.27664404\n",
            "Iteration 51, loss = 2.27649935\n",
            "Iteration 52, loss = 2.27634408\n",
            "Iteration 53, loss = 2.27619230\n",
            "Iteration 54, loss = 2.27605698\n",
            "Iteration 55, loss = 2.27589804\n",
            "Iteration 56, loss = 2.27576038\n",
            "Iteration 57, loss = 2.27561262\n",
            "Iteration 58, loss = 2.27546203\n",
            "Iteration 59, loss = 2.27530454\n",
            "Iteration 60, loss = 2.27515870\n",
            "Iteration 61, loss = 2.27500964\n",
            "Iteration 62, loss = 2.27486521\n",
            "Iteration 63, loss = 2.27471543\n",
            "Iteration 64, loss = 2.27456425\n",
            "Iteration 65, loss = 2.27441245\n",
            "Iteration 66, loss = 2.27426105\n",
            "Iteration 67, loss = 2.27412595\n",
            "Iteration 68, loss = 2.27398929\n",
            "Iteration 69, loss = 2.27385990\n",
            "Iteration 70, loss = 2.27372932\n",
            "Iteration 71, loss = 2.27359106\n",
            "Iteration 72, loss = 2.27347012\n",
            "Iteration 73, loss = 2.27334640\n",
            "Iteration 74, loss = 2.27321454\n",
            "Iteration 75, loss = 2.27311343\n",
            "Iteration 76, loss = 2.27300181\n",
            "Iteration 77, loss = 2.27289503\n",
            "Iteration 78, loss = 2.27278596\n",
            "Iteration 79, loss = 2.27268052\n",
            "Iteration 80, loss = 2.27259148\n",
            "Iteration 81, loss = 2.27247692\n",
            "Iteration 82, loss = 2.27238268\n",
            "Iteration 83, loss = 2.27228443\n",
            "Iteration 84, loss = 2.27217784\n",
            "Iteration 85, loss = 2.27208289\n",
            "Iteration 86, loss = 2.27200001\n",
            "Iteration 87, loss = 2.27190415\n",
            "Iteration 88, loss = 2.27181511\n",
            "Iteration 89, loss = 2.27172914\n",
            "Iteration 90, loss = 2.27163111\n",
            "Iteration 91, loss = 2.27155387\n",
            "Iteration 92, loss = 2.27147183\n",
            "Iteration 93, loss = 2.27138113\n",
            "Iteration 94, loss = 2.27130034\n",
            "Iteration 95, loss = 2.27121092\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "[CV] END activation=relu, alpha=0.05, hidden_layer_sizes=(24, 24), learning_rate=constant, max_iter=100, solver=sgd, validation_fraction=0.3, verbose=True; total time= 1.2min\n",
            "Iteration 1, loss = 2.31501303\n",
            "Iteration 2, loss = 2.30198952\n",
            "Iteration 3, loss = 2.29899608\n",
            "Iteration 4, loss = 2.29653567\n",
            "Iteration 5, loss = 2.29405454\n",
            "Iteration 6, loss = 2.29123997\n",
            "Iteration 7, loss = 2.28860760\n",
            "Iteration 8, loss = 2.28656880\n",
            "Iteration 9, loss = 2.28478118\n",
            "Iteration 10, loss = 2.28325457\n",
            "Iteration 11, loss = 2.28197437\n",
            "Iteration 12, loss = 2.28095806\n",
            "Iteration 13, loss = 2.28014284\n",
            "Iteration 14, loss = 2.27947780\n",
            "Iteration 15, loss = 2.27893429\n",
            "Iteration 16, loss = 2.27846327\n",
            "Iteration 17, loss = 2.27806745\n",
            "Iteration 18, loss = 2.27770952\n",
            "Iteration 19, loss = 2.27741338\n",
            "Iteration 20, loss = 2.27711327\n",
            "Iteration 21, loss = 2.27683021\n",
            "Iteration 22, loss = 2.27656405\n",
            "Iteration 23, loss = 2.27630219\n",
            "Iteration 24, loss = 2.27603790\n",
            "Iteration 25, loss = 2.27578240\n",
            "Iteration 26, loss = 2.27551182\n",
            "Iteration 27, loss = 2.27528794\n",
            "Iteration 28, loss = 2.27502922\n",
            "Iteration 29, loss = 2.27478734\n",
            "Iteration 30, loss = 2.27453786\n",
            "Iteration 31, loss = 2.27431483\n",
            "Iteration 32, loss = 2.27405355\n",
            "Iteration 33, loss = 2.27381216\n",
            "Iteration 34, loss = 2.27355841\n",
            "Iteration 35, loss = 2.27331298\n",
            "Iteration 36, loss = 2.27308341\n",
            "Iteration 37, loss = 2.27283789\n",
            "Iteration 38, loss = 2.27259722\n",
            "Iteration 39, loss = 2.27238741\n",
            "Iteration 40, loss = 2.27218075\n",
            "Iteration 41, loss = 2.27197954\n",
            "Iteration 42, loss = 2.27177748\n",
            "Iteration 43, loss = 2.27159542\n",
            "Iteration 44, loss = 2.27140372\n",
            "Iteration 45, loss = 2.27124587\n",
            "Iteration 46, loss = 2.27106447\n",
            "Iteration 47, loss = 2.27091482\n",
            "Iteration 48, loss = 2.27076290\n",
            "Iteration 49, loss = 2.27059180\n",
            "Iteration 50, loss = 2.27046877\n",
            "Iteration 51, loss = 2.27033638\n",
            "Iteration 52, loss = 2.27020758\n",
            "Iteration 53, loss = 2.27010087\n",
            "Iteration 54, loss = 2.26997363\n",
            "Iteration 55, loss = 2.26984846\n",
            "Iteration 56, loss = 2.26973083\n",
            "Iteration 57, loss = 2.26962400\n",
            "Iteration 58, loss = 2.26951455\n",
            "Iteration 59, loss = 2.26940524\n",
            "Iteration 60, loss = 2.26930923\n",
            "Iteration 61, loss = 2.26922634\n",
            "Iteration 62, loss = 2.26910655\n",
            "Iteration 63, loss = 2.26901646\n",
            "Iteration 64, loss = 2.26891801\n",
            "Iteration 65, loss = 2.26882759\n",
            "Iteration 66, loss = 2.26873487\n",
            "Iteration 67, loss = 2.26864356\n",
            "Iteration 68, loss = 2.26854701\n",
            "Iteration 69, loss = 2.26845951\n",
            "Iteration 70, loss = 2.26836269\n",
            "Iteration 71, loss = 2.26828066\n",
            "Iteration 72, loss = 2.26820049\n",
            "Iteration 73, loss = 2.26813923\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "[CV] END activation=relu, alpha=0.05, hidden_layer_sizes=(24, 24), learning_rate=constant, max_iter=100, solver=sgd, validation_fraction=0.3, verbose=True; total time=  58.6s\n",
            "Iteration 1, loss = 2.31990863\n",
            "Iteration 2, loss = 2.30540984\n",
            "Iteration 3, loss = 2.30307349\n",
            "Iteration 4, loss = 2.30149630\n",
            "Iteration 5, loss = 2.30004385\n",
            "Iteration 6, loss = 2.29852616\n",
            "Iteration 7, loss = 2.29685968\n",
            "Iteration 8, loss = 2.29504767\n",
            "Iteration 9, loss = 2.29317933\n",
            "Iteration 10, loss = 2.29130852\n",
            "Iteration 11, loss = 2.28947601\n",
            "Iteration 12, loss = 2.28778114\n",
            "Iteration 13, loss = 2.28624572\n",
            "Iteration 14, loss = 2.28495557\n",
            "Iteration 15, loss = 2.28389757\n",
            "Iteration 16, loss = 2.28307513\n",
            "Iteration 17, loss = 2.28242911\n",
            "Iteration 18, loss = 2.28191998\n",
            "Iteration 19, loss = 2.28149650\n",
            "Iteration 20, loss = 2.28115226\n",
            "Iteration 21, loss = 2.28083529\n",
            "Iteration 22, loss = 2.28052904\n",
            "Iteration 23, loss = 2.28026338\n",
            "Iteration 24, loss = 2.28002854\n",
            "Iteration 25, loss = 2.27977558\n",
            "Iteration 26, loss = 2.27952901\n",
            "Iteration 27, loss = 2.27931040\n",
            "Iteration 28, loss = 2.27907376\n",
            "Iteration 29, loss = 2.27884175\n",
            "Iteration 30, loss = 2.27861724\n",
            "Iteration 31, loss = 2.27837731\n",
            "Iteration 32, loss = 2.27817527\n",
            "Iteration 33, loss = 2.27796420\n",
            "Iteration 34, loss = 2.27775289\n",
            "Iteration 35, loss = 2.27755136\n",
            "Iteration 36, loss = 2.27733786\n",
            "Iteration 37, loss = 2.27714078\n",
            "Iteration 38, loss = 2.27693578\n",
            "Iteration 39, loss = 2.27675983\n",
            "Iteration 40, loss = 2.27657542\n",
            "Iteration 41, loss = 2.27639385\n",
            "Iteration 42, loss = 2.27623192\n",
            "Iteration 43, loss = 2.27603937\n",
            "Iteration 44, loss = 2.27589545\n",
            "Iteration 45, loss = 2.27572628\n",
            "Iteration 46, loss = 2.27557209\n",
            "Iteration 47, loss = 2.27542451\n",
            "Iteration 48, loss = 2.27528492\n",
            "Iteration 49, loss = 2.27515532\n",
            "Iteration 50, loss = 2.27501797\n",
            "Iteration 51, loss = 2.27489088\n",
            "Iteration 52, loss = 2.27477654\n",
            "Iteration 53, loss = 2.27467036\n",
            "Iteration 54, loss = 2.27455165\n",
            "Iteration 55, loss = 2.27443741\n",
            "Iteration 56, loss = 2.27433743\n",
            "Iteration 57, loss = 2.27422673\n",
            "Iteration 58, loss = 2.27413766\n",
            "Iteration 59, loss = 2.27402906\n",
            "Iteration 60, loss = 2.27393022\n",
            "Iteration 61, loss = 2.27382273\n",
            "Iteration 62, loss = 2.27373031\n",
            "Iteration 63, loss = 2.27363244\n",
            "Iteration 64, loss = 2.27353497\n",
            "Iteration 65, loss = 2.27342514\n",
            "Iteration 66, loss = 2.27335774\n",
            "Iteration 67, loss = 2.27325224\n",
            "Iteration 68, loss = 2.27317656\n",
            "Iteration 69, loss = 2.27307624\n",
            "Iteration 70, loss = 2.27297627\n",
            "Iteration 71, loss = 2.27289131\n",
            "Iteration 72, loss = 2.27280067\n",
            "Iteration 73, loss = 2.27271534\n",
            "Iteration 74, loss = 2.27261842\n",
            "Iteration 75, loss = 2.27255204\n",
            "Iteration 76, loss = 2.27246609\n",
            "Iteration 77, loss = 2.27237548\n",
            "Iteration 78, loss = 2.27229895\n",
            "Iteration 79, loss = 2.27220612\n",
            "Iteration 80, loss = 2.27211807\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "[CV] END activation=relu, alpha=0.05, hidden_layer_sizes=(24, 24), learning_rate=constant, max_iter=100, solver=sgd, validation_fraction=0.3, verbose=True; total time=  59.9s\n",
            "Iteration 1, loss = 2.31513889\n",
            "Iteration 2, loss = 2.30474727\n",
            "Iteration 3, loss = 2.30089324\n",
            "Iteration 4, loss = 2.29770880\n",
            "Iteration 5, loss = 2.29497171\n",
            "Iteration 6, loss = 2.29234414\n",
            "Iteration 7, loss = 2.28989908\n",
            "Iteration 8, loss = 2.28766996\n",
            "Iteration 9, loss = 2.28567283\n",
            "Iteration 10, loss = 2.28397074\n",
            "Iteration 11, loss = 2.28252837\n",
            "Iteration 12, loss = 2.28136629\n",
            "Iteration 13, loss = 2.28044978\n",
            "Iteration 14, loss = 2.27969862\n",
            "Iteration 15, loss = 2.27908999\n",
            "Iteration 16, loss = 2.27860397\n",
            "Iteration 17, loss = 2.27815630\n",
            "Iteration 18, loss = 2.27778292\n",
            "Iteration 19, loss = 2.27748320\n",
            "Iteration 20, loss = 2.27715771\n",
            "Iteration 21, loss = 2.27687885\n",
            "Iteration 22, loss = 2.27661071\n",
            "Iteration 23, loss = 2.27634824\n",
            "Iteration 24, loss = 2.27610421\n",
            "Iteration 25, loss = 2.27585369\n",
            "Iteration 26, loss = 2.27561598\n",
            "Iteration 27, loss = 2.27536683\n",
            "Iteration 28, loss = 2.27515001\n",
            "Iteration 29, loss = 2.27491564\n",
            "Iteration 30, loss = 2.27470333\n",
            "Iteration 31, loss = 2.27447958\n",
            "Iteration 32, loss = 2.27424110\n",
            "Iteration 33, loss = 2.27404150\n",
            "Iteration 34, loss = 2.27380308\n",
            "Iteration 35, loss = 2.27361141\n",
            "Iteration 36, loss = 2.27337985\n",
            "Iteration 37, loss = 2.27316496\n",
            "Iteration 38, loss = 2.27296858\n",
            "Iteration 39, loss = 2.27274698\n",
            "Iteration 40, loss = 2.27257632\n",
            "Iteration 41, loss = 2.27238982\n",
            "Iteration 42, loss = 2.27219050\n",
            "Iteration 43, loss = 2.27201220\n",
            "Iteration 44, loss = 2.27182238\n",
            "Iteration 45, loss = 2.27164783\n",
            "Iteration 46, loss = 2.27147322\n",
            "Iteration 47, loss = 2.27130975\n",
            "Iteration 48, loss = 2.27114268\n",
            "Iteration 49, loss = 2.27100441\n",
            "Iteration 50, loss = 2.27084182\n",
            "Iteration 51, loss = 2.27066756\n",
            "Iteration 52, loss = 2.27054728\n",
            "Iteration 53, loss = 2.27037664\n",
            "Iteration 54, loss = 2.27020947\n",
            "Iteration 55, loss = 2.27008119\n",
            "Iteration 56, loss = 2.26994215\n",
            "Iteration 57, loss = 2.26979416\n",
            "Iteration 58, loss = 2.26968099\n",
            "Iteration 59, loss = 2.26952107\n",
            "Iteration 60, loss = 2.26940194\n",
            "Iteration 61, loss = 2.26926525\n",
            "Iteration 62, loss = 2.26912892\n",
            "Iteration 63, loss = 2.26902360\n",
            "Iteration 64, loss = 2.26887816\n",
            "Iteration 65, loss = 2.26876672\n",
            "Iteration 66, loss = 2.26863964\n",
            "Iteration 67, loss = 2.26852202\n",
            "Iteration 68, loss = 2.26840818\n",
            "Iteration 69, loss = 2.26829340\n",
            "Iteration 70, loss = 2.26819941\n",
            "Iteration 71, loss = 2.26808814\n",
            "Iteration 72, loss = 2.26797613\n",
            "Iteration 73, loss = 2.26786879\n",
            "Iteration 74, loss = 2.26774563\n",
            "Iteration 75, loss = 2.26766391\n",
            "Iteration 76, loss = 2.26754215\n",
            "Iteration 77, loss = 2.26747520\n",
            "Iteration 78, loss = 2.26738239\n",
            "Iteration 79, loss = 2.26726954\n",
            "Iteration 80, loss = 2.26720011\n",
            "Iteration 81, loss = 2.26709986\n",
            "Iteration 82, loss = 2.26702934\n",
            "Iteration 83, loss = 2.26692840\n",
            "Iteration 84, loss = 2.26686497\n",
            "Iteration 85, loss = 2.26678232\n",
            "Iteration 86, loss = 2.26668740\n",
            "Iteration 87, loss = 2.26659622\n",
            "Iteration 88, loss = 2.26654700\n",
            "Iteration 89, loss = 2.26646841\n",
            "Iteration 90, loss = 2.26638191\n",
            "Iteration 91, loss = 2.26631704\n",
            "Iteration 92, loss = 2.26626127\n",
            "Iteration 93, loss = 2.26618540\n",
            "Iteration 94, loss = 2.26610267\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "[CV] END activation=relu, alpha=0.05, hidden_layer_sizes=(24, 24), learning_rate=constant, max_iter=100, solver=sgd, validation_fraction=0.3, verbose=True; total time= 1.2min\n",
            "Iteration 1, loss = 2.30807844\n",
            "Iteration 2, loss = 2.29865117\n",
            "Iteration 3, loss = 2.29557899\n",
            "Iteration 4, loss = 2.29316176\n",
            "Iteration 5, loss = 2.29101597\n",
            "Iteration 6, loss = 2.28912488\n",
            "Iteration 7, loss = 2.28742622\n",
            "Iteration 8, loss = 2.28592925\n",
            "Iteration 9, loss = 2.28461253\n",
            "Iteration 10, loss = 2.28342871\n",
            "Iteration 11, loss = 2.28237658\n",
            "Iteration 12, loss = 2.28142398\n",
            "Iteration 13, loss = 2.28060188\n",
            "Iteration 14, loss = 2.27987561\n",
            "Iteration 15, loss = 2.27924589\n",
            "Iteration 16, loss = 2.27868767\n",
            "Iteration 17, loss = 2.27819369\n",
            "Iteration 18, loss = 2.27774810\n",
            "Iteration 19, loss = 2.27732840\n",
            "Iteration 20, loss = 2.27694710\n",
            "Iteration 21, loss = 2.27654339\n",
            "Iteration 22, loss = 2.27624836\n",
            "Iteration 23, loss = 2.27595370\n",
            "Iteration 24, loss = 2.27565916\n",
            "Iteration 25, loss = 2.27537313\n",
            "Iteration 26, loss = 2.27512339\n",
            "Iteration 27, loss = 2.27487643\n",
            "Iteration 28, loss = 2.27463429\n",
            "Iteration 29, loss = 2.27441464\n",
            "Iteration 30, loss = 2.27416684\n",
            "Iteration 31, loss = 2.27395647\n",
            "Iteration 32, loss = 2.27374300\n",
            "Iteration 33, loss = 2.27351334\n",
            "Iteration 34, loss = 2.27331644\n",
            "Iteration 35, loss = 2.27309834\n",
            "Iteration 36, loss = 2.27290682\n",
            "Iteration 37, loss = 2.27267323\n",
            "Iteration 38, loss = 2.27245432\n",
            "Iteration 39, loss = 2.27227997\n",
            "Iteration 40, loss = 2.27205627\n",
            "Iteration 41, loss = 2.27185451\n",
            "Iteration 42, loss = 2.27165803\n",
            "Iteration 43, loss = 2.27146809\n",
            "Iteration 44, loss = 2.27130684\n",
            "Iteration 45, loss = 2.27110665\n",
            "Iteration 46, loss = 2.27091673\n",
            "Iteration 47, loss = 2.27077384\n",
            "Iteration 48, loss = 2.27062604\n",
            "Iteration 49, loss = 2.27043026\n",
            "Iteration 50, loss = 2.27026997\n",
            "Iteration 51, loss = 2.27011288\n",
            "Iteration 52, loss = 2.26996288\n",
            "Iteration 53, loss = 2.26980269\n",
            "Iteration 54, loss = 2.26964907\n",
            "Iteration 55, loss = 2.26949745\n",
            "Iteration 56, loss = 2.26935902\n",
            "Iteration 57, loss = 2.26921055\n",
            "Iteration 58, loss = 2.26907463\n",
            "Iteration 59, loss = 2.26894296\n",
            "Iteration 60, loss = 2.26879202\n",
            "Iteration 61, loss = 2.26867341\n",
            "Iteration 62, loss = 2.26854858\n",
            "Iteration 63, loss = 2.26843172\n",
            "Iteration 64, loss = 2.26831255\n",
            "Iteration 65, loss = 2.26820289\n",
            "Iteration 66, loss = 2.26806751\n",
            "Iteration 67, loss = 2.26798831\n",
            "Iteration 68, loss = 2.26787448\n",
            "Iteration 69, loss = 2.26778948\n",
            "Iteration 70, loss = 2.26768747\n",
            "Iteration 71, loss = 2.26757875\n",
            "Iteration 72, loss = 2.26749157\n",
            "Iteration 73, loss = 2.26741544\n",
            "Iteration 74, loss = 2.26731609\n",
            "Iteration 75, loss = 2.26724058\n",
            "Iteration 76, loss = 2.26715377\n",
            "Iteration 77, loss = 2.26708334\n",
            "Iteration 78, loss = 2.26699345\n",
            "Iteration 79, loss = 2.26690984\n",
            "Iteration 80, loss = 2.26686825\n",
            "Iteration 81, loss = 2.26678084\n",
            "Iteration 82, loss = 2.26673445\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "[CV] END activation=relu, alpha=0.05, hidden_layer_sizes=(24, 24), learning_rate=constant, max_iter=100, solver=sgd, validation_fraction=0.3, verbose=True; total time= 1.0min\n",
            "Iteration 1, loss = 2.31759211\n",
            "Iteration 2, loss = 2.30271980\n",
            "Iteration 3, loss = 2.30013786\n",
            "Iteration 4, loss = 2.29800759\n",
            "Iteration 5, loss = 2.29578001\n",
            "Iteration 6, loss = 2.29348329\n",
            "Iteration 7, loss = 2.29132000\n",
            "Iteration 8, loss = 2.28946379\n",
            "Iteration 9, loss = 2.28794201\n",
            "Iteration 10, loss = 2.28660580\n",
            "Iteration 11, loss = 2.28535528\n",
            "Iteration 12, loss = 2.28415385\n",
            "Iteration 13, loss = 2.28307663\n",
            "Iteration 14, loss = 2.28217670\n",
            "Iteration 15, loss = 2.28141809\n",
            "Iteration 16, loss = 2.28078160\n",
            "Iteration 17, loss = 2.28025403\n",
            "Iteration 18, loss = 2.27979993\n",
            "Iteration 19, loss = 2.27939496\n",
            "Iteration 20, loss = 2.27903907\n",
            "Iteration 21, loss = 2.27868841\n",
            "Iteration 22, loss = 2.27838307\n",
            "Iteration 23, loss = 2.27808561\n",
            "Iteration 24, loss = 2.27778785\n",
            "Iteration 25, loss = 2.27751669\n",
            "Iteration 26, loss = 2.27725128\n",
            "Iteration 27, loss = 2.27699298\n",
            "Iteration 28, loss = 2.27672914\n",
            "Iteration 29, loss = 2.27646339\n",
            "Iteration 30, loss = 2.27618993\n",
            "Iteration 31, loss = 2.27594488\n",
            "Iteration 32, loss = 2.27569192\n",
            "Iteration 33, loss = 2.27547264\n",
            "Iteration 34, loss = 2.27524781\n",
            "Iteration 35, loss = 2.27501802\n",
            "Iteration 36, loss = 2.27479675\n",
            "Iteration 37, loss = 2.27455016\n",
            "Iteration 38, loss = 2.27434965\n",
            "Iteration 39, loss = 2.27413559\n",
            "Iteration 40, loss = 2.27390656\n",
            "Iteration 41, loss = 2.27370018\n",
            "Iteration 42, loss = 2.27348123\n",
            "Iteration 43, loss = 2.27329076\n",
            "Iteration 44, loss = 2.27309353\n",
            "Iteration 45, loss = 2.27289073\n",
            "Iteration 46, loss = 2.27268473\n",
            "Iteration 47, loss = 2.27249079\n",
            "Iteration 48, loss = 2.27231267\n",
            "Iteration 49, loss = 2.27212019\n",
            "Iteration 50, loss = 2.27193389\n",
            "Iteration 51, loss = 2.27174597\n",
            "Iteration 52, loss = 2.27156370\n",
            "Iteration 53, loss = 2.27138281\n",
            "Iteration 54, loss = 2.27120005\n",
            "Iteration 55, loss = 2.27103860\n",
            "Iteration 56, loss = 2.27086569\n",
            "Iteration 57, loss = 2.27070873\n",
            "Iteration 58, loss = 2.27053789\n",
            "Iteration 59, loss = 2.27037620\n",
            "Iteration 60, loss = 2.27025544\n",
            "Iteration 61, loss = 2.27008845\n",
            "Iteration 62, loss = 2.26997415\n",
            "Iteration 63, loss = 2.26980351\n",
            "Iteration 64, loss = 2.26966574\n",
            "Iteration 65, loss = 2.26955955\n",
            "Iteration 66, loss = 2.26945256\n",
            "Iteration 67, loss = 2.26932028\n",
            "Iteration 68, loss = 2.26921996\n",
            "Iteration 69, loss = 2.26911185\n",
            "Iteration 70, loss = 2.26900341\n",
            "Iteration 71, loss = 2.26891848\n",
            "Iteration 72, loss = 2.26883173\n",
            "Iteration 73, loss = 2.26872612\n",
            "Iteration 74, loss = 2.26863756\n",
            "Iteration 75, loss = 2.26856498\n",
            "Iteration 76, loss = 2.26849385\n",
            "Iteration 77, loss = 2.26842985\n",
            "Iteration 78, loss = 2.26834294\n",
            "Iteration 79, loss = 2.26827880\n",
            "Iteration 80, loss = 2.26822816\n",
            "Iteration 81, loss = 2.26815236\n",
            "Iteration 82, loss = 2.26809739\n",
            "Iteration 83, loss = 2.26802432\n",
            "Iteration 84, loss = 2.26798614\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
            "Iteration 85, loss = 2.26787806\n",
            "Iteration 86, loss = 2.26786693\n",
            "Iteration 87, loss = 2.26785381\n",
            "Iteration 88, loss = 2.26784275\n",
            "Iteration 89, loss = 2.26783254\n",
            "Iteration 90, loss = 2.26782014\n",
            "Iteration 91, loss = 2.26781142\n",
            "Iteration 92, loss = 2.26779767\n",
            "Iteration 93, loss = 2.26778971\n",
            "Iteration 94, loss = 2.26777873\n",
            "Iteration 95, loss = 2.26776839\n",
            "Iteration 96, loss = 2.26775652\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000040\n",
            "Iteration 97, loss = 2.26773210\n",
            "Iteration 98, loss = 2.26772999\n",
            "Iteration 99, loss = 2.26772721\n",
            "Iteration 100, loss = 2.26772545\n",
            "[CV] END activation=relu, alpha=0.05, hidden_layer_sizes=(24, 24), learning_rate=adaptive, max_iter=100, solver=sgd, validation_fraction=0.3, verbose=True; total time= 1.3min\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  ConvergenceWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1, loss = 2.31662637\n",
            "Iteration 2, loss = 2.30208548\n",
            "Iteration 3, loss = 2.29937791\n",
            "Iteration 4, loss = 2.29710458\n",
            "Iteration 5, loss = 2.29486239\n",
            "Iteration 6, loss = 2.29258921\n",
            "Iteration 7, loss = 2.29029415\n",
            "Iteration 8, loss = 2.28809394\n",
            "Iteration 9, loss = 2.28605282\n",
            "Iteration 10, loss = 2.28429126\n",
            "Iteration 11, loss = 2.28285396\n",
            "Iteration 12, loss = 2.28173421\n",
            "Iteration 13, loss = 2.28089877\n",
            "Iteration 14, loss = 2.28024689\n",
            "Iteration 15, loss = 2.27978182\n",
            "Iteration 16, loss = 2.27940311\n",
            "Iteration 17, loss = 2.27909062\n",
            "Iteration 18, loss = 2.27881957\n",
            "Iteration 19, loss = 2.27859115\n",
            "Iteration 20, loss = 2.27836165\n",
            "Iteration 21, loss = 2.27814859\n",
            "Iteration 22, loss = 2.27793083\n",
            "Iteration 23, loss = 2.27770439\n",
            "Iteration 24, loss = 2.27750869\n",
            "Iteration 25, loss = 2.27728860\n",
            "Iteration 26, loss = 2.27709582\n",
            "Iteration 27, loss = 2.27690197\n",
            "Iteration 28, loss = 2.27673379\n",
            "Iteration 29, loss = 2.27654102\n",
            "Iteration 30, loss = 2.27635338\n",
            "Iteration 31, loss = 2.27616800\n",
            "Iteration 32, loss = 2.27599102\n",
            "Iteration 33, loss = 2.27579673\n",
            "Iteration 34, loss = 2.27561825\n",
            "Iteration 35, loss = 2.27542225\n",
            "Iteration 36, loss = 2.27523217\n",
            "Iteration 37, loss = 2.27505119\n",
            "Iteration 38, loss = 2.27485241\n",
            "Iteration 39, loss = 2.27465391\n",
            "Iteration 40, loss = 2.27445202\n",
            "Iteration 41, loss = 2.27424080\n",
            "Iteration 42, loss = 2.27411004\n",
            "Iteration 43, loss = 2.27391409\n",
            "Iteration 44, loss = 2.27372774\n",
            "Iteration 45, loss = 2.27353258\n",
            "Iteration 46, loss = 2.27334524\n",
            "Iteration 47, loss = 2.27313364\n",
            "Iteration 48, loss = 2.27295663\n",
            "Iteration 49, loss = 2.27276619\n",
            "Iteration 50, loss = 2.27261967\n",
            "Iteration 51, loss = 2.27245856\n",
            "Iteration 52, loss = 2.27231268\n",
            "Iteration 53, loss = 2.27216037\n",
            "Iteration 54, loss = 2.27200195\n",
            "Iteration 55, loss = 2.27187240\n",
            "Iteration 56, loss = 2.27175431\n",
            "Iteration 57, loss = 2.27163229\n",
            "Iteration 58, loss = 2.27150318\n",
            "Iteration 59, loss = 2.27138106\n",
            "Iteration 60, loss = 2.27126080\n",
            "Iteration 61, loss = 2.27116477\n",
            "Iteration 62, loss = 2.27103987\n",
            "Iteration 63, loss = 2.27092308\n",
            "Iteration 64, loss = 2.27081404\n",
            "Iteration 65, loss = 2.27071247\n",
            "Iteration 66, loss = 2.27061379\n",
            "Iteration 67, loss = 2.27050549\n",
            "Iteration 68, loss = 2.27040060\n",
            "Iteration 69, loss = 2.27030587\n",
            "Iteration 70, loss = 2.27020672\n",
            "Iteration 71, loss = 2.27010133\n",
            "Iteration 72, loss = 2.26999018\n",
            "Iteration 73, loss = 2.26989512\n",
            "Iteration 74, loss = 2.26979847\n",
            "Iteration 75, loss = 2.26968880\n",
            "Iteration 76, loss = 2.26960389\n",
            "Iteration 77, loss = 2.26948798\n",
            "Iteration 78, loss = 2.26936304\n",
            "Iteration 79, loss = 2.26926136\n",
            "Iteration 80, loss = 2.26917283\n",
            "Iteration 81, loss = 2.26907193\n",
            "Iteration 82, loss = 2.26897898\n",
            "Iteration 83, loss = 2.26888360\n",
            "Iteration 84, loss = 2.26880670\n",
            "Iteration 85, loss = 2.26871246\n",
            "Iteration 86, loss = 2.26864970\n",
            "Iteration 87, loss = 2.26855429\n",
            "Iteration 88, loss = 2.26848812\n",
            "Iteration 89, loss = 2.26840988\n",
            "Iteration 90, loss = 2.26833679\n",
            "Iteration 91, loss = 2.26826924\n",
            "Iteration 92, loss = 2.26816854\n",
            "Iteration 93, loss = 2.26810308\n",
            "Iteration 94, loss = 2.26805086\n",
            "Iteration 95, loss = 2.26799014\n",
            "Iteration 96, loss = 2.26791878\n",
            "Iteration 97, loss = 2.26785611\n",
            "Iteration 98, loss = 2.26778725\n",
            "Iteration 99, loss = 2.26773430\n",
            "Iteration 100, loss = 2.26767535\n",
            "[CV] END activation=relu, alpha=0.05, hidden_layer_sizes=(24, 24), learning_rate=adaptive, max_iter=100, solver=sgd, validation_fraction=0.3, verbose=True; total time= 1.3min\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  ConvergenceWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1, loss = 2.31921070\n",
            "Iteration 2, loss = 2.30334894\n",
            "Iteration 3, loss = 2.30086499\n",
            "Iteration 4, loss = 2.29873027\n",
            "Iteration 5, loss = 2.29652436\n",
            "Iteration 6, loss = 2.29419189\n",
            "Iteration 7, loss = 2.29139303\n",
            "Iteration 8, loss = 2.28885267\n",
            "Iteration 9, loss = 2.28707736\n",
            "Iteration 10, loss = 2.28562180\n",
            "Iteration 11, loss = 2.28445173\n",
            "Iteration 12, loss = 2.28353976\n",
            "Iteration 13, loss = 2.28280563\n",
            "Iteration 14, loss = 2.28221770\n",
            "Iteration 15, loss = 2.28173606\n",
            "Iteration 16, loss = 2.28132398\n",
            "Iteration 17, loss = 2.28096565\n",
            "Iteration 18, loss = 2.28062618\n",
            "Iteration 19, loss = 2.28032172\n",
            "Iteration 20, loss = 2.28001937\n",
            "Iteration 21, loss = 2.27969910\n",
            "Iteration 22, loss = 2.27938850\n",
            "Iteration 23, loss = 2.27907139\n",
            "Iteration 24, loss = 2.27873829\n",
            "Iteration 25, loss = 2.27845202\n",
            "Iteration 26, loss = 2.27819571\n",
            "Iteration 27, loss = 2.27794338\n",
            "Iteration 28, loss = 2.27771973\n",
            "Iteration 29, loss = 2.27748160\n",
            "Iteration 30, loss = 2.27725456\n",
            "Iteration 31, loss = 2.27700350\n",
            "Iteration 32, loss = 2.27679536\n",
            "Iteration 33, loss = 2.27657807\n",
            "Iteration 34, loss = 2.27635283\n",
            "Iteration 35, loss = 2.27615048\n",
            "Iteration 36, loss = 2.27592339\n",
            "Iteration 37, loss = 2.27572691\n",
            "Iteration 38, loss = 2.27552192\n",
            "Iteration 39, loss = 2.27533108\n",
            "Iteration 40, loss = 2.27512812\n",
            "Iteration 41, loss = 2.27495722\n",
            "Iteration 42, loss = 2.27477010\n",
            "Iteration 43, loss = 2.27459833\n",
            "Iteration 44, loss = 2.27440617\n",
            "Iteration 45, loss = 2.27424122\n",
            "Iteration 46, loss = 2.27408158\n",
            "Iteration 47, loss = 2.27390645\n",
            "Iteration 48, loss = 2.27373838\n",
            "Iteration 49, loss = 2.27356926\n",
            "Iteration 50, loss = 2.27341345\n",
            "Iteration 51, loss = 2.27324060\n",
            "Iteration 52, loss = 2.27311544\n",
            "Iteration 53, loss = 2.27295702\n",
            "Iteration 54, loss = 2.27282552\n",
            "Iteration 55, loss = 2.27269590\n",
            "Iteration 56, loss = 2.27254899\n",
            "Iteration 57, loss = 2.27240772\n",
            "Iteration 58, loss = 2.27228553\n",
            "Iteration 59, loss = 2.27216302\n",
            "Iteration 60, loss = 2.27206007\n",
            "Iteration 61, loss = 2.27193587\n",
            "Iteration 62, loss = 2.27182741\n",
            "Iteration 63, loss = 2.27171432\n",
            "Iteration 64, loss = 2.27160462\n",
            "Iteration 65, loss = 2.27149456\n",
            "Iteration 66, loss = 2.27139000\n",
            "Iteration 67, loss = 2.27129310\n",
            "Iteration 68, loss = 2.27118941\n",
            "Iteration 69, loss = 2.27110293\n",
            "Iteration 70, loss = 2.27098931\n",
            "Iteration 71, loss = 2.27092307\n",
            "Iteration 72, loss = 2.27082172\n",
            "Iteration 73, loss = 2.27074359\n",
            "Iteration 74, loss = 2.27065503\n",
            "Iteration 75, loss = 2.27057849\n",
            "Iteration 76, loss = 2.27047841\n",
            "Iteration 77, loss = 2.27043056\n",
            "Iteration 78, loss = 2.27034828\n",
            "Iteration 79, loss = 2.27026527\n",
            "Iteration 80, loss = 2.27019021\n",
            "Iteration 81, loss = 2.27013881\n",
            "Iteration 82, loss = 2.27005466\n",
            "Iteration 83, loss = 2.27000022\n",
            "Iteration 84, loss = 2.26992137\n",
            "Iteration 85, loss = 2.26986650\n",
            "Iteration 86, loss = 2.26980977\n",
            "Iteration 87, loss = 2.26972198\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
            "Iteration 88, loss = 2.26962417\n",
            "Iteration 89, loss = 2.26960361\n",
            "Iteration 90, loss = 2.26958488\n",
            "Iteration 91, loss = 2.26958036\n",
            "Iteration 92, loss = 2.26956558\n",
            "Iteration 93, loss = 2.26955295\n",
            "Iteration 94, loss = 2.26954351\n",
            "Iteration 95, loss = 2.26953089\n",
            "Iteration 96, loss = 2.26951855\n",
            "Iteration 97, loss = 2.26950719\n",
            "Iteration 98, loss = 2.26948939\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000040\n",
            "Iteration 99, loss = 2.26947404\n",
            "Iteration 100, loss = 2.26946610\n",
            "[CV] END activation=relu, alpha=0.05, hidden_layer_sizes=(24, 24), learning_rate=adaptive, max_iter=100, solver=sgd, validation_fraction=0.3, verbose=True; total time= 1.3min\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  ConvergenceWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1, loss = 2.30939101\n",
            "Iteration 2, loss = 2.29742308\n",
            "Iteration 3, loss = 2.29318340\n",
            "Iteration 4, loss = 2.28997919\n",
            "Iteration 5, loss = 2.28734934\n",
            "Iteration 6, loss = 2.28519448\n",
            "Iteration 7, loss = 2.28345015\n",
            "Iteration 8, loss = 2.28212497\n",
            "Iteration 9, loss = 2.28112955\n",
            "Iteration 10, loss = 2.28040354\n",
            "Iteration 11, loss = 2.27985469\n",
            "Iteration 12, loss = 2.27943091\n",
            "Iteration 13, loss = 2.27910593\n",
            "Iteration 14, loss = 2.27883435\n",
            "Iteration 15, loss = 2.27859280\n",
            "Iteration 16, loss = 2.27837674\n",
            "Iteration 17, loss = 2.27819779\n",
            "Iteration 18, loss = 2.27802913\n",
            "Iteration 19, loss = 2.27786742\n",
            "Iteration 20, loss = 2.27771797\n",
            "Iteration 21, loss = 2.27756086\n",
            "Iteration 22, loss = 2.27739204\n",
            "Iteration 23, loss = 2.27716999\n",
            "Iteration 24, loss = 2.27691341\n",
            "Iteration 25, loss = 2.27673996\n",
            "Iteration 26, loss = 2.27659104\n",
            "Iteration 27, loss = 2.27641887\n",
            "Iteration 28, loss = 2.27626527\n",
            "Iteration 29, loss = 2.27609587\n",
            "Iteration 30, loss = 2.27595375\n",
            "Iteration 31, loss = 2.27578228\n",
            "Iteration 32, loss = 2.27562090\n",
            "Iteration 33, loss = 2.27547355\n",
            "Iteration 34, loss = 2.27530444\n",
            "Iteration 35, loss = 2.27511823\n",
            "Iteration 36, loss = 2.27494932\n",
            "Iteration 37, loss = 2.27477226\n",
            "Iteration 38, loss = 2.27457859\n",
            "Iteration 39, loss = 2.27439239\n",
            "Iteration 40, loss = 2.27420787\n",
            "Iteration 41, loss = 2.27399912\n",
            "Iteration 42, loss = 2.27379999\n",
            "Iteration 43, loss = 2.27359076\n",
            "Iteration 44, loss = 2.27337644\n",
            "Iteration 45, loss = 2.27318391\n",
            "Iteration 46, loss = 2.27295654\n",
            "Iteration 47, loss = 2.27280035\n",
            "Iteration 48, loss = 2.27256693\n",
            "Iteration 49, loss = 2.27237016\n",
            "Iteration 50, loss = 2.27218240\n",
            "Iteration 51, loss = 2.27197193\n",
            "Iteration 52, loss = 2.27175675\n",
            "Iteration 53, loss = 2.27155427\n",
            "Iteration 54, loss = 2.27136818\n",
            "Iteration 55, loss = 2.27115853\n",
            "Iteration 56, loss = 2.27097782\n",
            "Iteration 57, loss = 2.27078719\n",
            "Iteration 58, loss = 2.27058487\n",
            "Iteration 59, loss = 2.27040722\n",
            "Iteration 60, loss = 2.27022104\n",
            "Iteration 61, loss = 2.27003437\n",
            "Iteration 62, loss = 2.26982780\n",
            "Iteration 63, loss = 2.26965357\n",
            "Iteration 64, loss = 2.26946931\n",
            "Iteration 65, loss = 2.26930890\n",
            "Iteration 66, loss = 2.26911273\n",
            "Iteration 67, loss = 2.26895721\n",
            "Iteration 68, loss = 2.26880179\n",
            "Iteration 69, loss = 2.26863170\n",
            "Iteration 70, loss = 2.26847032\n",
            "Iteration 71, loss = 2.26834364\n",
            "Iteration 72, loss = 2.26821296\n",
            "Iteration 73, loss = 2.26807022\n",
            "Iteration 74, loss = 2.26796244\n",
            "Iteration 75, loss = 2.26783398\n",
            "Iteration 76, loss = 2.26771293\n",
            "Iteration 77, loss = 2.26760800\n",
            "Iteration 78, loss = 2.26751339\n",
            "Iteration 79, loss = 2.26739615\n",
            "Iteration 80, loss = 2.26730468\n",
            "Iteration 81, loss = 2.26723707\n",
            "Iteration 82, loss = 2.26717164\n",
            "Iteration 83, loss = 2.26706587\n",
            "Iteration 84, loss = 2.26699158\n",
            "Iteration 85, loss = 2.26692039\n",
            "Iteration 86, loss = 2.26684572\n",
            "Iteration 87, loss = 2.26681090\n",
            "Iteration 88, loss = 2.26673101\n",
            "Iteration 89, loss = 2.26669194\n",
            "Iteration 90, loss = 2.26661218\n",
            "Iteration 91, loss = 2.26657037\n",
            "Iteration 92, loss = 2.26652040\n",
            "Iteration 93, loss = 2.26646182\n",
            "Iteration 94, loss = 2.26640745\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
            "Iteration 95, loss = 2.26631848\n",
            "Iteration 96, loss = 2.26629369\n",
            "Iteration 97, loss = 2.26627931\n",
            "Iteration 98, loss = 2.26627103\n",
            "Iteration 99, loss = 2.26626268\n",
            "Iteration 100, loss = 2.26625321\n",
            "[CV] END activation=relu, alpha=0.05, hidden_layer_sizes=(24, 24), learning_rate=adaptive, max_iter=100, solver=sgd, validation_fraction=0.3, verbose=True; total time= 1.2min\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  ConvergenceWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1, loss = 2.31283154\n",
            "Iteration 2, loss = 2.30149019\n",
            "Iteration 3, loss = 2.29947074\n",
            "Iteration 4, loss = 2.29754079\n",
            "Iteration 5, loss = 2.29553800\n",
            "Iteration 6, loss = 2.29350203\n",
            "Iteration 7, loss = 2.29150618\n",
            "Iteration 8, loss = 2.28960213\n",
            "Iteration 9, loss = 2.28780146\n",
            "Iteration 10, loss = 2.28618080\n",
            "Iteration 11, loss = 2.28472964\n",
            "Iteration 12, loss = 2.28347607\n",
            "Iteration 13, loss = 2.28239467\n",
            "Iteration 14, loss = 2.28145788\n",
            "Iteration 15, loss = 2.28068138\n",
            "Iteration 16, loss = 2.27998837\n",
            "Iteration 17, loss = 2.27941454\n",
            "Iteration 18, loss = 2.27887786\n",
            "Iteration 19, loss = 2.27842507\n",
            "Iteration 20, loss = 2.27801572\n",
            "Iteration 21, loss = 2.27764873\n",
            "Iteration 22, loss = 2.27729912\n",
            "Iteration 23, loss = 2.27700569\n",
            "Iteration 24, loss = 2.27670677\n",
            "Iteration 25, loss = 2.27642845\n",
            "Iteration 26, loss = 2.27617877\n",
            "Iteration 27, loss = 2.27591896\n",
            "Iteration 28, loss = 2.27567921\n",
            "Iteration 29, loss = 2.27544905\n",
            "Iteration 30, loss = 2.27521329\n",
            "Iteration 31, loss = 2.27497939\n",
            "Iteration 32, loss = 2.27473867\n",
            "Iteration 33, loss = 2.27449820\n",
            "Iteration 34, loss = 2.27425687\n",
            "Iteration 35, loss = 2.27400259\n",
            "Iteration 36, loss = 2.27376530\n",
            "Iteration 37, loss = 2.27352342\n",
            "Iteration 38, loss = 2.27329538\n",
            "Iteration 39, loss = 2.27307261\n",
            "Iteration 40, loss = 2.27286442\n",
            "Iteration 41, loss = 2.27265337\n",
            "Iteration 42, loss = 2.27246437\n",
            "Iteration 43, loss = 2.27227009\n",
            "Iteration 44, loss = 2.27208523\n",
            "Iteration 45, loss = 2.27190621\n",
            "Iteration 46, loss = 2.27173354\n",
            "Iteration 47, loss = 2.27156212\n",
            "Iteration 48, loss = 2.27137858\n",
            "Iteration 49, loss = 2.27122110\n",
            "Iteration 50, loss = 2.27106142\n",
            "Iteration 51, loss = 2.27089822\n",
            "Iteration 52, loss = 2.27074169\n",
            "Iteration 53, loss = 2.27060402\n",
            "Iteration 54, loss = 2.27044575\n",
            "Iteration 55, loss = 2.27029680\n",
            "Iteration 56, loss = 2.27015223\n",
            "Iteration 57, loss = 2.26999575\n",
            "Iteration 58, loss = 2.26985085\n",
            "Iteration 59, loss = 2.26970827\n",
            "Iteration 60, loss = 2.26955854\n",
            "Iteration 61, loss = 2.26941277\n",
            "Iteration 62, loss = 2.26925696\n",
            "Iteration 63, loss = 2.26912428\n",
            "Iteration 64, loss = 2.26896844\n",
            "Iteration 65, loss = 2.26885671\n",
            "Iteration 66, loss = 2.26868919\n",
            "Iteration 67, loss = 2.26857746\n",
            "Iteration 68, loss = 2.26845762\n",
            "Iteration 69, loss = 2.26832762\n",
            "Iteration 70, loss = 2.26818136\n",
            "Iteration 71, loss = 2.26807211\n",
            "Iteration 72, loss = 2.26794320\n",
            "Iteration 73, loss = 2.26782897\n",
            "Iteration 74, loss = 2.26769012\n",
            "Iteration 75, loss = 2.26756838\n",
            "Iteration 76, loss = 2.26746214\n",
            "Iteration 77, loss = 2.26733111\n",
            "Iteration 78, loss = 2.26721604\n",
            "Iteration 79, loss = 2.26707311\n",
            "Iteration 80, loss = 2.26699304\n",
            "Iteration 81, loss = 2.26683849\n",
            "Iteration 82, loss = 2.26674631\n",
            "Iteration 83, loss = 2.26663985\n",
            "Iteration 84, loss = 2.26650402\n",
            "Iteration 85, loss = 2.26642134\n",
            "Iteration 86, loss = 2.26629737\n",
            "Iteration 87, loss = 2.26620891\n",
            "Iteration 88, loss = 2.26609838\n",
            "Iteration 89, loss = 2.26601580\n",
            "Iteration 90, loss = 2.26593100\n",
            "Iteration 91, loss = 2.26583822\n",
            "Iteration 92, loss = 2.26575955\n",
            "Iteration 93, loss = 2.26568319\n",
            "Iteration 94, loss = 2.26560641\n",
            "Iteration 95, loss = 2.26553683\n",
            "Iteration 96, loss = 2.26545298\n",
            "Iteration 97, loss = 2.26538078\n",
            "Iteration 98, loss = 2.26532511\n",
            "Iteration 99, loss = 2.26524345\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
            "Iteration 100, loss = 2.26515011\n",
            "[CV] END activation=relu, alpha=0.05, hidden_layer_sizes=(24, 24), learning_rate=adaptive, max_iter=100, solver=sgd, validation_fraction=0.3, verbose=True; total time= 1.2min\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  ConvergenceWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1, loss = 2.31737464\n",
            "Iteration 2, loss = 2.30364529\n",
            "Iteration 3, loss = 2.30188639\n",
            "Iteration 4, loss = 2.30065495\n",
            "Iteration 5, loss = 2.29962070\n",
            "Iteration 6, loss = 2.29869917\n",
            "Iteration 7, loss = 2.29774740\n",
            "Iteration 8, loss = 2.29684831\n",
            "Iteration 9, loss = 2.29589820\n",
            "Iteration 10, loss = 2.29489191\n",
            "Iteration 11, loss = 2.29387904\n",
            "Iteration 12, loss = 2.29282939\n",
            "Iteration 13, loss = 2.29175492\n",
            "Iteration 14, loss = 2.29065259\n",
            "Iteration 15, loss = 2.28958395\n",
            "Iteration 16, loss = 2.28845560\n",
            "Iteration 17, loss = 2.28746384\n",
            "Iteration 18, loss = 2.28650661\n",
            "Iteration 19, loss = 2.28561998\n",
            "Iteration 20, loss = 2.28484360\n",
            "Iteration 21, loss = 2.28412484\n",
            "Iteration 22, loss = 2.28355038\n",
            "Iteration 23, loss = 2.28301580\n",
            "Iteration 24, loss = 2.28259395\n",
            "Iteration 25, loss = 2.28222675\n",
            "Iteration 26, loss = 2.28192567\n",
            "Iteration 27, loss = 2.28168962\n",
            "Iteration 28, loss = 2.28146439\n",
            "Iteration 29, loss = 2.28130549\n",
            "Iteration 30, loss = 2.28114408\n",
            "Iteration 31, loss = 2.28104279\n",
            "Iteration 32, loss = 2.28090343\n",
            "Iteration 33, loss = 2.28082372\n",
            "Iteration 34, loss = 2.28072202\n",
            "Iteration 35, loss = 2.28065332\n",
            "Iteration 36, loss = 2.28059661\n",
            "Iteration 37, loss = 2.28053769\n",
            "Iteration 38, loss = 2.28049927\n",
            "Iteration 39, loss = 2.28042813\n",
            "Iteration 40, loss = 2.28038311\n",
            "Iteration 41, loss = 2.28036021\n",
            "Iteration 42, loss = 2.28032960\n",
            "Iteration 43, loss = 2.28025086\n",
            "Iteration 44, loss = 2.28024266\n",
            "Iteration 45, loss = 2.28018930\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "[CV] END activation=tanh, alpha=0.05, hidden_layer_sizes=(24, 24), learning_rate=constant, max_iter=100, solver=sgd, validation_fraction=0.3, verbose=True; total time=  41.1s\n",
            "Iteration 1, loss = 2.30811951\n",
            "Iteration 2, loss = 2.30264532\n",
            "Iteration 3, loss = 2.30110404\n",
            "Iteration 4, loss = 2.29995083\n",
            "Iteration 5, loss = 2.29894730\n",
            "Iteration 6, loss = 2.29799925\n",
            "Iteration 7, loss = 2.29699125\n",
            "Iteration 8, loss = 2.29597350\n",
            "Iteration 9, loss = 2.29484630\n",
            "Iteration 10, loss = 2.29368029\n",
            "Iteration 11, loss = 2.29244889\n",
            "Iteration 12, loss = 2.29119252\n",
            "Iteration 13, loss = 2.28991588\n",
            "Iteration 14, loss = 2.28868320\n",
            "Iteration 15, loss = 2.28750573\n",
            "Iteration 16, loss = 2.28641823\n",
            "Iteration 17, loss = 2.28545037\n",
            "Iteration 18, loss = 2.28457896\n",
            "Iteration 19, loss = 2.28385580\n",
            "Iteration 20, loss = 2.28320178\n",
            "Iteration 21, loss = 2.28265935\n",
            "Iteration 22, loss = 2.28222845\n",
            "Iteration 23, loss = 2.28182835\n",
            "Iteration 24, loss = 2.28154358\n",
            "Iteration 25, loss = 2.28127511\n",
            "Iteration 26, loss = 2.28105047\n",
            "Iteration 27, loss = 2.28086239\n",
            "Iteration 28, loss = 2.28070873\n",
            "Iteration 29, loss = 2.28059973\n",
            "Iteration 30, loss = 2.28048176\n",
            "Iteration 31, loss = 2.28040969\n",
            "Iteration 32, loss = 2.28031080\n",
            "Iteration 33, loss = 2.28024533\n",
            "Iteration 34, loss = 2.28016233\n",
            "Iteration 35, loss = 2.28010399\n",
            "Iteration 36, loss = 2.28004126\n",
            "Iteration 37, loss = 2.27997995\n",
            "Iteration 38, loss = 2.27995312\n",
            "Iteration 39, loss = 2.27989184\n",
            "Iteration 40, loss = 2.27985074\n",
            "Iteration 41, loss = 2.27978167\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "[CV] END activation=tanh, alpha=0.05, hidden_layer_sizes=(24, 24), learning_rate=constant, max_iter=100, solver=sgd, validation_fraction=0.3, verbose=True; total time=  37.5s\n",
            "Iteration 1, loss = 2.31360698\n",
            "Iteration 2, loss = 2.30106586\n",
            "Iteration 3, loss = 2.29946100\n",
            "Iteration 4, loss = 2.29825262\n",
            "Iteration 5, loss = 2.29709433\n",
            "Iteration 6, loss = 2.29599989\n",
            "Iteration 7, loss = 2.29483726\n",
            "Iteration 8, loss = 2.29365350\n",
            "Iteration 9, loss = 2.29249881\n",
            "Iteration 10, loss = 2.29132824\n",
            "Iteration 11, loss = 2.29017526\n",
            "Iteration 12, loss = 2.28909929\n",
            "Iteration 13, loss = 2.28809380\n",
            "Iteration 14, loss = 2.28722195\n",
            "Iteration 15, loss = 2.28638199\n",
            "Iteration 16, loss = 2.28564620\n",
            "Iteration 17, loss = 2.28502705\n",
            "Iteration 18, loss = 2.28447816\n",
            "Iteration 19, loss = 2.28400658\n",
            "Iteration 20, loss = 2.28360339\n",
            "Iteration 21, loss = 2.28324196\n",
            "Iteration 22, loss = 2.28295462\n",
            "Iteration 23, loss = 2.28272003\n",
            "Iteration 24, loss = 2.28252191\n",
            "Iteration 25, loss = 2.28234219\n",
            "Iteration 26, loss = 2.28217596\n",
            "Iteration 27, loss = 2.28204301\n",
            "Iteration 28, loss = 2.28191623\n",
            "Iteration 29, loss = 2.28182987\n",
            "Iteration 30, loss = 2.28169544\n",
            "Iteration 31, loss = 2.28162102\n",
            "Iteration 32, loss = 2.28157115\n",
            "Iteration 33, loss = 2.28148119\n",
            "Iteration 34, loss = 2.28143675\n",
            "Iteration 35, loss = 2.28136050\n",
            "Iteration 36, loss = 2.28128952\n",
            "Iteration 37, loss = 2.28125787\n",
            "Iteration 38, loss = 2.28121322\n",
            "Iteration 39, loss = 2.28117047\n",
            "Iteration 40, loss = 2.28111057\n",
            "Iteration 41, loss = 2.28104669\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "[CV] END activation=tanh, alpha=0.05, hidden_layer_sizes=(24, 24), learning_rate=constant, max_iter=100, solver=sgd, validation_fraction=0.3, verbose=True; total time=  37.6s\n",
            "Iteration 1, loss = 2.31977876\n",
            "Iteration 2, loss = 2.30405177\n",
            "Iteration 3, loss = 2.30174806\n",
            "Iteration 4, loss = 2.30020624\n",
            "Iteration 5, loss = 2.29898700\n",
            "Iteration 6, loss = 2.29780510\n",
            "Iteration 7, loss = 2.29663838\n",
            "Iteration 8, loss = 2.29540976\n",
            "Iteration 9, loss = 2.29413932\n",
            "Iteration 10, loss = 2.29280869\n",
            "Iteration 11, loss = 2.29144049\n",
            "Iteration 12, loss = 2.29006385\n",
            "Iteration 13, loss = 2.28871086\n",
            "Iteration 14, loss = 2.28746503\n",
            "Iteration 15, loss = 2.28627491\n",
            "Iteration 16, loss = 2.28522882\n",
            "Iteration 17, loss = 2.28427901\n",
            "Iteration 18, loss = 2.28346783\n",
            "Iteration 19, loss = 2.28275436\n",
            "Iteration 20, loss = 2.28217301\n",
            "Iteration 21, loss = 2.28169882\n",
            "Iteration 22, loss = 2.28128853\n",
            "Iteration 23, loss = 2.28095388\n",
            "Iteration 24, loss = 2.28066705\n",
            "Iteration 25, loss = 2.28043692\n",
            "Iteration 26, loss = 2.28025304\n",
            "Iteration 27, loss = 2.28009943\n",
            "Iteration 28, loss = 2.27993154\n",
            "Iteration 29, loss = 2.27980761\n",
            "Iteration 30, loss = 2.27974462\n",
            "Iteration 31, loss = 2.27963350\n",
            "Iteration 32, loss = 2.27955430\n",
            "Iteration 33, loss = 2.27949350\n",
            "Iteration 34, loss = 2.27941084\n",
            "Iteration 35, loss = 2.27936290\n",
            "Iteration 36, loss = 2.27929472\n",
            "Iteration 37, loss = 2.27922124\n",
            "Iteration 38, loss = 2.27919984\n",
            "Iteration 39, loss = 2.27913127\n",
            "Iteration 40, loss = 2.27908084\n",
            "Iteration 41, loss = 2.27904523\n",
            "Iteration 42, loss = 2.27900643\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "[CV] END activation=tanh, alpha=0.05, hidden_layer_sizes=(24, 24), learning_rate=constant, max_iter=100, solver=sgd, validation_fraction=0.3, verbose=True; total time=  38.1s\n",
            "Iteration 1, loss = 2.32064277\n",
            "Iteration 2, loss = 2.30178482\n",
            "Iteration 3, loss = 2.29957724\n",
            "Iteration 4, loss = 2.29828862\n",
            "Iteration 5, loss = 2.29714010\n",
            "Iteration 6, loss = 2.29604408\n",
            "Iteration 7, loss = 2.29486502\n",
            "Iteration 8, loss = 2.29360535\n",
            "Iteration 9, loss = 2.29231015\n",
            "Iteration 10, loss = 2.29097602\n",
            "Iteration 11, loss = 2.28963768\n",
            "Iteration 12, loss = 2.28836739\n",
            "Iteration 13, loss = 2.28715723\n",
            "Iteration 14, loss = 2.28606957\n",
            "Iteration 15, loss = 2.28512029\n",
            "Iteration 16, loss = 2.28428354\n",
            "Iteration 17, loss = 2.28356441\n",
            "Iteration 18, loss = 2.28293922\n",
            "Iteration 19, loss = 2.28246619\n",
            "Iteration 20, loss = 2.28200812\n",
            "Iteration 21, loss = 2.28167431\n",
            "Iteration 22, loss = 2.28133972\n",
            "Iteration 23, loss = 2.28110184\n",
            "Iteration 24, loss = 2.28087391\n",
            "Iteration 25, loss = 2.28067330\n",
            "Iteration 26, loss = 2.28052803\n",
            "Iteration 27, loss = 2.28039570\n",
            "Iteration 28, loss = 2.28024865\n",
            "Iteration 29, loss = 2.28011412\n",
            "Iteration 30, loss = 2.28000612\n",
            "Iteration 31, loss = 2.27993091\n",
            "Iteration 32, loss = 2.27984682\n",
            "Iteration 33, loss = 2.27975072\n",
            "Iteration 34, loss = 2.27967304\n",
            "Iteration 35, loss = 2.27961848\n",
            "Iteration 36, loss = 2.27952344\n",
            "Iteration 37, loss = 2.27948631\n",
            "Iteration 38, loss = 2.27940640\n",
            "Iteration 39, loss = 2.27934660\n",
            "Iteration 40, loss = 2.27925996\n",
            "Iteration 41, loss = 2.27921116\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "[CV] END activation=tanh, alpha=0.05, hidden_layer_sizes=(24, 24), learning_rate=constant, max_iter=100, solver=sgd, validation_fraction=0.3, verbose=True; total time=  37.7s\n",
            "Iteration 1, loss = 2.31844233\n",
            "Iteration 2, loss = 2.30439518\n",
            "Iteration 3, loss = 2.30122015\n",
            "Iteration 4, loss = 2.29818153\n",
            "Iteration 5, loss = 2.29551032\n",
            "Iteration 6, loss = 2.29302974\n",
            "Iteration 7, loss = 2.29065717\n",
            "Iteration 8, loss = 2.28856795\n",
            "Iteration 9, loss = 2.28684129\n",
            "Iteration 10, loss = 2.28540393\n",
            "Iteration 11, loss = 2.28427392\n",
            "Iteration 12, loss = 2.28338254\n",
            "Iteration 13, loss = 2.28266717\n",
            "Iteration 14, loss = 2.28210910\n",
            "Iteration 15, loss = 2.28168291\n",
            "Iteration 16, loss = 2.28132980\n",
            "Iteration 17, loss = 2.28102829\n",
            "Iteration 18, loss = 2.28074798\n",
            "Iteration 19, loss = 2.28048623\n",
            "Iteration 20, loss = 2.28025177\n",
            "Iteration 21, loss = 2.28003899\n",
            "Iteration 22, loss = 2.27979481\n",
            "Iteration 23, loss = 2.27959779\n",
            "Iteration 24, loss = 2.27938800\n",
            "Iteration 25, loss = 2.27918568\n",
            "Iteration 26, loss = 2.27899048\n",
            "Iteration 27, loss = 2.27878980\n",
            "Iteration 28, loss = 2.27858881\n",
            "Iteration 29, loss = 2.27839317\n",
            "Iteration 30, loss = 2.27820518\n",
            "Iteration 31, loss = 2.27801186\n",
            "Iteration 32, loss = 2.27781109\n",
            "Iteration 33, loss = 2.27764313\n",
            "Iteration 34, loss = 2.27745446\n",
            "Iteration 35, loss = 2.27724703\n",
            "Iteration 36, loss = 2.27707592\n",
            "Iteration 37, loss = 2.27689883\n",
            "Iteration 38, loss = 2.27671389\n",
            "Iteration 39, loss = 2.27653827\n",
            "Iteration 40, loss = 2.27635439\n",
            "Iteration 41, loss = 2.27620392\n",
            "Iteration 42, loss = 2.27601971\n",
            "Iteration 43, loss = 2.27584792\n",
            "Iteration 44, loss = 2.27567784\n",
            "Iteration 45, loss = 2.27550584\n",
            "Iteration 46, loss = 2.27535920\n",
            "Iteration 47, loss = 2.27519127\n",
            "Iteration 48, loss = 2.27503256\n",
            "Iteration 49, loss = 2.27488379\n",
            "Iteration 50, loss = 2.27470550\n",
            "Iteration 51, loss = 2.27456480\n",
            "Iteration 52, loss = 2.27443230\n",
            "Iteration 53, loss = 2.27429737\n",
            "Iteration 54, loss = 2.27415764\n",
            "Iteration 55, loss = 2.27398217\n",
            "Iteration 56, loss = 2.27388266\n",
            "Iteration 57, loss = 2.27371156\n",
            "Iteration 58, loss = 2.27358607\n",
            "Iteration 59, loss = 2.27345065\n",
            "Iteration 60, loss = 2.27331123\n",
            "Iteration 61, loss = 2.27319770\n",
            "Iteration 62, loss = 2.27304980\n",
            "Iteration 63, loss = 2.27292707\n",
            "Iteration 64, loss = 2.27281359\n",
            "Iteration 65, loss = 2.27269564\n",
            "Iteration 66, loss = 2.27255912\n",
            "Iteration 67, loss = 2.27245966\n",
            "Iteration 68, loss = 2.27232542\n",
            "Iteration 69, loss = 2.27221168\n",
            "Iteration 70, loss = 2.27208361\n",
            "Iteration 71, loss = 2.27198088\n",
            "Iteration 72, loss = 2.27186563\n",
            "Iteration 73, loss = 2.27175702\n",
            "Iteration 74, loss = 2.27163575\n",
            "Iteration 75, loss = 2.27152595\n",
            "Iteration 76, loss = 2.27145000\n",
            "Iteration 77, loss = 2.27134799\n",
            "Iteration 78, loss = 2.27122261\n",
            "Iteration 79, loss = 2.27110860\n",
            "Iteration 80, loss = 2.27102588\n",
            "Iteration 81, loss = 2.27092898\n",
            "Iteration 82, loss = 2.27087572\n",
            "Iteration 83, loss = 2.27076823\n",
            "Iteration 84, loss = 2.27065865\n",
            "Iteration 85, loss = 2.27056511\n",
            "Iteration 86, loss = 2.27050373\n",
            "Iteration 87, loss = 2.27042154\n",
            "Iteration 88, loss = 2.27033697\n",
            "Iteration 89, loss = 2.27025511\n",
            "Iteration 90, loss = 2.27017414\n",
            "Iteration 91, loss = 2.27008381\n",
            "Iteration 92, loss = 2.27000593\n",
            "Iteration 93, loss = 2.26994915\n",
            "Iteration 94, loss = 2.26985021\n",
            "Iteration 95, loss = 2.26979231\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "[CV] END activation=relu, alpha=0.05, hidden_layer_sizes=(32, 32), learning_rate=constant, max_iter=100, solver=sgd, validation_fraction=0.3, verbose=True; total time= 1.3min\n",
            "Iteration 1, loss = 2.31114742\n",
            "Iteration 2, loss = 2.30046489\n",
            "Iteration 3, loss = 2.29812873\n",
            "Iteration 4, loss = 2.29570113\n",
            "Iteration 5, loss = 2.29327089\n",
            "Iteration 6, loss = 2.29092258\n",
            "Iteration 7, loss = 2.28869952\n",
            "Iteration 8, loss = 2.28672883\n",
            "Iteration 9, loss = 2.28504177\n",
            "Iteration 10, loss = 2.28369343\n",
            "Iteration 11, loss = 2.28261344\n",
            "Iteration 12, loss = 2.28177563\n",
            "Iteration 13, loss = 2.28109751\n",
            "Iteration 14, loss = 2.28053248\n",
            "Iteration 15, loss = 2.28004141\n",
            "Iteration 16, loss = 2.27966821\n",
            "Iteration 17, loss = 2.27930266\n",
            "Iteration 18, loss = 2.27897624\n",
            "Iteration 19, loss = 2.27866362\n",
            "Iteration 20, loss = 2.27835614\n",
            "Iteration 21, loss = 2.27807443\n",
            "Iteration 22, loss = 2.27778590\n",
            "Iteration 23, loss = 2.27750482\n",
            "Iteration 24, loss = 2.27723307\n",
            "Iteration 25, loss = 2.27696057\n",
            "Iteration 26, loss = 2.27670288\n",
            "Iteration 27, loss = 2.27643659\n",
            "Iteration 28, loss = 2.27617364\n",
            "Iteration 29, loss = 2.27592240\n",
            "Iteration 30, loss = 2.27567180\n",
            "Iteration 31, loss = 2.27542330\n",
            "Iteration 32, loss = 2.27516863\n",
            "Iteration 33, loss = 2.27490255\n",
            "Iteration 34, loss = 2.27467687\n",
            "Iteration 35, loss = 2.27444148\n",
            "Iteration 36, loss = 2.27420877\n",
            "Iteration 37, loss = 2.27398206\n",
            "Iteration 38, loss = 2.27377074\n",
            "Iteration 39, loss = 2.27352695\n",
            "Iteration 40, loss = 2.27331872\n",
            "Iteration 41, loss = 2.27308937\n",
            "Iteration 42, loss = 2.27287243\n",
            "Iteration 43, loss = 2.27267059\n",
            "Iteration 44, loss = 2.27246603\n",
            "Iteration 45, loss = 2.27229521\n",
            "Iteration 46, loss = 2.27210176\n",
            "Iteration 47, loss = 2.27191184\n",
            "Iteration 48, loss = 2.27176205\n",
            "Iteration 49, loss = 2.27157134\n",
            "Iteration 50, loss = 2.27139582\n",
            "Iteration 51, loss = 2.27121864\n",
            "Iteration 52, loss = 2.27103000\n",
            "Iteration 53, loss = 2.27084857\n",
            "Iteration 54, loss = 2.27064218\n",
            "Iteration 55, loss = 2.27044580\n",
            "Iteration 56, loss = 2.27027505\n",
            "Iteration 57, loss = 2.27009248\n",
            "Iteration 58, loss = 2.26992458\n",
            "Iteration 59, loss = 2.26974119\n",
            "Iteration 60, loss = 2.26956668\n",
            "Iteration 61, loss = 2.26940629\n",
            "Iteration 62, loss = 2.26927049\n",
            "Iteration 63, loss = 2.26912173\n",
            "Iteration 64, loss = 2.26899780\n",
            "Iteration 65, loss = 2.26882909\n",
            "Iteration 66, loss = 2.26873136\n",
            "Iteration 67, loss = 2.26860644\n",
            "Iteration 68, loss = 2.26849236\n",
            "Iteration 69, loss = 2.26837573\n",
            "Iteration 70, loss = 2.26825222\n",
            "Iteration 71, loss = 2.26814808\n",
            "Iteration 72, loss = 2.26804862\n",
            "Iteration 73, loss = 2.26797112\n",
            "Iteration 74, loss = 2.26784780\n",
            "Iteration 75, loss = 2.26777798\n",
            "Iteration 76, loss = 2.26767733\n",
            "Iteration 77, loss = 2.26757010\n",
            "Iteration 78, loss = 2.26748570\n",
            "Iteration 79, loss = 2.26740433\n",
            "Iteration 80, loss = 2.26732059\n",
            "Iteration 81, loss = 2.26724131\n",
            "Iteration 82, loss = 2.26717560\n",
            "Iteration 83, loss = 2.26709317\n",
            "Iteration 84, loss = 2.26703763\n",
            "Iteration 85, loss = 2.26693932\n",
            "Iteration 86, loss = 2.26690601\n",
            "Iteration 87, loss = 2.26683938\n",
            "Iteration 88, loss = 2.26675832\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "[CV] END activation=relu, alpha=0.05, hidden_layer_sizes=(32, 32), learning_rate=constant, max_iter=100, solver=sgd, validation_fraction=0.3, verbose=True; total time= 1.2min\n",
            "Iteration 1, loss = 2.31108747\n",
            "Iteration 2, loss = 2.30167087\n",
            "Iteration 3, loss = 2.29890304\n",
            "Iteration 4, loss = 2.29645736\n",
            "Iteration 5, loss = 2.29417198\n",
            "Iteration 6, loss = 2.29204974\n",
            "Iteration 7, loss = 2.29017603\n",
            "Iteration 8, loss = 2.28859604\n",
            "Iteration 9, loss = 2.28731521\n",
            "Iteration 10, loss = 2.28631004\n",
            "Iteration 11, loss = 2.28548715\n",
            "Iteration 12, loss = 2.28480561\n",
            "Iteration 13, loss = 2.28420425\n",
            "Iteration 14, loss = 2.28368762\n",
            "Iteration 15, loss = 2.28321746\n",
            "Iteration 16, loss = 2.28278384\n",
            "Iteration 17, loss = 2.28238512\n",
            "Iteration 18, loss = 2.28198345\n",
            "Iteration 19, loss = 2.28162398\n",
            "Iteration 20, loss = 2.28128198\n",
            "Iteration 21, loss = 2.28095026\n",
            "Iteration 22, loss = 2.28065247\n",
            "Iteration 23, loss = 2.28036556\n",
            "Iteration 24, loss = 2.28007146\n",
            "Iteration 25, loss = 2.27980009\n",
            "Iteration 26, loss = 2.27953819\n",
            "Iteration 27, loss = 2.27926302\n",
            "Iteration 28, loss = 2.27899708\n",
            "Iteration 29, loss = 2.27875342\n",
            "Iteration 30, loss = 2.27847446\n",
            "Iteration 31, loss = 2.27825265\n",
            "Iteration 32, loss = 2.27797822\n",
            "Iteration 33, loss = 2.27775029\n",
            "Iteration 34, loss = 2.27752050\n",
            "Iteration 35, loss = 2.27730249\n",
            "Iteration 36, loss = 2.27707077\n",
            "Iteration 37, loss = 2.27687596\n",
            "Iteration 38, loss = 2.27667577\n",
            "Iteration 39, loss = 2.27647228\n",
            "Iteration 40, loss = 2.27628228\n",
            "Iteration 41, loss = 2.27608298\n",
            "Iteration 42, loss = 2.27590882\n",
            "Iteration 43, loss = 2.27571623\n",
            "Iteration 44, loss = 2.27554601\n",
            "Iteration 45, loss = 2.27535919\n",
            "Iteration 46, loss = 2.27521469\n",
            "Iteration 47, loss = 2.27501339\n",
            "Iteration 48, loss = 2.27484037\n",
            "Iteration 49, loss = 2.27468402\n",
            "Iteration 50, loss = 2.27451165\n",
            "Iteration 51, loss = 2.27434936\n",
            "Iteration 52, loss = 2.27418232\n",
            "Iteration 53, loss = 2.27402666\n",
            "Iteration 54, loss = 2.27386811\n",
            "Iteration 55, loss = 2.27372579\n",
            "Iteration 56, loss = 2.27355982\n",
            "Iteration 57, loss = 2.27342697\n",
            "Iteration 58, loss = 2.27326406\n",
            "Iteration 59, loss = 2.27312263\n",
            "Iteration 60, loss = 2.27297079\n",
            "Iteration 61, loss = 2.27281696\n",
            "Iteration 62, loss = 2.27267504\n",
            "Iteration 63, loss = 2.27253535\n",
            "Iteration 64, loss = 2.27239534\n",
            "Iteration 65, loss = 2.27224304\n",
            "Iteration 66, loss = 2.27212595\n",
            "Iteration 67, loss = 2.27199396\n",
            "Iteration 68, loss = 2.27188263\n",
            "Iteration 69, loss = 2.27172837\n",
            "Iteration 70, loss = 2.27161481\n",
            "Iteration 71, loss = 2.27147340\n",
            "Iteration 72, loss = 2.27139093\n",
            "Iteration 73, loss = 2.27125173\n",
            "Iteration 74, loss = 2.27115887\n",
            "Iteration 75, loss = 2.27103798\n",
            "Iteration 76, loss = 2.27094602\n",
            "Iteration 77, loss = 2.27081330\n",
            "Iteration 78, loss = 2.27073089\n",
            "Iteration 79, loss = 2.27065014\n",
            "Iteration 80, loss = 2.27055218\n",
            "Iteration 81, loss = 2.27047023\n",
            "Iteration 82, loss = 2.27036433\n",
            "Iteration 83, loss = 2.27029207\n",
            "Iteration 84, loss = 2.27022053\n",
            "Iteration 85, loss = 2.27013398\n",
            "Iteration 86, loss = 2.27005132\n",
            "Iteration 87, loss = 2.26997164\n",
            "Iteration 88, loss = 2.26987813\n",
            "Iteration 89, loss = 2.26982821\n",
            "Iteration 90, loss = 2.26976933\n",
            "Iteration 91, loss = 2.26968925\n",
            "Iteration 92, loss = 2.26962394\n",
            "Iteration 93, loss = 2.26952787\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "[CV] END activation=relu, alpha=0.05, hidden_layer_sizes=(32, 32), learning_rate=constant, max_iter=100, solver=sgd, validation_fraction=0.3, verbose=True; total time= 1.3min\n",
            "Iteration 1, loss = 2.32402446\n",
            "Iteration 2, loss = 2.30347038\n",
            "Iteration 3, loss = 2.29957368\n",
            "Iteration 4, loss = 2.29652561\n",
            "Iteration 5, loss = 2.29371253\n",
            "Iteration 6, loss = 2.29108078\n",
            "Iteration 7, loss = 2.28866024\n",
            "Iteration 8, loss = 2.28653032\n",
            "Iteration 9, loss = 2.28470789\n",
            "Iteration 10, loss = 2.28324848\n",
            "Iteration 11, loss = 2.28211617\n",
            "Iteration 12, loss = 2.28124696\n",
            "Iteration 13, loss = 2.28054362\n",
            "Iteration 14, loss = 2.28003894\n",
            "Iteration 15, loss = 2.27956293\n",
            "Iteration 16, loss = 2.27919215\n",
            "Iteration 17, loss = 2.27883923\n",
            "Iteration 18, loss = 2.27853242\n",
            "Iteration 19, loss = 2.27823354\n",
            "Iteration 20, loss = 2.27796859\n",
            "Iteration 21, loss = 2.27771043\n",
            "Iteration 22, loss = 2.27745263\n",
            "Iteration 23, loss = 2.27719727\n",
            "Iteration 24, loss = 2.27695166\n",
            "Iteration 25, loss = 2.27673209\n",
            "Iteration 26, loss = 2.27648046\n",
            "Iteration 27, loss = 2.27624713\n",
            "Iteration 28, loss = 2.27601782\n",
            "Iteration 29, loss = 2.27580972\n",
            "Iteration 30, loss = 2.27557708\n",
            "Iteration 31, loss = 2.27533016\n",
            "Iteration 32, loss = 2.27513110\n",
            "Iteration 33, loss = 2.27491686\n",
            "Iteration 34, loss = 2.27469442\n",
            "Iteration 35, loss = 2.27448318\n",
            "Iteration 36, loss = 2.27425226\n",
            "Iteration 37, loss = 2.27404503\n",
            "Iteration 38, loss = 2.27384946\n",
            "Iteration 39, loss = 2.27362457\n",
            "Iteration 40, loss = 2.27344666\n",
            "Iteration 41, loss = 2.27323784\n",
            "Iteration 42, loss = 2.27304462\n",
            "Iteration 43, loss = 2.27284682\n",
            "Iteration 44, loss = 2.27263246\n",
            "Iteration 45, loss = 2.27245242\n",
            "Iteration 46, loss = 2.27227180\n",
            "Iteration 47, loss = 2.27206895\n",
            "Iteration 48, loss = 2.27187497\n",
            "Iteration 49, loss = 2.27172379\n",
            "Iteration 50, loss = 2.27150544\n",
            "Iteration 51, loss = 2.27135920\n",
            "Iteration 52, loss = 2.27117037\n",
            "Iteration 53, loss = 2.27097835\n",
            "Iteration 54, loss = 2.27081580\n",
            "Iteration 55, loss = 2.27064705\n",
            "Iteration 56, loss = 2.27046784\n",
            "Iteration 57, loss = 2.27029806\n",
            "Iteration 58, loss = 2.27017646\n",
            "Iteration 59, loss = 2.27002669\n",
            "Iteration 60, loss = 2.26987378\n",
            "Iteration 61, loss = 2.26973487\n",
            "Iteration 62, loss = 2.26959560\n",
            "Iteration 63, loss = 2.26944648\n",
            "Iteration 64, loss = 2.26929972\n",
            "Iteration 65, loss = 2.26916508\n",
            "Iteration 66, loss = 2.26905779\n",
            "Iteration 67, loss = 2.26892546\n",
            "Iteration 68, loss = 2.26881880\n",
            "Iteration 69, loss = 2.26871708\n",
            "Iteration 70, loss = 2.26860593\n",
            "Iteration 71, loss = 2.26849476\n",
            "Iteration 72, loss = 2.26838161\n",
            "Iteration 73, loss = 2.26827251\n",
            "Iteration 74, loss = 2.26818748\n",
            "Iteration 75, loss = 2.26807771\n",
            "Iteration 76, loss = 2.26798801\n",
            "Iteration 77, loss = 2.26789979\n",
            "Iteration 78, loss = 2.26781919\n",
            "Iteration 79, loss = 2.26770923\n",
            "Iteration 80, loss = 2.26764036\n",
            "Iteration 81, loss = 2.26756377\n",
            "Iteration 82, loss = 2.26748312\n",
            "Iteration 83, loss = 2.26738959\n",
            "Iteration 84, loss = 2.26731575\n",
            "Iteration 85, loss = 2.26723305\n",
            "Iteration 86, loss = 2.26715686\n",
            "Iteration 87, loss = 2.26708293\n",
            "Iteration 88, loss = 2.26700757\n",
            "Iteration 89, loss = 2.26691704\n",
            "Iteration 90, loss = 2.26684470\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "[CV] END activation=relu, alpha=0.05, hidden_layer_sizes=(32, 32), learning_rate=constant, max_iter=100, solver=sgd, validation_fraction=0.3, verbose=True; total time= 1.2min\n",
            "Iteration 1, loss = 2.31168092\n",
            "Iteration 2, loss = 2.29905162\n",
            "Iteration 3, loss = 2.29439370\n",
            "Iteration 4, loss = 2.29102319\n",
            "Iteration 5, loss = 2.28844182\n",
            "Iteration 6, loss = 2.28644617\n",
            "Iteration 7, loss = 2.28488911\n",
            "Iteration 8, loss = 2.28365625\n",
            "Iteration 9, loss = 2.28263644\n",
            "Iteration 10, loss = 2.28181608\n",
            "Iteration 11, loss = 2.28108735\n",
            "Iteration 12, loss = 2.28046176\n",
            "Iteration 13, loss = 2.27989841\n",
            "Iteration 14, loss = 2.27938930\n",
            "Iteration 15, loss = 2.27893095\n",
            "Iteration 16, loss = 2.27847452\n",
            "Iteration 17, loss = 2.27808038\n",
            "Iteration 18, loss = 2.27770306\n",
            "Iteration 19, loss = 2.27735110\n",
            "Iteration 20, loss = 2.27703597\n",
            "Iteration 21, loss = 2.27673216\n",
            "Iteration 22, loss = 2.27645089\n",
            "Iteration 23, loss = 2.27618072\n",
            "Iteration 24, loss = 2.27593128\n",
            "Iteration 25, loss = 2.27567731\n",
            "Iteration 26, loss = 2.27543530\n",
            "Iteration 27, loss = 2.27522040\n",
            "Iteration 28, loss = 2.27498480\n",
            "Iteration 29, loss = 2.27475984\n",
            "Iteration 30, loss = 2.27456586\n",
            "Iteration 31, loss = 2.27435069\n",
            "Iteration 32, loss = 2.27415449\n",
            "Iteration 33, loss = 2.27394804\n",
            "Iteration 34, loss = 2.27375153\n",
            "Iteration 35, loss = 2.27355040\n",
            "Iteration 36, loss = 2.27337478\n",
            "Iteration 37, loss = 2.27317326\n",
            "Iteration 38, loss = 2.27301253\n",
            "Iteration 39, loss = 2.27283627\n",
            "Iteration 40, loss = 2.27264987\n",
            "Iteration 41, loss = 2.27245764\n",
            "Iteration 42, loss = 2.27227253\n",
            "Iteration 43, loss = 2.27214453\n",
            "Iteration 44, loss = 2.27197477\n",
            "Iteration 45, loss = 2.27182408\n",
            "Iteration 46, loss = 2.27165092\n",
            "Iteration 47, loss = 2.27148839\n",
            "Iteration 48, loss = 2.27132872\n",
            "Iteration 49, loss = 2.27116938\n",
            "Iteration 50, loss = 2.27099782\n",
            "Iteration 51, loss = 2.27082268\n",
            "Iteration 52, loss = 2.27067535\n",
            "Iteration 53, loss = 2.27051169\n",
            "Iteration 54, loss = 2.27034237\n",
            "Iteration 55, loss = 2.27017609\n",
            "Iteration 56, loss = 2.27003714\n",
            "Iteration 57, loss = 2.26986001\n",
            "Iteration 58, loss = 2.26971302\n",
            "Iteration 59, loss = 2.26955526\n",
            "Iteration 60, loss = 2.26938852\n",
            "Iteration 61, loss = 2.26922362\n",
            "Iteration 62, loss = 2.26907734\n",
            "Iteration 63, loss = 2.26891785\n",
            "Iteration 64, loss = 2.26877191\n",
            "Iteration 65, loss = 2.26863782\n",
            "Iteration 66, loss = 2.26847425\n",
            "Iteration 67, loss = 2.26835597\n",
            "Iteration 68, loss = 2.26822646\n",
            "Iteration 69, loss = 2.26808956\n",
            "Iteration 70, loss = 2.26796183\n",
            "Iteration 71, loss = 2.26783599\n",
            "Iteration 72, loss = 2.26772941\n",
            "Iteration 73, loss = 2.26759649\n",
            "Iteration 74, loss = 2.26749368\n",
            "Iteration 75, loss = 2.26737561\n",
            "Iteration 76, loss = 2.26728458\n",
            "Iteration 77, loss = 2.26717552\n",
            "Iteration 78, loss = 2.26705938\n",
            "Iteration 79, loss = 2.26698170\n",
            "Iteration 80, loss = 2.26688897\n",
            "Iteration 81, loss = 2.26679097\n",
            "Iteration 82, loss = 2.26671101\n",
            "Iteration 83, loss = 2.26661698\n",
            "Iteration 84, loss = 2.26652100\n",
            "Iteration 85, loss = 2.26643442\n",
            "Iteration 86, loss = 2.26639844\n",
            "Iteration 87, loss = 2.26629650\n",
            "Iteration 88, loss = 2.26621680\n",
            "Iteration 89, loss = 2.26617817\n",
            "Iteration 90, loss = 2.26609522\n",
            "Iteration 91, loss = 2.26601526\n",
            "Iteration 92, loss = 2.26596455\n",
            "Iteration 93, loss = 2.26589699\n",
            "Iteration 94, loss = 2.26585140\n",
            "Iteration 95, loss = 2.26577471\n",
            "Iteration 96, loss = 2.26572193\n",
            "Iteration 97, loss = 2.26567205\n",
            "Iteration 98, loss = 2.26562444\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "[CV] END activation=relu, alpha=0.05, hidden_layer_sizes=(32, 32), learning_rate=constant, max_iter=100, solver=sgd, validation_fraction=0.3, verbose=True; total time= 1.4min\n",
            "Iteration 1, loss = 2.29736269\n",
            "Iteration 2, loss = 2.28289996\n",
            "Iteration 3, loss = 2.28117314\n",
            "Iteration 4, loss = 2.28048489\n",
            "Iteration 5, loss = 2.27987590\n",
            "Iteration 6, loss = 2.27933441\n",
            "Iteration 7, loss = 2.27879708\n",
            "Iteration 8, loss = 2.27838619\n",
            "Iteration 9, loss = 2.27769434\n",
            "Iteration 10, loss = 2.27696426\n",
            "Iteration 11, loss = 2.27611874\n",
            "Iteration 12, loss = 2.27524454\n",
            "Iteration 13, loss = 2.27448169\n",
            "Iteration 14, loss = 2.27354644\n",
            "Iteration 15, loss = 2.27264179\n",
            "Iteration 16, loss = 2.27215727\n",
            "Iteration 17, loss = 2.27200443\n",
            "Iteration 18, loss = 2.27154057\n",
            "Iteration 19, loss = 2.27135284\n",
            "Iteration 20, loss = 2.27112119\n",
            "Iteration 21, loss = 2.27101460\n",
            "Iteration 22, loss = 2.27081292\n",
            "Iteration 23, loss = 2.27084716\n",
            "Iteration 24, loss = 2.27068945\n",
            "Iteration 25, loss = 2.27062432\n",
            "Iteration 26, loss = 2.27054996\n",
            "Iteration 27, loss = 2.27036475\n",
            "Iteration 28, loss = 2.27033298\n",
            "Iteration 29, loss = 2.27033116\n",
            "Iteration 30, loss = 2.27027160\n",
            "Iteration 31, loss = 2.27025330\n",
            "Iteration 32, loss = 2.27010641\n",
            "Iteration 33, loss = 2.27012084\n",
            "Iteration 34, loss = 2.27004582\n",
            "Iteration 35, loss = 2.27002676\n",
            "Iteration 36, loss = 2.27000196\n",
            "Iteration 37, loss = 2.26993869\n",
            "Iteration 38, loss = 2.27001971\n",
            "Iteration 39, loss = 2.26989290\n",
            "Iteration 40, loss = 2.26989678\n",
            "Iteration 41, loss = 2.26980917\n",
            "Iteration 42, loss = 2.26984644\n",
            "Iteration 43, loss = 2.26979589\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "[CV] END activation=tanh, alpha=0.05, hidden_layer_sizes=(24, 24), learning_rate=constant, max_iter=100, solver=adam, validation_fraction=0.3, verbose=True; total time=  42.0s\n",
            "Iteration 1, loss = 2.29573117\n",
            "Iteration 2, loss = 2.28241075\n",
            "Iteration 3, loss = 2.28044670\n",
            "Iteration 4, loss = 2.27952212\n",
            "Iteration 5, loss = 2.27862690\n",
            "Iteration 6, loss = 2.27786410\n",
            "Iteration 7, loss = 2.27679922\n",
            "Iteration 8, loss = 2.27579493\n",
            "Iteration 9, loss = 2.27496561\n",
            "Iteration 10, loss = 2.27408762\n",
            "Iteration 11, loss = 2.27354264\n",
            "Iteration 12, loss = 2.27314329\n",
            "Iteration 13, loss = 2.27269511\n",
            "Iteration 14, loss = 2.27222017\n",
            "Iteration 15, loss = 2.27195277\n",
            "Iteration 16, loss = 2.27179557\n",
            "Iteration 17, loss = 2.27139397\n",
            "Iteration 18, loss = 2.27110217\n",
            "Iteration 19, loss = 2.27075761\n",
            "Iteration 20, loss = 2.27048479\n",
            "Iteration 21, loss = 2.27032643\n",
            "Iteration 22, loss = 2.27001647\n",
            "Iteration 23, loss = 2.26966362\n",
            "Iteration 24, loss = 2.26953302\n",
            "Iteration 25, loss = 2.26932007\n",
            "Iteration 26, loss = 2.26923968\n",
            "Iteration 27, loss = 2.26900625\n",
            "Iteration 28, loss = 2.26886508\n",
            "Iteration 29, loss = 2.26872561\n",
            "Iteration 30, loss = 2.26864335\n",
            "Iteration 31, loss = 2.26853977\n",
            "Iteration 32, loss = 2.26838226\n",
            "Iteration 33, loss = 2.26823474\n",
            "Iteration 34, loss = 2.26816989\n",
            "Iteration 35, loss = 2.26813714\n",
            "Iteration 36, loss = 2.26809496\n",
            "Iteration 37, loss = 2.26807871\n",
            "Iteration 38, loss = 2.26799437\n",
            "Iteration 39, loss = 2.26797949\n",
            "Iteration 40, loss = 2.26790597\n",
            "Iteration 41, loss = 2.26786629\n",
            "Iteration 42, loss = 2.26783637\n",
            "Iteration 43, loss = 2.26782983\n",
            "Iteration 44, loss = 2.26778560\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "[CV] END activation=tanh, alpha=0.05, hidden_layer_sizes=(24, 24), learning_rate=constant, max_iter=100, solver=adam, validation_fraction=0.3, verbose=True; total time=  53.3s\n",
            "Iteration 1, loss = 2.30022666\n",
            "Iteration 2, loss = 2.28525868\n",
            "Iteration 3, loss = 2.28269400\n",
            "Iteration 4, loss = 2.28183809\n",
            "Iteration 5, loss = 2.28119755\n",
            "Iteration 6, loss = 2.28063459\n",
            "Iteration 7, loss = 2.28017942\n",
            "Iteration 8, loss = 2.27972038\n",
            "Iteration 9, loss = 2.27930550\n",
            "Iteration 10, loss = 2.27870052\n",
            "Iteration 11, loss = 2.27815672\n",
            "Iteration 12, loss = 2.27715952\n",
            "Iteration 13, loss = 2.27603427\n",
            "Iteration 14, loss = 2.27526531\n",
            "Iteration 15, loss = 2.27473635\n",
            "Iteration 16, loss = 2.27438238\n",
            "Iteration 17, loss = 2.27409810\n",
            "Iteration 18, loss = 2.27381701\n",
            "Iteration 19, loss = 2.27364101\n",
            "Iteration 20, loss = 2.27342794\n",
            "Iteration 21, loss = 2.27326342\n",
            "Iteration 22, loss = 2.27307540\n",
            "Iteration 23, loss = 2.27297407\n",
            "Iteration 24, loss = 2.27280581\n",
            "Iteration 25, loss = 2.27273704\n",
            "Iteration 26, loss = 2.27257946\n",
            "Iteration 27, loss = 2.27255908\n",
            "Iteration 28, loss = 2.27248313\n",
            "Iteration 29, loss = 2.27236455\n",
            "Iteration 30, loss = 2.27225875\n",
            "Iteration 31, loss = 2.27217885\n",
            "Iteration 32, loss = 2.27206000\n",
            "Iteration 33, loss = 2.27199601\n",
            "Iteration 34, loss = 2.27193743\n",
            "Iteration 35, loss = 2.27192299\n",
            "Iteration 36, loss = 2.27172864\n",
            "Iteration 37, loss = 2.27171198\n",
            "Iteration 38, loss = 2.27169393\n",
            "Iteration 39, loss = 2.27154043\n",
            "Iteration 40, loss = 2.27144971\n",
            "Iteration 41, loss = 2.27144036\n",
            "Iteration 42, loss = 2.27135969\n",
            "Iteration 43, loss = 2.27127088\n",
            "Iteration 44, loss = 2.27115576\n",
            "Iteration 45, loss = 2.27119805\n",
            "Iteration 46, loss = 2.27111143\n",
            "Iteration 47, loss = 2.27101597\n",
            "Iteration 48, loss = 2.27110774\n",
            "Iteration 49, loss = 2.27103246\n",
            "Iteration 50, loss = 2.27105839\n",
            "Iteration 51, loss = 2.27098834\n",
            "Iteration 52, loss = 2.27090761\n",
            "Iteration 53, loss = 2.27098597\n",
            "Iteration 54, loss = 2.27082145\n",
            "Iteration 55, loss = 2.27091862\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "[CV] END activation=tanh, alpha=0.05, hidden_layer_sizes=(24, 24), learning_rate=constant, max_iter=100, solver=adam, validation_fraction=0.3, verbose=True; total time=  52.4s\n",
            "Iteration 1, loss = 2.29342609\n",
            "Iteration 2, loss = 2.28110541\n",
            "Iteration 3, loss = 2.27947622\n",
            "Iteration 4, loss = 2.27856245\n",
            "Iteration 5, loss = 2.27760251\n",
            "Iteration 6, loss = 2.27641718\n",
            "Iteration 7, loss = 2.27462404\n",
            "Iteration 8, loss = 2.27298287\n",
            "Iteration 9, loss = 2.27190475\n",
            "Iteration 10, loss = 2.27145000\n",
            "Iteration 11, loss = 2.27085825\n",
            "Iteration 12, loss = 2.27073382\n",
            "Iteration 13, loss = 2.27036954\n",
            "Iteration 14, loss = 2.27004790\n",
            "Iteration 15, loss = 2.26987331\n",
            "Iteration 16, loss = 2.26972468\n",
            "Iteration 17, loss = 2.26944981\n",
            "Iteration 18, loss = 2.26926836\n",
            "Iteration 19, loss = 2.26927658\n",
            "Iteration 20, loss = 2.26900944\n",
            "Iteration 21, loss = 2.26882737\n",
            "Iteration 22, loss = 2.26866074\n",
            "Iteration 23, loss = 2.26833676\n",
            "Iteration 24, loss = 2.26812373\n",
            "Iteration 25, loss = 2.26785663\n",
            "Iteration 26, loss = 2.26779419\n",
            "Iteration 27, loss = 2.26768545\n",
            "Iteration 28, loss = 2.26755077\n",
            "Iteration 29, loss = 2.26745656\n",
            "Iteration 30, loss = 2.26747293\n",
            "Iteration 31, loss = 2.26737026\n",
            "Iteration 32, loss = 2.26734377\n",
            "Iteration 33, loss = 2.26715749\n",
            "Iteration 34, loss = 2.26710429\n",
            "Iteration 35, loss = 2.26719367\n",
            "Iteration 36, loss = 2.26709041\n",
            "Iteration 37, loss = 2.26697118\n",
            "Iteration 38, loss = 2.26699235\n",
            "Iteration 39, loss = 2.26687881\n",
            "Iteration 40, loss = 2.26679920\n",
            "Iteration 41, loss = 2.26679067\n",
            "Iteration 42, loss = 2.26673809\n",
            "Iteration 43, loss = 2.26685630\n",
            "Iteration 44, loss = 2.26665921\n",
            "Iteration 45, loss = 2.26657204\n",
            "Iteration 46, loss = 2.26653835\n",
            "Iteration 47, loss = 2.26660859\n",
            "Iteration 48, loss = 2.26644345\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "[CV] END activation=tanh, alpha=0.05, hidden_layer_sizes=(24, 24), learning_rate=constant, max_iter=100, solver=adam, validation_fraction=0.3, verbose=True; total time=  47.4s\n",
            "Iteration 1, loss = 2.29740268\n",
            "Iteration 2, loss = 2.28286339\n",
            "Iteration 3, loss = 2.27913335\n",
            "Iteration 4, loss = 2.27704310\n",
            "Iteration 5, loss = 2.27540633\n",
            "Iteration 6, loss = 2.27437563\n",
            "Iteration 7, loss = 2.27345225\n",
            "Iteration 8, loss = 2.27274043\n",
            "Iteration 9, loss = 2.27230934\n",
            "Iteration 10, loss = 2.27192032\n",
            "Iteration 11, loss = 2.27145909\n",
            "Iteration 12, loss = 2.27108583\n",
            "Iteration 13, loss = 2.27093354\n",
            "Iteration 14, loss = 2.27064297\n",
            "Iteration 15, loss = 2.27031778\n",
            "Iteration 16, loss = 2.27010517\n",
            "Iteration 17, loss = 2.26975709\n",
            "Iteration 18, loss = 2.26965317\n",
            "Iteration 19, loss = 2.26937556\n",
            "Iteration 20, loss = 2.26921996\n",
            "Iteration 21, loss = 2.26904080\n",
            "Iteration 22, loss = 2.26893668\n",
            "Iteration 23, loss = 2.26877964\n",
            "Iteration 24, loss = 2.26872879\n",
            "Iteration 25, loss = 2.26858686\n",
            "Iteration 26, loss = 2.26846948\n",
            "Iteration 27, loss = 2.26841678\n",
            "Iteration 28, loss = 2.26831617\n",
            "Iteration 29, loss = 2.26827082\n",
            "Iteration 30, loss = 2.26822844\n",
            "Iteration 31, loss = 2.26817520\n",
            "Iteration 32, loss = 2.26808605\n",
            "Iteration 33, loss = 2.26807404\n",
            "Iteration 34, loss = 2.26794085\n",
            "Iteration 35, loss = 2.26793700\n",
            "Iteration 36, loss = 2.26792206\n",
            "Iteration 37, loss = 2.26793031\n",
            "Iteration 38, loss = 2.26780397\n",
            "Iteration 39, loss = 2.26774098\n",
            "Iteration 40, loss = 2.26771979\n",
            "Iteration 41, loss = 2.26772721\n",
            "Iteration 42, loss = 2.26764847\n",
            "Iteration 43, loss = 2.26762702\n",
            "Iteration 44, loss = 2.26764191\n",
            "Iteration 45, loss = 2.26753636\n",
            "Iteration 46, loss = 2.26756300\n",
            "Iteration 47, loss = 2.26753713\n",
            "Iteration 48, loss = 2.26735462\n",
            "Iteration 49, loss = 2.26744632\n",
            "Iteration 50, loss = 2.26736867\n",
            "Iteration 51, loss = 2.26736972\n",
            "Iteration 52, loss = 2.26724946\n",
            "Iteration 53, loss = 2.26724982\n",
            "Iteration 54, loss = 2.26709547\n",
            "Iteration 55, loss = 2.26721617\n",
            "Iteration 56, loss = 2.26716166\n",
            "Iteration 57, loss = 2.26702005\n",
            "Iteration 58, loss = 2.26711080\n",
            "Iteration 59, loss = 2.26707869\n",
            "Iteration 60, loss = 2.26700548\n",
            "Iteration 61, loss = 2.26707484\n",
            "Iteration 62, loss = 2.26699016\n",
            "Iteration 63, loss = 2.26700474\n",
            "Iteration 64, loss = 2.26702472\n",
            "Iteration 65, loss = 2.26699222\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "[CV] END activation=tanh, alpha=0.05, hidden_layer_sizes=(24, 24), learning_rate=constant, max_iter=100, solver=adam, validation_fraction=0.3, verbose=True; total time= 1.1min\n",
            "Iteration 1, loss = 2.28566655\n",
            "Iteration 2, loss = 2.27349940\n",
            "Iteration 3, loss = 2.26999524\n",
            "Iteration 4, loss = 2.26830263\n",
            "Iteration 5, loss = 2.26743644\n",
            "Iteration 6, loss = 2.26681155\n",
            "Iteration 7, loss = 2.26618042\n",
            "Iteration 8, loss = 2.26569521\n",
            "Iteration 9, loss = 2.26549495\n",
            "Iteration 10, loss = 2.26512034\n",
            "Iteration 11, loss = 2.26489763\n",
            "Iteration 12, loss = 2.26472385\n",
            "Iteration 13, loss = 2.26451464\n",
            "Iteration 14, loss = 2.26443570\n",
            "Iteration 15, loss = 2.26436110\n",
            "Iteration 16, loss = 2.26421404\n",
            "Iteration 17, loss = 2.26408062\n",
            "Iteration 18, loss = 2.26399747\n",
            "Iteration 19, loss = 2.26401260\n",
            "Iteration 20, loss = 2.26398224\n",
            "Iteration 21, loss = 2.26380929\n",
            "Iteration 22, loss = 2.26373307\n",
            "Iteration 23, loss = 2.26381456\n",
            "Iteration 24, loss = 2.26376885\n",
            "Iteration 25, loss = 2.26361954\n",
            "Iteration 26, loss = 2.26367432\n",
            "Iteration 27, loss = 2.26361611\n",
            "Iteration 28, loss = 2.26358065\n",
            "Iteration 29, loss = 2.26354634\n",
            "Iteration 30, loss = 2.26358487\n",
            "Iteration 31, loss = 2.26345334\n",
            "Iteration 32, loss = 2.26348198\n",
            "Iteration 33, loss = 2.26340797\n",
            "Iteration 34, loss = 2.26354327\n",
            "Iteration 35, loss = 2.26341895\n",
            "Iteration 36, loss = 2.26328765\n",
            "Iteration 37, loss = 2.26337777\n",
            "Iteration 38, loss = 2.26335841\n",
            "Iteration 39, loss = 2.26342206\n",
            "Iteration 40, loss = 2.26332017\n",
            "Iteration 41, loss = 2.26329129\n",
            "Iteration 42, loss = 2.26328964\n",
            "Iteration 43, loss = 2.26331966\n",
            "Iteration 44, loss = 2.26326272\n",
            "Iteration 45, loss = 2.26322192\n",
            "Iteration 46, loss = 2.26329044\n",
            "Iteration 47, loss = 2.26320321\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "[CV] END activation=relu, alpha=0.05, hidden_layer_sizes=(32, 32), learning_rate=constant, max_iter=100, solver=adam, validation_fraction=0.3, verbose=True; total time= 1.1min\n",
            "Iteration 1, loss = 2.28310016\n",
            "Iteration 2, loss = 2.26911156\n",
            "Iteration 3, loss = 2.26619312\n",
            "Iteration 4, loss = 2.26473752\n",
            "Iteration 5, loss = 2.26401583\n",
            "Iteration 6, loss = 2.26357091\n",
            "Iteration 7, loss = 2.26335176\n",
            "Iteration 8, loss = 2.26304968\n",
            "Iteration 9, loss = 2.26286916\n",
            "Iteration 10, loss = 2.26271364\n",
            "Iteration 11, loss = 2.26265251\n",
            "Iteration 12, loss = 2.26249214\n",
            "Iteration 13, loss = 2.26244539\n",
            "Iteration 14, loss = 2.26233922\n",
            "Iteration 15, loss = 2.26232660\n",
            "Iteration 16, loss = 2.26222552\n",
            "Iteration 17, loss = 2.26219764\n",
            "Iteration 18, loss = 2.26215906\n",
            "Iteration 19, loss = 2.26211959\n",
            "Iteration 20, loss = 2.26202378\n",
            "Iteration 21, loss = 2.26198598\n",
            "Iteration 22, loss = 2.26196075\n",
            "Iteration 23, loss = 2.26192155\n",
            "Iteration 24, loss = 2.26191210\n",
            "Iteration 25, loss = 2.26188156\n",
            "Iteration 26, loss = 2.26186838\n",
            "Iteration 27, loss = 2.26182684\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "[CV] END activation=relu, alpha=0.05, hidden_layer_sizes=(32, 32), learning_rate=constant, max_iter=100, solver=adam, validation_fraction=0.3, verbose=True; total time=  35.1s\n",
            "Iteration 1, loss = 2.28554357\n",
            "Iteration 2, loss = 2.27161612\n",
            "Iteration 3, loss = 2.26884404\n",
            "Iteration 4, loss = 2.26744424\n",
            "Iteration 5, loss = 2.26679654\n",
            "Iteration 6, loss = 2.26635566\n",
            "Iteration 7, loss = 2.26604128\n",
            "Iteration 8, loss = 2.26573924\n",
            "Iteration 9, loss = 2.26567004\n",
            "Iteration 10, loss = 2.26544436\n",
            "Iteration 11, loss = 2.26538162\n",
            "Iteration 12, loss = 2.26532123\n",
            "Iteration 13, loss = 2.26524409\n",
            "Iteration 14, loss = 2.26507207\n",
            "Iteration 15, loss = 2.26507872\n",
            "Iteration 16, loss = 2.26502732\n",
            "Iteration 17, loss = 2.26492873\n",
            "Iteration 18, loss = 2.26480681\n",
            "Iteration 19, loss = 2.26480672\n",
            "Iteration 20, loss = 2.26471749\n",
            "Iteration 21, loss = 2.26471905\n",
            "Iteration 22, loss = 2.26464343\n",
            "Iteration 23, loss = 2.26465263\n",
            "Iteration 24, loss = 2.26459947\n",
            "Iteration 25, loss = 2.26467913\n",
            "Iteration 26, loss = 2.26457780\n",
            "Iteration 27, loss = 2.26462382\n",
            "Iteration 28, loss = 2.26452927\n",
            "Iteration 29, loss = 2.26454300\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "[CV] END activation=relu, alpha=0.05, hidden_layer_sizes=(32, 32), learning_rate=constant, max_iter=100, solver=adam, validation_fraction=0.3, verbose=True; total time=  42.4s\n",
            "Iteration 1, loss = 2.28069018\n",
            "Iteration 2, loss = 2.26842907\n",
            "Iteration 3, loss = 2.26621514\n",
            "Iteration 4, loss = 2.26511877\n",
            "Iteration 5, loss = 2.26456509\n",
            "Iteration 6, loss = 2.26410114\n",
            "Iteration 7, loss = 2.26371086\n",
            "Iteration 8, loss = 2.26331514\n",
            "Iteration 9, loss = 2.26297907\n",
            "Iteration 10, loss = 2.26261530\n",
            "Iteration 11, loss = 2.26236361\n",
            "Iteration 12, loss = 2.26203236\n",
            "Iteration 13, loss = 2.26188567\n",
            "Iteration 14, loss = 2.26164288\n",
            "Iteration 15, loss = 2.26151814\n",
            "Iteration 16, loss = 2.26133154\n",
            "Iteration 17, loss = 2.26122141\n",
            "Iteration 18, loss = 2.26116885\n",
            "Iteration 19, loss = 2.26098980\n",
            "Iteration 20, loss = 2.26087147\n",
            "Iteration 21, loss = 2.26081058\n",
            "Iteration 22, loss = 2.26092284\n",
            "Iteration 23, loss = 2.26078294\n",
            "Iteration 24, loss = 2.26076490\n",
            "Iteration 25, loss = 2.26070036\n",
            "Iteration 26, loss = 2.26068613\n",
            "Iteration 27, loss = 2.26065099\n",
            "Iteration 28, loss = 2.26065371\n",
            "Iteration 29, loss = 2.26063576\n",
            "Iteration 30, loss = 2.26059297\n",
            "Iteration 31, loss = 2.26065328\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "[CV] END activation=relu, alpha=0.05, hidden_layer_sizes=(32, 32), learning_rate=constant, max_iter=100, solver=adam, validation_fraction=0.3, verbose=True; total time=  53.0s\n",
            "Iteration 1, loss = 2.28864111\n",
            "Iteration 2, loss = 2.27085903\n",
            "Iteration 3, loss = 2.26710082\n",
            "Iteration 4, loss = 2.26546473\n",
            "Iteration 5, loss = 2.26435069\n",
            "Iteration 6, loss = 2.26359175\n",
            "Iteration 7, loss = 2.26327315\n",
            "Iteration 8, loss = 2.26287181\n",
            "Iteration 9, loss = 2.26262557\n",
            "Iteration 10, loss = 2.26242766\n",
            "Iteration 11, loss = 2.26236026\n",
            "Iteration 12, loss = 2.26213821\n",
            "Iteration 13, loss = 2.26204404\n",
            "Iteration 14, loss = 2.26194110\n",
            "Iteration 15, loss = 2.26180319\n",
            "Iteration 16, loss = 2.26178040\n",
            "Iteration 17, loss = 2.26165560\n",
            "Iteration 18, loss = 2.26158615\n",
            "Iteration 19, loss = 2.26157163\n",
            "Iteration 20, loss = 2.26152927\n",
            "Iteration 21, loss = 2.26131987\n",
            "Iteration 22, loss = 2.26142092\n",
            "Iteration 23, loss = 2.26138407\n",
            "Iteration 24, loss = 2.26133399\n",
            "Iteration 25, loss = 2.26129130\n",
            "Iteration 26, loss = 2.26128118\n",
            "Iteration 27, loss = 2.26120076\n",
            "Iteration 28, loss = 2.26116651\n",
            "Iteration 29, loss = 2.26118693\n",
            "Iteration 30, loss = 2.26116489\n",
            "Iteration 31, loss = 2.26119788\n",
            "Iteration 32, loss = 2.26109019\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "[CV] END activation=relu, alpha=0.05, hidden_layer_sizes=(32, 32), learning_rate=constant, max_iter=100, solver=adam, validation_fraction=0.3, verbose=True; total time=  51.7s\n",
            "Iteration 1, loss = 2.31547547\n",
            "Iteration 2, loss = 2.30071133\n",
            "Iteration 3, loss = 2.29878647\n",
            "Iteration 4, loss = 2.29765746\n",
            "Iteration 5, loss = 2.29666681\n",
            "Iteration 6, loss = 2.29565736\n",
            "Iteration 7, loss = 2.29449360\n",
            "Iteration 8, loss = 2.29314194\n",
            "Iteration 9, loss = 2.29152222\n",
            "Iteration 10, loss = 2.28958449\n",
            "Iteration 11, loss = 2.28742745\n",
            "Iteration 12, loss = 2.28517933\n",
            "Iteration 13, loss = 2.28301538\n",
            "Iteration 14, loss = 2.28106808\n",
            "Iteration 15, loss = 2.27943608\n",
            "Iteration 16, loss = 2.27810969\n",
            "Iteration 17, loss = 2.27706240\n",
            "Iteration 18, loss = 2.27624080\n",
            "Iteration 19, loss = 2.27559764\n",
            "Iteration 20, loss = 2.27505076\n",
            "Iteration 21, loss = 2.27458829\n",
            "Iteration 22, loss = 2.27419576\n",
            "Iteration 23, loss = 2.27384135\n",
            "Iteration 24, loss = 2.27351836\n",
            "Iteration 25, loss = 2.27323529\n",
            "Iteration 26, loss = 2.27295633\n",
            "Iteration 27, loss = 2.27266154\n",
            "Iteration 28, loss = 2.27241213\n",
            "Iteration 29, loss = 2.27213694\n",
            "Iteration 30, loss = 2.27186104\n",
            "Iteration 31, loss = 2.27158306\n",
            "Iteration 32, loss = 2.27129462\n",
            "Iteration 33, loss = 2.27102388\n",
            "Iteration 34, loss = 2.27078146\n",
            "Iteration 35, loss = 2.27050351\n",
            "Iteration 36, loss = 2.27030232\n",
            "Iteration 37, loss = 2.27006481\n",
            "Iteration 38, loss = 2.26986689\n",
            "Iteration 39, loss = 2.26966435\n",
            "Iteration 40, loss = 2.26948624\n",
            "Iteration 41, loss = 2.26929892\n",
            "Iteration 42, loss = 2.26913220\n",
            "Iteration 43, loss = 2.26898625\n",
            "Iteration 44, loss = 2.26882133\n",
            "Iteration 45, loss = 2.26869466\n",
            "Iteration 46, loss = 2.26854743\n",
            "Iteration 47, loss = 2.26842569\n",
            "Iteration 48, loss = 2.26832932\n",
            "Iteration 49, loss = 2.26820145\n",
            "Iteration 50, loss = 2.26809382\n",
            "Iteration 51, loss = 2.26799047\n",
            "Iteration 52, loss = 2.26787138\n",
            "Iteration 53, loss = 2.26777775\n",
            "Iteration 54, loss = 2.26771033\n",
            "Iteration 55, loss = 2.26761117\n",
            "Iteration 56, loss = 2.26750818\n",
            "Iteration 57, loss = 2.26742144\n",
            "Iteration 58, loss = 2.26734156\n",
            "Iteration 59, loss = 2.26725250\n",
            "Iteration 60, loss = 2.26716993\n",
            "Iteration 61, loss = 2.26708266\n",
            "Iteration 62, loss = 2.26701803\n",
            "Iteration 63, loss = 2.26694528\n",
            "Iteration 64, loss = 2.26683512\n",
            "Iteration 65, loss = 2.26677474\n",
            "Iteration 66, loss = 2.26668926\n",
            "Iteration 67, loss = 2.26662862\n",
            "Iteration 68, loss = 2.26653786\n",
            "Iteration 69, loss = 2.26646664\n",
            "Iteration 70, loss = 2.26637758\n",
            "Iteration 71, loss = 2.26629141\n",
            "Iteration 72, loss = 2.26621874\n",
            "Iteration 73, loss = 2.26614283\n",
            "Iteration 74, loss = 2.26603942\n",
            "Iteration 75, loss = 2.26597370\n",
            "Iteration 76, loss = 2.26588090\n",
            "Iteration 77, loss = 2.26579593\n",
            "Iteration 78, loss = 2.26570493\n",
            "Iteration 79, loss = 2.26561145\n",
            "Iteration 80, loss = 2.26551575\n",
            "Iteration 81, loss = 2.26544196\n",
            "Iteration 82, loss = 2.26533979\n",
            "Iteration 83, loss = 2.26524040\n",
            "Iteration 84, loss = 2.26512235\n",
            "Iteration 85, loss = 2.26501463\n",
            "Iteration 86, loss = 2.26492540\n",
            "Iteration 87, loss = 2.26479867\n",
            "Iteration 88, loss = 2.26470485\n",
            "Iteration 89, loss = 2.26460262\n",
            "Iteration 90, loss = 2.26449470\n",
            "Iteration 91, loss = 2.26437872\n",
            "Iteration 92, loss = 2.26425979\n",
            "Iteration 93, loss = 2.26415163\n",
            "Iteration 94, loss = 2.26405035\n",
            "Iteration 95, loss = 2.26394731\n",
            "Iteration 96, loss = 2.26383384\n",
            "Iteration 97, loss = 2.26370535\n",
            "Iteration 98, loss = 2.26359202\n",
            "Iteration 99, loss = 2.26347314\n",
            "Iteration 100, loss = 2.26335462\n",
            "[CV] END activation=relu, alpha=0, hidden_layer_sizes=(24, 24), learning_rate=adaptive, max_iter=100, solver=sgd, validation_fraction=0.3, verbose=True; total time= 1.2min\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  ConvergenceWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1, loss = 2.30541639\n",
            "Iteration 2, loss = 2.29324781\n",
            "Iteration 3, loss = 2.28927748\n",
            "Iteration 4, loss = 2.28649941\n",
            "Iteration 5, loss = 2.28405278\n",
            "Iteration 6, loss = 2.28189513\n",
            "Iteration 7, loss = 2.28005636\n",
            "Iteration 8, loss = 2.27852053\n",
            "Iteration 9, loss = 2.27725091\n",
            "Iteration 10, loss = 2.27620534\n",
            "Iteration 11, loss = 2.27536950\n",
            "Iteration 12, loss = 2.27467299\n",
            "Iteration 13, loss = 2.27414485\n",
            "Iteration 14, loss = 2.27366831\n",
            "Iteration 15, loss = 2.27334062\n",
            "Iteration 16, loss = 2.27303183\n",
            "Iteration 17, loss = 2.27278293\n",
            "Iteration 18, loss = 2.27254921\n",
            "Iteration 19, loss = 2.27235698\n",
            "Iteration 20, loss = 2.27216511\n",
            "Iteration 21, loss = 2.27198489\n",
            "Iteration 22, loss = 2.27179832\n",
            "Iteration 23, loss = 2.27162712\n",
            "Iteration 24, loss = 2.27146797\n",
            "Iteration 25, loss = 2.27128185\n",
            "Iteration 26, loss = 2.27109231\n",
            "Iteration 27, loss = 2.27090468\n",
            "Iteration 28, loss = 2.27071885\n",
            "Iteration 29, loss = 2.27051115\n",
            "Iteration 30, loss = 2.27031329\n",
            "Iteration 31, loss = 2.27008088\n",
            "Iteration 32, loss = 2.26987186\n",
            "Iteration 33, loss = 2.26965602\n",
            "Iteration 34, loss = 2.26942038\n",
            "Iteration 35, loss = 2.26919300\n",
            "Iteration 36, loss = 2.26896705\n",
            "Iteration 37, loss = 2.26872509\n",
            "Iteration 38, loss = 2.26849091\n",
            "Iteration 39, loss = 2.26825798\n",
            "Iteration 40, loss = 2.26802354\n",
            "Iteration 41, loss = 2.26778578\n",
            "Iteration 42, loss = 2.26756215\n",
            "Iteration 43, loss = 2.26731455\n",
            "Iteration 44, loss = 2.26711776\n",
            "Iteration 45, loss = 2.26691039\n",
            "Iteration 46, loss = 2.26669231\n",
            "Iteration 47, loss = 2.26647680\n",
            "Iteration 48, loss = 2.26628925\n",
            "Iteration 49, loss = 2.26607063\n",
            "Iteration 50, loss = 2.26588313\n",
            "Iteration 51, loss = 2.26567218\n",
            "Iteration 52, loss = 2.26545756\n",
            "Iteration 53, loss = 2.26526760\n",
            "Iteration 54, loss = 2.26507057\n",
            "Iteration 55, loss = 2.26490487\n",
            "Iteration 56, loss = 2.26472075\n",
            "Iteration 57, loss = 2.26454724\n",
            "Iteration 58, loss = 2.26438473\n",
            "Iteration 59, loss = 2.26421111\n",
            "Iteration 60, loss = 2.26408360\n",
            "Iteration 61, loss = 2.26393765\n",
            "Iteration 62, loss = 2.26380093\n",
            "Iteration 63, loss = 2.26366477\n",
            "Iteration 64, loss = 2.26355433\n",
            "Iteration 65, loss = 2.26341216\n",
            "Iteration 66, loss = 2.26330700\n",
            "Iteration 67, loss = 2.26317862\n",
            "Iteration 68, loss = 2.26306909\n",
            "Iteration 69, loss = 2.26295953\n",
            "Iteration 70, loss = 2.26288795\n",
            "Iteration 71, loss = 2.26277843\n",
            "Iteration 72, loss = 2.26266723\n",
            "Iteration 73, loss = 2.26257627\n",
            "Iteration 74, loss = 2.26247607\n",
            "Iteration 75, loss = 2.26239868\n",
            "Iteration 76, loss = 2.26229165\n",
            "Iteration 77, loss = 2.26222483\n",
            "Iteration 78, loss = 2.26213579\n",
            "Iteration 79, loss = 2.26207234\n",
            "Iteration 80, loss = 2.26196459\n",
            "Iteration 81, loss = 2.26187695\n",
            "Iteration 82, loss = 2.26179974\n",
            "Iteration 83, loss = 2.26174010\n",
            "Iteration 84, loss = 2.26166691\n",
            "Iteration 85, loss = 2.26158089\n",
            "Iteration 86, loss = 2.26149798\n",
            "Iteration 87, loss = 2.26143590\n",
            "Iteration 88, loss = 2.26139662\n",
            "Iteration 89, loss = 2.26128536\n",
            "Iteration 90, loss = 2.26125293\n",
            "Iteration 91, loss = 2.26119764\n",
            "Iteration 92, loss = 2.26113065\n",
            "Iteration 93, loss = 2.26106056\n",
            "Iteration 94, loss = 2.26099795\n",
            "Iteration 95, loss = 2.26094963\n",
            "Iteration 96, loss = 2.26089759\n",
            "Iteration 97, loss = 2.26083197\n",
            "Iteration 98, loss = 2.26080142\n",
            "Iteration 99, loss = 2.26072145\n",
            "Iteration 100, loss = 2.26069407\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
            "[CV] END activation=relu, alpha=0, hidden_layer_sizes=(24, 24), learning_rate=adaptive, max_iter=100, solver=sgd, validation_fraction=0.3, verbose=True; total time= 1.3min\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  ConvergenceWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1, loss = 2.30833747\n",
            "Iteration 2, loss = 2.29698817\n",
            "Iteration 3, loss = 2.29496563\n",
            "Iteration 4, loss = 2.29322456\n",
            "Iteration 5, loss = 2.29124227\n",
            "Iteration 6, loss = 2.28896510\n",
            "Iteration 7, loss = 2.28643101\n",
            "Iteration 8, loss = 2.28382366\n",
            "Iteration 9, loss = 2.28145428\n",
            "Iteration 10, loss = 2.27950601\n",
            "Iteration 11, loss = 2.27802263\n",
            "Iteration 12, loss = 2.27692537\n",
            "Iteration 13, loss = 2.27609092\n",
            "Iteration 14, loss = 2.27546211\n",
            "Iteration 15, loss = 2.27495691\n",
            "Iteration 16, loss = 2.27453227\n",
            "Iteration 17, loss = 2.27417118\n",
            "Iteration 18, loss = 2.27384670\n",
            "Iteration 19, loss = 2.27353315\n",
            "Iteration 20, loss = 2.27325720\n",
            "Iteration 21, loss = 2.27298716\n",
            "Iteration 22, loss = 2.27269399\n",
            "Iteration 23, loss = 2.27246394\n",
            "Iteration 24, loss = 2.27221472\n",
            "Iteration 25, loss = 2.27195837\n",
            "Iteration 26, loss = 2.27173660\n",
            "Iteration 27, loss = 2.27153584\n",
            "Iteration 28, loss = 2.27134005\n",
            "Iteration 29, loss = 2.27115407\n",
            "Iteration 30, loss = 2.27096936\n",
            "Iteration 31, loss = 2.27079786\n",
            "Iteration 32, loss = 2.27062168\n",
            "Iteration 33, loss = 2.27046633\n",
            "Iteration 34, loss = 2.27028772\n",
            "Iteration 35, loss = 2.27015499\n",
            "Iteration 36, loss = 2.26999671\n",
            "Iteration 37, loss = 2.26985679\n",
            "Iteration 38, loss = 2.26971762\n",
            "Iteration 39, loss = 2.26957234\n",
            "Iteration 40, loss = 2.26943534\n",
            "Iteration 41, loss = 2.26930993\n",
            "Iteration 42, loss = 2.26918526\n",
            "Iteration 43, loss = 2.26904186\n",
            "Iteration 44, loss = 2.26891124\n",
            "Iteration 45, loss = 2.26878572\n",
            "Iteration 46, loss = 2.26865202\n",
            "Iteration 47, loss = 2.26853750\n",
            "Iteration 48, loss = 2.26839798\n",
            "Iteration 49, loss = 2.26829034\n",
            "Iteration 50, loss = 2.26815460\n",
            "Iteration 51, loss = 2.26804340\n",
            "Iteration 52, loss = 2.26791264\n",
            "Iteration 53, loss = 2.26779847\n",
            "Iteration 54, loss = 2.26768194\n",
            "Iteration 55, loss = 2.26756608\n",
            "Iteration 56, loss = 2.26745496\n",
            "Iteration 57, loss = 2.26731990\n",
            "Iteration 58, loss = 2.26719654\n",
            "Iteration 59, loss = 2.26708153\n",
            "Iteration 60, loss = 2.26696824\n",
            "Iteration 61, loss = 2.26684260\n",
            "Iteration 62, loss = 2.26673688\n",
            "Iteration 63, loss = 2.26662944\n",
            "Iteration 64, loss = 2.26650658\n",
            "Iteration 65, loss = 2.26640524\n",
            "Iteration 66, loss = 2.26630762\n",
            "Iteration 67, loss = 2.26619318\n",
            "Iteration 68, loss = 2.26609361\n",
            "Iteration 69, loss = 2.26598395\n",
            "Iteration 70, loss = 2.26586935\n",
            "Iteration 71, loss = 2.26577294\n",
            "Iteration 72, loss = 2.26570658\n",
            "Iteration 73, loss = 2.26559038\n",
            "Iteration 74, loss = 2.26551613\n",
            "Iteration 75, loss = 2.26542432\n",
            "Iteration 76, loss = 2.26535314\n",
            "Iteration 77, loss = 2.26526502\n",
            "Iteration 78, loss = 2.26517720\n",
            "Iteration 79, loss = 2.26509870\n",
            "Iteration 80, loss = 2.26501331\n",
            "Iteration 81, loss = 2.26493638\n",
            "Iteration 82, loss = 2.26486115\n",
            "Iteration 83, loss = 2.26477904\n",
            "Iteration 84, loss = 2.26471313\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
            "Iteration 85, loss = 2.26459477\n",
            "Iteration 86, loss = 2.26457943\n",
            "Iteration 87, loss = 2.26456208\n",
            "Iteration 88, loss = 2.26454508\n",
            "Iteration 89, loss = 2.26453086\n",
            "Iteration 90, loss = 2.26451534\n",
            "Iteration 91, loss = 2.26450196\n",
            "Iteration 92, loss = 2.26448787\n",
            "Iteration 93, loss = 2.26447083\n",
            "Iteration 94, loss = 2.26445770\n",
            "Iteration 95, loss = 2.26444427\n",
            "Iteration 96, loss = 2.26443215\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000040\n",
            "Iteration 97, loss = 2.26440372\n",
            "Iteration 98, loss = 2.26440088\n",
            "Iteration 99, loss = 2.26439741\n",
            "Iteration 100, loss = 2.26439503\n",
            "[CV] END activation=relu, alpha=0, hidden_layer_sizes=(24, 24), learning_rate=adaptive, max_iter=100, solver=sgd, validation_fraction=0.3, verbose=True; total time= 1.3min\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  ConvergenceWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1, loss = 2.30456294\n",
            "Iteration 2, loss = 2.29113460\n",
            "Iteration 3, loss = 2.28684288\n",
            "Iteration 4, loss = 2.28331314\n",
            "Iteration 5, loss = 2.28039904\n",
            "Iteration 6, loss = 2.27805560\n",
            "Iteration 7, loss = 2.27623015\n",
            "Iteration 8, loss = 2.27488055\n",
            "Iteration 9, loss = 2.27391240\n",
            "Iteration 10, loss = 2.27318027\n",
            "Iteration 11, loss = 2.27264525\n",
            "Iteration 12, loss = 2.27219616\n",
            "Iteration 13, loss = 2.27186161\n",
            "Iteration 14, loss = 2.27155261\n",
            "Iteration 15, loss = 2.27126775\n",
            "Iteration 16, loss = 2.27100032\n",
            "Iteration 17, loss = 2.27075170\n",
            "Iteration 18, loss = 2.27050662\n",
            "Iteration 19, loss = 2.27026288\n",
            "Iteration 20, loss = 2.27000684\n",
            "Iteration 21, loss = 2.26974441\n",
            "Iteration 22, loss = 2.26949773\n",
            "Iteration 23, loss = 2.26921893\n",
            "Iteration 24, loss = 2.26892890\n",
            "Iteration 25, loss = 2.26865831\n",
            "Iteration 26, loss = 2.26838819\n",
            "Iteration 27, loss = 2.26813034\n",
            "Iteration 28, loss = 2.26786149\n",
            "Iteration 29, loss = 2.26760386\n",
            "Iteration 30, loss = 2.26735453\n",
            "Iteration 31, loss = 2.26713109\n",
            "Iteration 32, loss = 2.26688246\n",
            "Iteration 33, loss = 2.26667668\n",
            "Iteration 34, loss = 2.26647170\n",
            "Iteration 35, loss = 2.26627003\n",
            "Iteration 36, loss = 2.26610381\n",
            "Iteration 37, loss = 2.26592284\n",
            "Iteration 38, loss = 2.26577287\n",
            "Iteration 39, loss = 2.26561860\n",
            "Iteration 40, loss = 2.26546453\n",
            "Iteration 41, loss = 2.26530830\n",
            "Iteration 42, loss = 2.26519652\n",
            "Iteration 43, loss = 2.26505119\n",
            "Iteration 44, loss = 2.26492224\n",
            "Iteration 45, loss = 2.26482043\n",
            "Iteration 46, loss = 2.26468247\n",
            "Iteration 47, loss = 2.26456390\n",
            "Iteration 48, loss = 2.26443564\n",
            "Iteration 49, loss = 2.26429758\n",
            "Iteration 50, loss = 2.26416756\n",
            "Iteration 51, loss = 2.26405559\n",
            "Iteration 52, loss = 2.26394094\n",
            "Iteration 53, loss = 2.26379940\n",
            "Iteration 54, loss = 2.26369110\n",
            "Iteration 55, loss = 2.26354895\n",
            "Iteration 56, loss = 2.26342411\n",
            "Iteration 57, loss = 2.26329227\n",
            "Iteration 58, loss = 2.26317297\n",
            "Iteration 59, loss = 2.26303138\n",
            "Iteration 60, loss = 2.26289145\n",
            "Iteration 61, loss = 2.26277185\n",
            "Iteration 62, loss = 2.26262672\n",
            "Iteration 63, loss = 2.26248988\n",
            "Iteration 64, loss = 2.26235730\n",
            "Iteration 65, loss = 2.26220937\n",
            "Iteration 66, loss = 2.26207473\n",
            "Iteration 67, loss = 2.26194073\n",
            "Iteration 68, loss = 2.26181828\n",
            "Iteration 69, loss = 2.26166721\n",
            "Iteration 70, loss = 2.26154431\n",
            "Iteration 71, loss = 2.26142048\n",
            "Iteration 72, loss = 2.26126337\n",
            "Iteration 73, loss = 2.26110534\n",
            "Iteration 74, loss = 2.26100573\n",
            "Iteration 75, loss = 2.26085838\n",
            "Iteration 76, loss = 2.26073971\n",
            "Iteration 77, loss = 2.26058949\n",
            "Iteration 78, loss = 2.26047926\n",
            "Iteration 79, loss = 2.26034222\n",
            "Iteration 80, loss = 2.26021057\n",
            "Iteration 81, loss = 2.26007230\n",
            "Iteration 82, loss = 2.25997424\n",
            "Iteration 83, loss = 2.25986305\n",
            "Iteration 84, loss = 2.25974943\n",
            "Iteration 85, loss = 2.25964658\n",
            "Iteration 86, loss = 2.25955576\n",
            "Iteration 87, loss = 2.25944612\n",
            "Iteration 88, loss = 2.25936175\n",
            "Iteration 89, loss = 2.25928118\n",
            "Iteration 90, loss = 2.25918034\n",
            "Iteration 91, loss = 2.25912322\n",
            "Iteration 92, loss = 2.25903347\n",
            "Iteration 93, loss = 2.25896120\n",
            "Iteration 94, loss = 2.25887834\n",
            "Iteration 95, loss = 2.25881765\n",
            "Iteration 96, loss = 2.25875274\n",
            "Iteration 97, loss = 2.25865738\n",
            "Iteration 98, loss = 2.25859874\n",
            "Iteration 99, loss = 2.25853467\n",
            "Iteration 100, loss = 2.25845845\n",
            "[CV] END activation=relu, alpha=0, hidden_layer_sizes=(24, 24), learning_rate=adaptive, max_iter=100, solver=sgd, validation_fraction=0.3, verbose=True; total time= 1.3min\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  ConvergenceWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1, loss = 2.31056867\n",
            "Iteration 2, loss = 2.29639599\n",
            "Iteration 3, loss = 2.29295610\n",
            "Iteration 4, loss = 2.28962098\n",
            "Iteration 5, loss = 2.28628002\n",
            "Iteration 6, loss = 2.28320918\n",
            "Iteration 7, loss = 2.28055580\n",
            "Iteration 8, loss = 2.27848883\n",
            "Iteration 9, loss = 2.27698293\n",
            "Iteration 10, loss = 2.27593736\n",
            "Iteration 11, loss = 2.27521381\n",
            "Iteration 12, loss = 2.27468624\n",
            "Iteration 13, loss = 2.27424997\n",
            "Iteration 14, loss = 2.27388457\n",
            "Iteration 15, loss = 2.27354748\n",
            "Iteration 16, loss = 2.27324940\n",
            "Iteration 17, loss = 2.27297597\n",
            "Iteration 18, loss = 2.27269814\n",
            "Iteration 19, loss = 2.27243015\n",
            "Iteration 20, loss = 2.27217284\n",
            "Iteration 21, loss = 2.27190328\n",
            "Iteration 22, loss = 2.27164946\n",
            "Iteration 23, loss = 2.27138750\n",
            "Iteration 24, loss = 2.27112817\n",
            "Iteration 25, loss = 2.27084989\n",
            "Iteration 26, loss = 2.27059389\n",
            "Iteration 27, loss = 2.27032577\n",
            "Iteration 28, loss = 2.27004825\n",
            "Iteration 29, loss = 2.26980221\n",
            "Iteration 30, loss = 2.26954697\n",
            "Iteration 31, loss = 2.26928431\n",
            "Iteration 32, loss = 2.26903912\n",
            "Iteration 33, loss = 2.26879898\n",
            "Iteration 34, loss = 2.26855022\n",
            "Iteration 35, loss = 2.26835782\n",
            "Iteration 36, loss = 2.26813987\n",
            "Iteration 37, loss = 2.26795092\n",
            "Iteration 38, loss = 2.26775778\n",
            "Iteration 39, loss = 2.26758026\n",
            "Iteration 40, loss = 2.26740678\n",
            "Iteration 41, loss = 2.26725756\n",
            "Iteration 42, loss = 2.26708623\n",
            "Iteration 43, loss = 2.26693698\n",
            "Iteration 44, loss = 2.26677780\n",
            "Iteration 45, loss = 2.26662836\n",
            "Iteration 46, loss = 2.26650118\n",
            "Iteration 47, loss = 2.26633050\n",
            "Iteration 48, loss = 2.26620673\n",
            "Iteration 49, loss = 2.26605574\n",
            "Iteration 50, loss = 2.26590346\n",
            "Iteration 51, loss = 2.26578557\n",
            "Iteration 52, loss = 2.26565386\n",
            "Iteration 53, loss = 2.26549993\n",
            "Iteration 54, loss = 2.26539178\n",
            "Iteration 55, loss = 2.26524387\n",
            "Iteration 56, loss = 2.26511300\n",
            "Iteration 57, loss = 2.26497087\n",
            "Iteration 58, loss = 2.26484705\n",
            "Iteration 59, loss = 2.26469793\n",
            "Iteration 60, loss = 2.26457703\n",
            "Iteration 61, loss = 2.26446119\n",
            "Iteration 62, loss = 2.26430488\n",
            "Iteration 63, loss = 2.26415691\n",
            "Iteration 64, loss = 2.26405128\n",
            "Iteration 65, loss = 2.26389872\n",
            "Iteration 66, loss = 2.26375267\n",
            "Iteration 67, loss = 2.26361428\n",
            "Iteration 68, loss = 2.26350698\n",
            "Iteration 69, loss = 2.26337256\n",
            "Iteration 70, loss = 2.26322712\n",
            "Iteration 71, loss = 2.26309476\n",
            "Iteration 72, loss = 2.26296535\n",
            "Iteration 73, loss = 2.26282723\n",
            "Iteration 74, loss = 2.26271472\n",
            "Iteration 75, loss = 2.26257908\n",
            "Iteration 76, loss = 2.26243365\n",
            "Iteration 77, loss = 2.26231229\n",
            "Iteration 78, loss = 2.26218810\n",
            "Iteration 79, loss = 2.26208453\n",
            "Iteration 80, loss = 2.26198779\n",
            "Iteration 81, loss = 2.26184482\n",
            "Iteration 82, loss = 2.26174101\n",
            "Iteration 83, loss = 2.26161679\n",
            "Iteration 84, loss = 2.26150910\n",
            "Iteration 85, loss = 2.26139857\n",
            "Iteration 86, loss = 2.26128943\n",
            "Iteration 87, loss = 2.26119783\n",
            "Iteration 88, loss = 2.26107506\n",
            "Iteration 89, loss = 2.26098936\n",
            "Iteration 90, loss = 2.26086477\n",
            "Iteration 91, loss = 2.26076020\n",
            "Iteration 92, loss = 2.26066497\n",
            "Iteration 93, loss = 2.26055398\n",
            "Iteration 94, loss = 2.26048058\n",
            "Iteration 95, loss = 2.26039545\n",
            "Iteration 96, loss = 2.26026504\n",
            "Iteration 97, loss = 2.26023461\n",
            "Iteration 98, loss = 2.26016969\n",
            "Iteration 99, loss = 2.26009872\n",
            "Iteration 100, loss = 2.25999524\n",
            "[CV] END activation=relu, alpha=0, hidden_layer_sizes=(24, 24), learning_rate=adaptive, max_iter=100, solver=sgd, validation_fraction=0.3, verbose=True; total time= 1.3min\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  ConvergenceWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1, loss = 2.31412295\n",
            "Iteration 2, loss = 2.30446966\n",
            "Iteration 3, loss = 2.30106608\n",
            "Iteration 4, loss = 2.29833563\n",
            "Iteration 5, loss = 2.29582065\n",
            "Iteration 6, loss = 2.29334007\n",
            "Iteration 7, loss = 2.29095206\n",
            "Iteration 8, loss = 2.28878516\n",
            "Iteration 9, loss = 2.28691216\n",
            "Iteration 10, loss = 2.28538983\n",
            "Iteration 11, loss = 2.28419666\n",
            "Iteration 12, loss = 2.28324166\n",
            "Iteration 13, loss = 2.28251407\n",
            "Iteration 14, loss = 2.28193799\n",
            "Iteration 15, loss = 2.28146085\n",
            "Iteration 16, loss = 2.28104969\n",
            "Iteration 17, loss = 2.28065117\n",
            "Iteration 18, loss = 2.28032895\n",
            "Iteration 19, loss = 2.27998836\n",
            "Iteration 20, loss = 2.27970275\n",
            "Iteration 21, loss = 2.27942248\n",
            "Iteration 22, loss = 2.27914198\n",
            "Iteration 23, loss = 2.27885582\n",
            "Iteration 24, loss = 2.27859131\n",
            "Iteration 25, loss = 2.27832437\n",
            "Iteration 26, loss = 2.27805512\n",
            "Iteration 27, loss = 2.27775047\n",
            "Iteration 28, loss = 2.27747692\n",
            "Iteration 29, loss = 2.27719774\n",
            "Iteration 30, loss = 2.27692826\n",
            "Iteration 31, loss = 2.27663767\n",
            "Iteration 32, loss = 2.27633452\n",
            "Iteration 33, loss = 2.27603582\n",
            "Iteration 34, loss = 2.27572314\n",
            "Iteration 35, loss = 2.27543570\n",
            "Iteration 36, loss = 2.27511170\n",
            "Iteration 37, loss = 2.27483993\n",
            "Iteration 38, loss = 2.27451978\n",
            "Iteration 39, loss = 2.27424793\n",
            "Iteration 40, loss = 2.27395814\n",
            "Iteration 41, loss = 2.27369578\n",
            "Iteration 42, loss = 2.27341727\n",
            "Iteration 43, loss = 2.27315708\n",
            "Iteration 44, loss = 2.27291484\n",
            "Iteration 45, loss = 2.27268977\n",
            "Iteration 46, loss = 2.27247382\n",
            "Iteration 47, loss = 2.27225256\n",
            "Iteration 48, loss = 2.27205401\n",
            "Iteration 49, loss = 2.27185209\n",
            "Iteration 50, loss = 2.27165938\n",
            "Iteration 51, loss = 2.27147831\n",
            "Iteration 52, loss = 2.27129502\n",
            "Iteration 53, loss = 2.27114181\n",
            "Iteration 54, loss = 2.27095698\n",
            "Iteration 55, loss = 2.27080392\n",
            "Iteration 56, loss = 2.27064639\n",
            "Iteration 57, loss = 2.27049108\n",
            "Iteration 58, loss = 2.27033768\n",
            "Iteration 59, loss = 2.27021548\n",
            "Iteration 60, loss = 2.27008567\n",
            "Iteration 61, loss = 2.26994129\n",
            "Iteration 62, loss = 2.26981277\n",
            "Iteration 63, loss = 2.26971866\n",
            "Iteration 64, loss = 2.26958235\n",
            "Iteration 65, loss = 2.26946636\n",
            "Iteration 66, loss = 2.26936190\n",
            "Iteration 67, loss = 2.26925428\n",
            "Iteration 68, loss = 2.26917223\n",
            "Iteration 69, loss = 2.26906136\n",
            "Iteration 70, loss = 2.26896247\n",
            "Iteration 71, loss = 2.26886744\n",
            "Iteration 72, loss = 2.26877633\n",
            "Iteration 73, loss = 2.26870849\n",
            "Iteration 74, loss = 2.26862557\n",
            "Iteration 75, loss = 2.26854925\n",
            "Iteration 76, loss = 2.26846676\n",
            "Iteration 77, loss = 2.26838301\n",
            "Iteration 78, loss = 2.26833504\n",
            "Iteration 79, loss = 2.26825007\n",
            "Iteration 80, loss = 2.26818417\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
            "Iteration 81, loss = 2.26808594\n",
            "Iteration 82, loss = 2.26804787\n",
            "Iteration 83, loss = 2.26802972\n",
            "Iteration 84, loss = 2.26801555\n",
            "Iteration 85, loss = 2.26800574\n",
            "Iteration 86, loss = 2.26799566\n",
            "Iteration 87, loss = 2.26798226\n",
            "Iteration 88, loss = 2.26797032\n",
            "Iteration 89, loss = 2.26795091\n",
            "Iteration 90, loss = 2.26794202\n",
            "Iteration 91, loss = 2.26793075\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000040\n",
            "Iteration 92, loss = 2.26790238\n",
            "Iteration 93, loss = 2.26789800\n",
            "Iteration 94, loss = 2.26789506\n",
            "Iteration 95, loss = 2.26789239\n",
            "Iteration 96, loss = 2.26788959\n",
            "Iteration 97, loss = 2.26788722\n",
            "Iteration 98, loss = 2.26788485\n",
            "Iteration 99, loss = 2.26788143\n",
            "Iteration 100, loss = 2.26788014\n",
            "[CV] END activation=relu, alpha=0.05, hidden_layer_sizes=(32, 32), learning_rate=adaptive, max_iter=100, solver=sgd, validation_fraction=0.3, verbose=True; total time= 1.4min\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  ConvergenceWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1, loss = 2.31470649\n",
            "Iteration 2, loss = 2.30133251\n",
            "Iteration 3, loss = 2.29674136\n",
            "Iteration 4, loss = 2.29300281\n",
            "Iteration 5, loss = 2.28991260\n",
            "Iteration 6, loss = 2.28752125\n",
            "Iteration 7, loss = 2.28575902\n",
            "Iteration 8, loss = 2.28445188\n",
            "Iteration 9, loss = 2.28349974\n",
            "Iteration 10, loss = 2.28277956\n",
            "Iteration 11, loss = 2.28220808\n",
            "Iteration 12, loss = 2.28176579\n",
            "Iteration 13, loss = 2.28141099\n",
            "Iteration 14, loss = 2.28107676\n",
            "Iteration 15, loss = 2.28080023\n",
            "Iteration 16, loss = 2.28054089\n",
            "Iteration 17, loss = 2.28028616\n",
            "Iteration 18, loss = 2.28007554\n",
            "Iteration 19, loss = 2.27984613\n",
            "Iteration 20, loss = 2.27964273\n",
            "Iteration 21, loss = 2.27945273\n",
            "Iteration 22, loss = 2.27924544\n",
            "Iteration 23, loss = 2.27907144\n",
            "Iteration 24, loss = 2.27886408\n",
            "Iteration 25, loss = 2.27870830\n",
            "Iteration 26, loss = 2.27851014\n",
            "Iteration 27, loss = 2.27834208\n",
            "Iteration 28, loss = 2.27815100\n",
            "Iteration 29, loss = 2.27797685\n",
            "Iteration 30, loss = 2.27777355\n",
            "Iteration 31, loss = 2.27758395\n",
            "Iteration 32, loss = 2.27740240\n",
            "Iteration 33, loss = 2.27719712\n",
            "Iteration 34, loss = 2.27700519\n",
            "Iteration 35, loss = 2.27678932\n",
            "Iteration 36, loss = 2.27659126\n",
            "Iteration 37, loss = 2.27636625\n",
            "Iteration 38, loss = 2.27614852\n",
            "Iteration 39, loss = 2.27593598\n",
            "Iteration 40, loss = 2.27568539\n",
            "Iteration 41, loss = 2.27546685\n",
            "Iteration 42, loss = 2.27523735\n",
            "Iteration 43, loss = 2.27500469\n",
            "Iteration 44, loss = 2.27477878\n",
            "Iteration 45, loss = 2.27454357\n",
            "Iteration 46, loss = 2.27431214\n",
            "Iteration 47, loss = 2.27408924\n",
            "Iteration 48, loss = 2.27385993\n",
            "Iteration 49, loss = 2.27364092\n",
            "Iteration 50, loss = 2.27344169\n",
            "Iteration 51, loss = 2.27319500\n",
            "Iteration 52, loss = 2.27298831\n",
            "Iteration 53, loss = 2.27277153\n",
            "Iteration 54, loss = 2.27255201\n",
            "Iteration 55, loss = 2.27235502\n",
            "Iteration 56, loss = 2.27214367\n",
            "Iteration 57, loss = 2.27194646\n",
            "Iteration 58, loss = 2.27178025\n",
            "Iteration 59, loss = 2.27158915\n",
            "Iteration 60, loss = 2.27140407\n",
            "Iteration 61, loss = 2.27126005\n",
            "Iteration 62, loss = 2.27109455\n",
            "Iteration 63, loss = 2.27091700\n",
            "Iteration 64, loss = 2.27079387\n",
            "Iteration 65, loss = 2.27066356\n",
            "Iteration 66, loss = 2.27051596\n",
            "Iteration 67, loss = 2.27039519\n",
            "Iteration 68, loss = 2.27026665\n",
            "Iteration 69, loss = 2.27016788\n",
            "Iteration 70, loss = 2.27004829\n",
            "Iteration 71, loss = 2.26994766\n",
            "Iteration 72, loss = 2.26981739\n",
            "Iteration 73, loss = 2.26975888\n",
            "Iteration 74, loss = 2.26964750\n",
            "Iteration 75, loss = 2.26956502\n",
            "Iteration 76, loss = 2.26947333\n",
            "Iteration 77, loss = 2.26941614\n",
            "Iteration 78, loss = 2.26930687\n",
            "Iteration 79, loss = 2.26923393\n",
            "Iteration 80, loss = 2.26914211\n",
            "Iteration 81, loss = 2.26905837\n",
            "Iteration 82, loss = 2.26895446\n",
            "Iteration 83, loss = 2.26891459\n",
            "Iteration 84, loss = 2.26883778\n",
            "Iteration 85, loss = 2.26875833\n",
            "Iteration 86, loss = 2.26867928\n",
            "Iteration 87, loss = 2.26861009\n",
            "Iteration 88, loss = 2.26852845\n",
            "Iteration 89, loss = 2.26848056\n",
            "Iteration 90, loss = 2.26841875\n",
            "Iteration 91, loss = 2.26834329\n",
            "Iteration 92, loss = 2.26829118\n",
            "Iteration 93, loss = 2.26823226\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
            "Iteration 94, loss = 2.26809466\n",
            "Iteration 95, loss = 2.26807338\n",
            "Iteration 96, loss = 2.26806201\n",
            "Iteration 97, loss = 2.26804892\n",
            "Iteration 98, loss = 2.26803778\n",
            "Iteration 99, loss = 2.26802304\n",
            "Iteration 100, loss = 2.26801721\n",
            "[CV] END activation=relu, alpha=0.05, hidden_layer_sizes=(32, 32), learning_rate=adaptive, max_iter=100, solver=sgd, validation_fraction=0.3, verbose=True; total time= 1.4min\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  ConvergenceWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1, loss = 2.31823991\n",
            "Iteration 2, loss = 2.30446124\n",
            "Iteration 3, loss = 2.30182305\n",
            "Iteration 4, loss = 2.29956459\n",
            "Iteration 5, loss = 2.29718954\n",
            "Iteration 6, loss = 2.29470808\n",
            "Iteration 7, loss = 2.29228348\n",
            "Iteration 8, loss = 2.29005274\n",
            "Iteration 9, loss = 2.28805830\n",
            "Iteration 10, loss = 2.28640778\n",
            "Iteration 11, loss = 2.28503970\n",
            "Iteration 12, loss = 2.28392264\n",
            "Iteration 13, loss = 2.28302482\n",
            "Iteration 14, loss = 2.28233349\n",
            "Iteration 15, loss = 2.28174261\n",
            "Iteration 16, loss = 2.28125971\n",
            "Iteration 17, loss = 2.28084136\n",
            "Iteration 18, loss = 2.28047378\n",
            "Iteration 19, loss = 2.28013118\n",
            "Iteration 20, loss = 2.27982385\n",
            "Iteration 21, loss = 2.27951589\n",
            "Iteration 22, loss = 2.27925323\n",
            "Iteration 23, loss = 2.27900276\n",
            "Iteration 24, loss = 2.27875999\n",
            "Iteration 25, loss = 2.27852125\n",
            "Iteration 26, loss = 2.27832316\n",
            "Iteration 27, loss = 2.27810682\n",
            "Iteration 28, loss = 2.27791109\n",
            "Iteration 29, loss = 2.27772827\n",
            "Iteration 30, loss = 2.27754750\n",
            "Iteration 31, loss = 2.27737860\n",
            "Iteration 32, loss = 2.27719761\n",
            "Iteration 33, loss = 2.27704539\n",
            "Iteration 34, loss = 2.27688693\n",
            "Iteration 35, loss = 2.27671640\n",
            "Iteration 36, loss = 2.27658674\n",
            "Iteration 37, loss = 2.27642784\n",
            "Iteration 38, loss = 2.27629114\n",
            "Iteration 39, loss = 2.27615503\n",
            "Iteration 40, loss = 2.27601397\n",
            "Iteration 41, loss = 2.27587711\n",
            "Iteration 42, loss = 2.27572476\n",
            "Iteration 43, loss = 2.27559553\n",
            "Iteration 44, loss = 2.27548467\n",
            "Iteration 45, loss = 2.27534413\n",
            "Iteration 46, loss = 2.27521008\n",
            "Iteration 47, loss = 2.27508633\n",
            "Iteration 48, loss = 2.27495799\n",
            "Iteration 49, loss = 2.27483598\n",
            "Iteration 50, loss = 2.27469655\n",
            "Iteration 51, loss = 2.27458666\n",
            "Iteration 52, loss = 2.27445654\n",
            "Iteration 53, loss = 2.27433876\n",
            "Iteration 54, loss = 2.27420557\n",
            "Iteration 55, loss = 2.27408825\n",
            "Iteration 56, loss = 2.27396616\n",
            "Iteration 57, loss = 2.27383332\n",
            "Iteration 58, loss = 2.27370972\n",
            "Iteration 59, loss = 2.27357401\n",
            "Iteration 60, loss = 2.27346561\n",
            "Iteration 61, loss = 2.27332605\n",
            "Iteration 62, loss = 2.27321011\n",
            "Iteration 63, loss = 2.27309524\n",
            "Iteration 64, loss = 2.27296592\n",
            "Iteration 65, loss = 2.27283859\n",
            "Iteration 66, loss = 2.27270918\n",
            "Iteration 67, loss = 2.27259013\n",
            "Iteration 68, loss = 2.27246915\n",
            "Iteration 69, loss = 2.27233133\n",
            "Iteration 70, loss = 2.27225168\n",
            "Iteration 71, loss = 2.27211626\n",
            "Iteration 72, loss = 2.27200878\n",
            "Iteration 73, loss = 2.27189610\n",
            "Iteration 74, loss = 2.27177613\n",
            "Iteration 75, loss = 2.27166937\n",
            "Iteration 76, loss = 2.27156670\n",
            "Iteration 77, loss = 2.27146062\n",
            "Iteration 78, loss = 2.27133863\n",
            "Iteration 79, loss = 2.27124466\n",
            "Iteration 80, loss = 2.27112898\n",
            "Iteration 81, loss = 2.27102659\n",
            "Iteration 82, loss = 2.27093895\n",
            "Iteration 83, loss = 2.27082644\n",
            "Iteration 84, loss = 2.27075329\n",
            "Iteration 85, loss = 2.27065994\n",
            "Iteration 86, loss = 2.27058310\n",
            "Iteration 87, loss = 2.27048662\n",
            "Iteration 88, loss = 2.27038514\n",
            "Iteration 89, loss = 2.27031643\n",
            "Iteration 90, loss = 2.27022305\n",
            "Iteration 91, loss = 2.27014225\n",
            "Iteration 92, loss = 2.27007079\n",
            "Iteration 93, loss = 2.26999054\n",
            "Iteration 94, loss = 2.26990251\n",
            "Iteration 95, loss = 2.26983096\n",
            "Iteration 96, loss = 2.26975840\n",
            "Iteration 97, loss = 2.26968273\n",
            "Iteration 98, loss = 2.26962025\n",
            "Iteration 99, loss = 2.26952587\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
            "Iteration 100, loss = 2.26947698\n",
            "[CV] END activation=relu, alpha=0.05, hidden_layer_sizes=(32, 32), learning_rate=adaptive, max_iter=100, solver=sgd, validation_fraction=0.3, verbose=True; total time= 1.4min\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  ConvergenceWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1, loss = 2.32000323\n",
            "Iteration 2, loss = 2.30701624\n",
            "Iteration 3, loss = 2.30389348\n",
            "Iteration 4, loss = 2.30127229\n",
            "Iteration 5, loss = 2.29843133\n",
            "Iteration 6, loss = 2.29552068\n",
            "Iteration 7, loss = 2.29280017\n",
            "Iteration 8, loss = 2.29019884\n",
            "Iteration 9, loss = 2.28774861\n",
            "Iteration 10, loss = 2.28553962\n",
            "Iteration 11, loss = 2.28361141\n",
            "Iteration 12, loss = 2.28201472\n",
            "Iteration 13, loss = 2.28073606\n",
            "Iteration 14, loss = 2.27970836\n",
            "Iteration 15, loss = 2.27890537\n",
            "Iteration 16, loss = 2.27828965\n",
            "Iteration 17, loss = 2.27776301\n",
            "Iteration 18, loss = 2.27734403\n",
            "Iteration 19, loss = 2.27701435\n",
            "Iteration 20, loss = 2.27665581\n",
            "Iteration 21, loss = 2.27634794\n",
            "Iteration 22, loss = 2.27608934\n",
            "Iteration 23, loss = 2.27580810\n",
            "Iteration 24, loss = 2.27555938\n",
            "Iteration 25, loss = 2.27532626\n",
            "Iteration 26, loss = 2.27510762\n",
            "Iteration 27, loss = 2.27487344\n",
            "Iteration 28, loss = 2.27467751\n",
            "Iteration 29, loss = 2.27446728\n",
            "Iteration 30, loss = 2.27425325\n",
            "Iteration 31, loss = 2.27405782\n",
            "Iteration 32, loss = 2.27387545\n",
            "Iteration 33, loss = 2.27374205\n",
            "Iteration 34, loss = 2.27358587\n",
            "Iteration 35, loss = 2.27346205\n",
            "Iteration 36, loss = 2.27333495\n",
            "Iteration 37, loss = 2.27321243\n",
            "Iteration 38, loss = 2.27306437\n",
            "Iteration 39, loss = 2.27295237\n",
            "Iteration 40, loss = 2.27282359\n",
            "Iteration 41, loss = 2.27271183\n",
            "Iteration 42, loss = 2.27257256\n",
            "Iteration 43, loss = 2.27248980\n",
            "Iteration 44, loss = 2.27237924\n",
            "Iteration 45, loss = 2.27225970\n",
            "Iteration 46, loss = 2.27214362\n",
            "Iteration 47, loss = 2.27205600\n",
            "Iteration 48, loss = 2.27194360\n",
            "Iteration 49, loss = 2.27181039\n",
            "Iteration 50, loss = 2.27171970\n",
            "Iteration 51, loss = 2.27157878\n",
            "Iteration 52, loss = 2.27146396\n",
            "Iteration 53, loss = 2.27138155\n",
            "Iteration 54, loss = 2.27125495\n",
            "Iteration 55, loss = 2.27114816\n",
            "Iteration 56, loss = 2.27100859\n",
            "Iteration 57, loss = 2.27090317\n",
            "Iteration 58, loss = 2.27078352\n",
            "Iteration 59, loss = 2.27064910\n",
            "Iteration 60, loss = 2.27054182\n",
            "Iteration 61, loss = 2.27041429\n",
            "Iteration 62, loss = 2.27029842\n",
            "Iteration 63, loss = 2.27018031\n",
            "Iteration 64, loss = 2.27005725\n",
            "Iteration 65, loss = 2.26993170\n",
            "Iteration 66, loss = 2.26978607\n",
            "Iteration 67, loss = 2.26967986\n",
            "Iteration 68, loss = 2.26956592\n",
            "Iteration 69, loss = 2.26944961\n",
            "Iteration 70, loss = 2.26931617\n",
            "Iteration 71, loss = 2.26919580\n",
            "Iteration 72, loss = 2.26907903\n",
            "Iteration 73, loss = 2.26894705\n",
            "Iteration 74, loss = 2.26883472\n",
            "Iteration 75, loss = 2.26872994\n",
            "Iteration 76, loss = 2.26858767\n",
            "Iteration 77, loss = 2.26846833\n",
            "Iteration 78, loss = 2.26836800\n",
            "Iteration 79, loss = 2.26826801\n",
            "Iteration 80, loss = 2.26814384\n",
            "Iteration 81, loss = 2.26801876\n",
            "Iteration 82, loss = 2.26792976\n",
            "Iteration 83, loss = 2.26782624\n",
            "Iteration 84, loss = 2.26772355\n",
            "Iteration 85, loss = 2.26763463\n",
            "Iteration 86, loss = 2.26752576\n",
            "Iteration 87, loss = 2.26742100\n",
            "Iteration 88, loss = 2.26733761\n",
            "Iteration 89, loss = 2.26723728\n",
            "Iteration 90, loss = 2.26713103\n",
            "Iteration 91, loss = 2.26704474\n",
            "Iteration 92, loss = 2.26695506\n",
            "Iteration 93, loss = 2.26685677\n",
            "Iteration 94, loss = 2.26676605\n",
            "Iteration 95, loss = 2.26667244\n",
            "Iteration 96, loss = 2.26657627\n",
            "Iteration 97, loss = 2.26650758\n",
            "Iteration 98, loss = 2.26642471\n",
            "Iteration 99, loss = 2.26636273\n",
            "Iteration 100, loss = 2.26625820\n",
            "[CV] END activation=relu, alpha=0.05, hidden_layer_sizes=(32, 32), learning_rate=adaptive, max_iter=100, solver=sgd, validation_fraction=0.3, verbose=True; total time= 1.4min\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  ConvergenceWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1, loss = 2.31904424\n",
            "Iteration 2, loss = 2.30207036\n",
            "Iteration 3, loss = 2.29842637\n",
            "Iteration 4, loss = 2.29493410\n",
            "Iteration 5, loss = 2.29154399\n",
            "Iteration 6, loss = 2.28846465\n",
            "Iteration 7, loss = 2.28579413\n",
            "Iteration 8, loss = 2.28371862\n",
            "Iteration 9, loss = 2.28213970\n",
            "Iteration 10, loss = 2.28097433\n",
            "Iteration 11, loss = 2.28008688\n",
            "Iteration 12, loss = 2.27939789\n",
            "Iteration 13, loss = 2.27880741\n",
            "Iteration 14, loss = 2.27832619\n",
            "Iteration 15, loss = 2.27791998\n",
            "Iteration 16, loss = 2.27752634\n",
            "Iteration 17, loss = 2.27716387\n",
            "Iteration 18, loss = 2.27680231\n",
            "Iteration 19, loss = 2.27650111\n",
            "Iteration 20, loss = 2.27618590\n",
            "Iteration 21, loss = 2.27589792\n",
            "Iteration 22, loss = 2.27561733\n",
            "Iteration 23, loss = 2.27533960\n",
            "Iteration 24, loss = 2.27509119\n",
            "Iteration 25, loss = 2.27486745\n",
            "Iteration 26, loss = 2.27461971\n",
            "Iteration 27, loss = 2.27438451\n",
            "Iteration 28, loss = 2.27417319\n",
            "Iteration 29, loss = 2.27395385\n",
            "Iteration 30, loss = 2.27374052\n",
            "Iteration 31, loss = 2.27354710\n",
            "Iteration 32, loss = 2.27334830\n",
            "Iteration 33, loss = 2.27315492\n",
            "Iteration 34, loss = 2.27299106\n",
            "Iteration 35, loss = 2.27281079\n",
            "Iteration 36, loss = 2.27261962\n",
            "Iteration 37, loss = 2.27248173\n",
            "Iteration 38, loss = 2.27229859\n",
            "Iteration 39, loss = 2.27214143\n",
            "Iteration 40, loss = 2.27197614\n",
            "Iteration 41, loss = 2.27183713\n",
            "Iteration 42, loss = 2.27167627\n",
            "Iteration 43, loss = 2.27152931\n",
            "Iteration 44, loss = 2.27137511\n",
            "Iteration 45, loss = 2.27122569\n",
            "Iteration 46, loss = 2.27105820\n",
            "Iteration 47, loss = 2.27094260\n",
            "Iteration 48, loss = 2.27077537\n",
            "Iteration 49, loss = 2.27062342\n",
            "Iteration 50, loss = 2.27048460\n",
            "Iteration 51, loss = 2.27029107\n",
            "Iteration 52, loss = 2.27017303\n",
            "Iteration 53, loss = 2.27001466\n",
            "Iteration 54, loss = 2.26986445\n",
            "Iteration 55, loss = 2.26971847\n",
            "Iteration 56, loss = 2.26956049\n",
            "Iteration 57, loss = 2.26942743\n",
            "Iteration 58, loss = 2.26928134\n",
            "Iteration 59, loss = 2.26913840\n",
            "Iteration 60, loss = 2.26899034\n",
            "Iteration 61, loss = 2.26886057\n",
            "Iteration 62, loss = 2.26874418\n",
            "Iteration 63, loss = 2.26860501\n",
            "Iteration 64, loss = 2.26847920\n",
            "Iteration 65, loss = 2.26837755\n",
            "Iteration 66, loss = 2.26823814\n",
            "Iteration 67, loss = 2.26812961\n",
            "Iteration 68, loss = 2.26801170\n",
            "Iteration 69, loss = 2.26791371\n",
            "Iteration 70, loss = 2.26781395\n",
            "Iteration 71, loss = 2.26769957\n",
            "Iteration 72, loss = 2.26759950\n",
            "Iteration 73, loss = 2.26748418\n",
            "Iteration 74, loss = 2.26740102\n",
            "Iteration 75, loss = 2.26729995\n",
            "Iteration 76, loss = 2.26723751\n",
            "Iteration 77, loss = 2.26711536\n",
            "Iteration 78, loss = 2.26703369\n",
            "Iteration 79, loss = 2.26697625\n",
            "Iteration 80, loss = 2.26687788\n",
            "Iteration 81, loss = 2.26678298\n",
            "Iteration 82, loss = 2.26672730\n",
            "Iteration 83, loss = 2.26666128\n",
            "Iteration 84, loss = 2.26656963\n",
            "Iteration 85, loss = 2.26651158\n",
            "Iteration 86, loss = 2.26642549\n",
            "Iteration 87, loss = 2.26636222\n",
            "Iteration 88, loss = 2.26630772\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
            "Iteration 89, loss = 2.26618402\n",
            "Iteration 90, loss = 2.26616196\n",
            "Iteration 91, loss = 2.26615051\n",
            "Iteration 92, loss = 2.26613628\n",
            "Iteration 93, loss = 2.26612301\n",
            "Iteration 94, loss = 2.26610490\n",
            "Iteration 95, loss = 2.26609524\n",
            "Iteration 96, loss = 2.26607835\n",
            "Iteration 97, loss = 2.26607123\n",
            "Iteration 98, loss = 2.26605330\n",
            "Iteration 99, loss = 2.26604171\n",
            "Iteration 100, loss = 2.26602764\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000040\n",
            "[CV] END activation=relu, alpha=0.05, hidden_layer_sizes=(32, 32), learning_rate=adaptive, max_iter=100, solver=sgd, validation_fraction=0.3, verbose=True; total time= 1.4min\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  ConvergenceWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1, loss = 2.27460743\n",
            "Iteration 2, loss = 2.26445257\n",
            "Iteration 3, loss = 2.26215181\n",
            "Iteration 4, loss = 2.26070241\n",
            "Iteration 5, loss = 2.25977729\n",
            "Iteration 6, loss = 2.25905194\n",
            "Iteration 7, loss = 2.25857673\n",
            "Iteration 8, loss = 2.25844070\n",
            "Iteration 9, loss = 2.25823111\n",
            "Iteration 10, loss = 2.25806503\n",
            "Iteration 11, loss = 2.25771542\n",
            "Iteration 12, loss = 2.25770516\n",
            "Iteration 13, loss = 2.25751906\n",
            "Iteration 14, loss = 2.25737720\n",
            "Iteration 15, loss = 2.25733598\n",
            "Iteration 16, loss = 2.25717828\n",
            "Iteration 17, loss = 2.25709498\n",
            "Iteration 18, loss = 2.25690386\n",
            "Iteration 19, loss = 2.25693023\n",
            "Iteration 20, loss = 2.25677206\n",
            "Iteration 21, loss = 2.25679760\n",
            "Iteration 22, loss = 2.25678800\n",
            "Iteration 23, loss = 2.25662996\n",
            "Iteration 24, loss = 2.25653752\n",
            "Iteration 25, loss = 2.25662187\n",
            "Iteration 26, loss = 2.25653628\n",
            "Iteration 27, loss = 2.25657106\n",
            "Iteration 28, loss = 2.25627112\n",
            "Iteration 29, loss = 2.25641442\n",
            "Iteration 30, loss = 2.25631793\n",
            "Iteration 31, loss = 2.25626594\n",
            "Iteration 32, loss = 2.25621639\n",
            "Iteration 33, loss = 2.25624135\n",
            "Iteration 34, loss = 2.25612906\n",
            "Iteration 35, loss = 2.25612512\n",
            "Iteration 36, loss = 2.25606705\n",
            "Iteration 37, loss = 2.25607186\n",
            "Iteration 38, loss = 2.25608608\n",
            "Iteration 39, loss = 2.25595361\n",
            "Iteration 40, loss = 2.25598546\n",
            "Iteration 41, loss = 2.25595369\n",
            "Iteration 42, loss = 2.25595873\n",
            "Iteration 43, loss = 2.25587368\n",
            "Iteration 44, loss = 2.25586476\n",
            "Iteration 45, loss = 2.25594027\n",
            "Iteration 46, loss = 2.25582558\n",
            "Iteration 47, loss = 2.25583400\n",
            "Iteration 48, loss = 2.25575291\n",
            "Iteration 49, loss = 2.25572479\n",
            "Iteration 50, loss = 2.25564637\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "[CV] END activation=relu, alpha=0, hidden_layer_sizes=(32, 32), learning_rate=adaptive, max_iter=100, solver=adam, validation_fraction=0.3, verbose=True; total time=  48.0s\n",
            "Iteration 1, loss = 2.27276844\n",
            "Iteration 2, loss = 2.26221057\n",
            "Iteration 3, loss = 2.25996256\n",
            "Iteration 4, loss = 2.25895750\n",
            "Iteration 5, loss = 2.25816404\n",
            "Iteration 6, loss = 2.25757058\n",
            "Iteration 7, loss = 2.25696034\n",
            "Iteration 8, loss = 2.25668899\n",
            "Iteration 9, loss = 2.25648241\n",
            "Iteration 10, loss = 2.25613167\n",
            "Iteration 11, loss = 2.25587373\n",
            "Iteration 12, loss = 2.25564488\n",
            "Iteration 13, loss = 2.25562124\n",
            "Iteration 14, loss = 2.25540701\n",
            "Iteration 15, loss = 2.25543907\n",
            "Iteration 16, loss = 2.25528192\n",
            "Iteration 17, loss = 2.25516744\n",
            "Iteration 18, loss = 2.25509645\n",
            "Iteration 19, loss = 2.25500967\n",
            "Iteration 20, loss = 2.25502395\n",
            "Iteration 21, loss = 2.25493925\n",
            "Iteration 22, loss = 2.25492440\n",
            "Iteration 23, loss = 2.25490352\n",
            "Iteration 24, loss = 2.25474061\n",
            "Iteration 25, loss = 2.25459803\n",
            "Iteration 26, loss = 2.25466640\n",
            "Iteration 27, loss = 2.25455564\n",
            "Iteration 28, loss = 2.25452737\n",
            "Iteration 29, loss = 2.25447594\n",
            "Iteration 30, loss = 2.25457825\n",
            "Iteration 31, loss = 2.25447511\n",
            "Iteration 32, loss = 2.25445372\n",
            "Iteration 33, loss = 2.25437638\n",
            "Iteration 34, loss = 2.25431206\n",
            "Iteration 35, loss = 2.25443249\n",
            "Iteration 36, loss = 2.25426584\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "[CV] END activation=relu, alpha=0, hidden_layer_sizes=(32, 32), learning_rate=adaptive, max_iter=100, solver=adam, validation_fraction=0.3, verbose=True; total time=  34.6s\n",
            "Iteration 1, loss = 2.27795965\n",
            "Iteration 2, loss = 2.26633253\n",
            "Iteration 3, loss = 2.26335687\n",
            "Iteration 4, loss = 2.26182040\n",
            "Iteration 5, loss = 2.26084990\n",
            "Iteration 6, loss = 2.26039273\n",
            "Iteration 7, loss = 2.25994838\n",
            "Iteration 8, loss = 2.25975379\n",
            "Iteration 9, loss = 2.25945419\n",
            "Iteration 10, loss = 2.25928408\n",
            "Iteration 11, loss = 2.25904661\n",
            "Iteration 12, loss = 2.25890075\n",
            "Iteration 13, loss = 2.25880676\n",
            "Iteration 14, loss = 2.25880850\n",
            "Iteration 15, loss = 2.25859366\n",
            "Iteration 16, loss = 2.25850538\n",
            "Iteration 17, loss = 2.25842963\n",
            "Iteration 18, loss = 2.25842176\n",
            "Iteration 19, loss = 2.25819525\n",
            "Iteration 20, loss = 2.25818709\n",
            "Iteration 21, loss = 2.25815933\n",
            "Iteration 22, loss = 2.25804724\n",
            "Iteration 23, loss = 2.25793483\n",
            "Iteration 24, loss = 2.25795102\n",
            "Iteration 25, loss = 2.25780146\n",
            "Iteration 26, loss = 2.25780684\n",
            "Iteration 27, loss = 2.25774372\n",
            "Iteration 28, loss = 2.25769379\n",
            "Iteration 29, loss = 2.25764866\n",
            "Iteration 30, loss = 2.25768966\n",
            "Iteration 31, loss = 2.25760489\n",
            "Iteration 32, loss = 2.25754652\n",
            "Iteration 33, loss = 2.25747119\n",
            "Iteration 34, loss = 2.25743255\n",
            "Iteration 35, loss = 2.25734646\n",
            "Iteration 36, loss = 2.25736624\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "[CV] END activation=relu, alpha=0, hidden_layer_sizes=(32, 32), learning_rate=adaptive, max_iter=100, solver=adam, validation_fraction=0.3, verbose=True; total time=  34.6s\n",
            "Iteration 1, loss = 2.27316486\n",
            "Iteration 2, loss = 2.26102309\n",
            "Iteration 3, loss = 2.25851546\n",
            "Iteration 4, loss = 2.25698777\n",
            "Iteration 5, loss = 2.25623921\n",
            "Iteration 6, loss = 2.25575642\n",
            "Iteration 7, loss = 2.25535635\n",
            "Iteration 8, loss = 2.25504042\n",
            "Iteration 9, loss = 2.25484261\n",
            "Iteration 10, loss = 2.25466809\n",
            "Iteration 11, loss = 2.25453946\n",
            "Iteration 12, loss = 2.25438964\n",
            "Iteration 13, loss = 2.25421793\n",
            "Iteration 14, loss = 2.25412842\n",
            "Iteration 15, loss = 2.25410674\n",
            "Iteration 16, loss = 2.25392997\n",
            "Iteration 17, loss = 2.25382950\n",
            "Iteration 18, loss = 2.25385463\n",
            "Iteration 19, loss = 2.25363097\n",
            "Iteration 20, loss = 2.25362732\n",
            "Iteration 21, loss = 2.25361205\n",
            "Iteration 22, loss = 2.25343165\n",
            "Iteration 23, loss = 2.25332956\n",
            "Iteration 24, loss = 2.25332059\n",
            "Iteration 25, loss = 2.25326957\n",
            "Iteration 26, loss = 2.25318747\n",
            "Iteration 27, loss = 2.25318798\n",
            "Iteration 28, loss = 2.25309971\n",
            "Iteration 29, loss = 2.25311037\n",
            "Iteration 30, loss = 2.25300159\n",
            "Iteration 31, loss = 2.25301384\n",
            "Iteration 32, loss = 2.25291463\n",
            "Iteration 33, loss = 2.25288997\n",
            "Iteration 34, loss = 2.25293655\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "[CV] END activation=relu, alpha=0, hidden_layer_sizes=(32, 32), learning_rate=adaptive, max_iter=100, solver=adam, validation_fraction=0.3, verbose=True; total time=  33.4s\n",
            "Iteration 1, loss = 2.27360888\n",
            "Iteration 2, loss = 2.26236061\n",
            "Iteration 3, loss = 2.25908633\n",
            "Iteration 4, loss = 2.25764380\n",
            "Iteration 5, loss = 2.25693058\n",
            "Iteration 6, loss = 2.25643419\n",
            "Iteration 7, loss = 2.25608657\n",
            "Iteration 8, loss = 2.25581971\n",
            "Iteration 9, loss = 2.25559541\n",
            "Iteration 10, loss = 2.25546710\n",
            "Iteration 11, loss = 2.25525125\n",
            "Iteration 12, loss = 2.25518946\n",
            "Iteration 13, loss = 2.25509364\n",
            "Iteration 14, loss = 2.25494785\n",
            "Iteration 15, loss = 2.25475688\n",
            "Iteration 16, loss = 2.25469351\n",
            "Iteration 17, loss = 2.25456416\n",
            "Iteration 18, loss = 2.25456012\n",
            "Iteration 19, loss = 2.25450262\n",
            "Iteration 20, loss = 2.25434990\n",
            "Iteration 21, loss = 2.25436416\n",
            "Iteration 22, loss = 2.25427339\n",
            "Iteration 23, loss = 2.25416933\n",
            "Iteration 24, loss = 2.25407800\n",
            "Iteration 25, loss = 2.25409561\n",
            "Iteration 26, loss = 2.25389742\n",
            "Iteration 27, loss = 2.25395539\n",
            "Iteration 28, loss = 2.25390623\n",
            "Iteration 29, loss = 2.25377796\n",
            "Iteration 30, loss = 2.25373276\n",
            "Iteration 31, loss = 2.25371708\n",
            "Iteration 32, loss = 2.25359630\n",
            "Iteration 33, loss = 2.25356322\n",
            "Iteration 34, loss = 2.25349443\n",
            "Iteration 35, loss = 2.25356662\n",
            "Iteration 36, loss = 2.25352935\n",
            "Iteration 37, loss = 2.25345729\n",
            "Iteration 38, loss = 2.25338448\n",
            "Iteration 39, loss = 2.25331346\n",
            "Iteration 40, loss = 2.25332795\n",
            "Iteration 41, loss = 2.25332660\n",
            "Iteration 42, loss = 2.25326315\n",
            "Iteration 43, loss = 2.25324700\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "[CV] END activation=relu, alpha=0, hidden_layer_sizes=(32, 32), learning_rate=adaptive, max_iter=100, solver=adam, validation_fraction=0.3, verbose=True; total time=  41.5s\n",
            "Iteration 1, loss = 2.27256256\n",
            "Iteration 2, loss = 2.26087270\n",
            "Iteration 3, loss = 2.25898832\n",
            "Iteration 4, loss = 2.25812500\n",
            "Iteration 5, loss = 2.25758122\n",
            "Iteration 6, loss = 2.25722859\n",
            "Iteration 7, loss = 2.25697543\n",
            "Iteration 8, loss = 2.25682747\n",
            "Iteration 9, loss = 2.25656757\n",
            "Iteration 10, loss = 2.25640984\n",
            "Iteration 11, loss = 2.25637233\n",
            "Iteration 12, loss = 2.25628852\n",
            "Iteration 13, loss = 2.25613276\n",
            "Iteration 14, loss = 2.25605018\n",
            "Iteration 15, loss = 2.25601462\n",
            "Iteration 16, loss = 2.25592783\n",
            "Iteration 17, loss = 2.25586022\n",
            "Iteration 18, loss = 2.25586208\n",
            "Iteration 19, loss = 2.25585988\n",
            "Iteration 20, loss = 2.25557895\n",
            "Iteration 21, loss = 2.25560610\n",
            "Iteration 22, loss = 2.25556763\n",
            "Iteration 23, loss = 2.25552640\n",
            "Iteration 24, loss = 2.25551884\n",
            "Iteration 25, loss = 2.25548885\n",
            "Iteration 26, loss = 2.25536010\n",
            "Iteration 27, loss = 2.25538023\n",
            "Iteration 28, loss = 2.25524485\n",
            "Iteration 29, loss = 2.25531859\n",
            "Iteration 30, loss = 2.25532554\n",
            "Iteration 31, loss = 2.25520924\n",
            "Iteration 32, loss = 2.25512413\n",
            "Iteration 33, loss = 2.25514259\n",
            "Iteration 34, loss = 2.25511952\n",
            "Iteration 35, loss = 2.25508014\n",
            "Iteration 36, loss = 2.25505544\n",
            "Iteration 37, loss = 2.25510472\n",
            "Iteration 38, loss = 2.25500097\n",
            "Iteration 39, loss = 2.25501220\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NotFittedError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNotFittedError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-25-cdfa5af9603d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mrs_clf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mRandomizedSearchCV\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmlp_rs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mparam_distributions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgrid\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mn_iter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mrs_clf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mclf2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mscore\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    456\u001b[0m         \"\"\"\n\u001b[1;32m    457\u001b[0m         \u001b[0m_check_refit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"score\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 458\u001b[0;31m         \u001b[0mcheck_is_fitted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    459\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscorer_\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    460\u001b[0m             raise ValueError(\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_is_fitted\u001b[0;34m(estimator, attributes, msg, all_or_any)\u001b[0m\n\u001b[1;32m   1220\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1221\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mfitted\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1222\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mNotFittedError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"name\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1223\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1224\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNotFittedError\u001b[0m: This GridSearchCV instance is not fitted yet. Call 'fit' with appropriate arguments before using this estimator."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "rs_clf.score(x_test,y_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "08UtxzRFd4dI",
        "outputId": "2159b527-e622-43d1-e7eb-9d5359720630"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.13466726394903192"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Grid Search CV\n",
        "\n"
      ],
      "metadata": {
        "id": "gIOSbu-BiCGX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "mlp_gs = MLPClassifier(max_iter=1)\n",
        "grid={\"hidden_layer_sizes\":[(32,32),(40,40),(48,48)],\"activation\":[\"relu\",\"tanh\"],\"solver\":['sgd','adam'],\"alpha\":[0],\"learning_rate\":['adaptive'],\"validation_fraction\":[0.3],\"max_iter\":[100],\"verbose\":[True]}\n",
        "clf2 = GridSearchCV(mlp_gs, grid, n_jobs=-1, cv=2)\n",
        "clf2.fit(x_train,y_train)\n",
        "clf2.score(x_test,y_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pK1vPijNGXDm",
        "outputId": "a759c6ea-ee15-4914-ee31-c404c4274495"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1, loss = 2.27363067\n",
            "Iteration 2, loss = 2.26144022\n",
            "Iteration 3, loss = 2.25937680\n",
            "Iteration 4, loss = 2.25836468\n",
            "Iteration 5, loss = 2.25784081\n",
            "Iteration 6, loss = 2.25743392\n",
            "Iteration 7, loss = 2.25700660\n",
            "Iteration 8, loss = 2.25700767\n",
            "Iteration 9, loss = 2.25676842\n",
            "Iteration 10, loss = 2.25666318\n",
            "Iteration 11, loss = 2.25651898\n",
            "Iteration 12, loss = 2.25630529\n",
            "Iteration 13, loss = 2.25625785\n",
            "Iteration 14, loss = 2.25623332\n",
            "Iteration 15, loss = 2.25609292\n",
            "Iteration 16, loss = 2.25598690\n",
            "Iteration 17, loss = 2.25584881\n",
            "Iteration 18, loss = 2.25577038\n",
            "Iteration 19, loss = 2.25578871\n",
            "Iteration 20, loss = 2.25566355\n",
            "Iteration 21, loss = 2.25560701\n",
            "Iteration 22, loss = 2.25551303\n",
            "Iteration 23, loss = 2.25549243\n",
            "Iteration 24, loss = 2.25541455\n",
            "Iteration 25, loss = 2.25529706\n",
            "Iteration 26, loss = 2.25531677\n",
            "Iteration 27, loss = 2.25524573\n",
            "Iteration 28, loss = 2.25516874\n",
            "Iteration 29, loss = 2.25522671\n",
            "Iteration 30, loss = 2.25510992\n",
            "Iteration 31, loss = 2.25504137\n",
            "Iteration 32, loss = 2.25506542\n",
            "Iteration 33, loss = 2.25507942\n",
            "Iteration 34, loss = 2.25490557\n",
            "Iteration 35, loss = 2.25494777\n",
            "Iteration 36, loss = 2.25483689\n",
            "Iteration 37, loss = 2.25490782\n",
            "Iteration 38, loss = 2.25486298\n",
            "Iteration 39, loss = 2.25484289\n",
            "Iteration 40, loss = 2.25472417\n",
            "Iteration 41, loss = 2.25471849\n",
            "Iteration 42, loss = 2.25470334\n",
            "Iteration 43, loss = 2.25468938\n",
            "Iteration 44, loss = 2.25459553\n",
            "Iteration 45, loss = 2.25464091\n",
            "Iteration 46, loss = 2.25460512\n",
            "Iteration 47, loss = 2.25453053\n",
            "Iteration 48, loss = 2.25454381\n",
            "Iteration 49, loss = 2.25453696\n",
            "Iteration 50, loss = 2.25444042\n",
            "Iteration 51, loss = 2.25447703\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.1357125080881987"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "\n",
        "fpath = 'clf_nn.pickle' # file name\n",
        "\n",
        "# Save\n",
        "pickle.dump(clf, open(fpath, 'wb'))\n",
        "\n",
        "# Load\n",
        "clf = pickle.load(open(fpath, 'rb'))"
      ],
      "metadata": {
        "id": "ENqOUorFM3T4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test=after_2010\n",
        "x_cols = ['ret','mom3m','mom6m','mom12m']"
      ],
      "metadata": {
        "id": "283VoHdnM_1x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 'H' and 'L' portfolio returns\n",
        "r_h_ew = []; r_l_ew = []  # equal-weight \n",
        "r_h_vw = []; r_l_vw = []  # value-weight\n",
        "\n",
        "# predict in each month during the test period\n",
        "for m, test_m in test.groupby('date'):\n",
        "    x_test_m = test_m[x_cols].values\n",
        "    \n",
        "    # Predict the return class\n",
        "    y_pred = clf.predict(x_test_m)\n",
        "    hidx = y_pred==0 # highest return class\n",
        "    lidx = y_pred==9 # lowest return class\n",
        "    \n",
        "    rh = test_m.loc[hidx, 'tgt_ret'] # returns of class 'H'\n",
        "    rl = test_m.loc[lidx, 'tgt_ret'] # returns of class 'L'\n",
        "    \n",
        "    print(f'{m} Number of stocks in H and L: {len(rh)}, {len(rl)}')\n",
        "\n",
        "    # equal-weight\n",
        "    # portfolio weights\n",
        "    wh_ew = (1/len(rh) if len(rh) else 0) * np.ones_like(rh)\n",
        "    wl_ew = (1/len(rl) if len(rl) else 0) * np.ones_like(rl)\n",
        "    \n",
        "    # portfolio return\n",
        "    r_h_ew.append(np.matmul(wh_ew, rh))\n",
        "    r_l_ew.append(np.matmul(wl_ew, rl))\n",
        "\n",
        "    # value-weight\n",
        "    # portfolio weights\n",
        "    sizeh = np.exp(test_m.loc[hidx, 'size'])\n",
        "    sizel = np.exp(test_m.loc[lidx, 'size'])\n",
        "    wh_vw = sizeh/np.sum(sizeh)\n",
        "    wl_vw = sizel/np.sum(sizel)\n",
        "\n",
        "    # portfolio return\n",
        "    r_h_vw.append(np.matmul(wh_vw, rh))\n",
        "    r_l_vw.append(np.matmul(wl_vw, rl))\n",
        "    \n",
        "\n",
        "r_h_ew = np.array(r_h_ew)\n",
        "r_l_ew = np.array(r_l_ew)\n",
        "# long-short portfolio return\n",
        "r_hl_ew = r_h_ew - r_l_ew \n",
        "\n",
        "r_h_vw = np.array(r_h_vw)\n",
        "r_l_vw = np.array(r_l_vw)\n",
        "# long-short portfolio return\n",
        "r_hl_vw = r_h_vw - r_l_vw\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IHbzwFllNCY5",
        "outputId": "6c70588b-5360-44a8-a05b-ad5a6deb5d25"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2010-01-29 Number of stocks in H and L: 163, 168\n",
            "2010-02-26 Number of stocks in H and L: 175, 209\n",
            "2010-03-31 Number of stocks in H and L: 178, 194\n",
            "2010-04-30 Number of stocks in H and L: 216, 117\n",
            "2010-05-28 Number of stocks in H and L: 250, 170\n",
            "2010-06-30 Number of stocks in H and L: 188, 197\n",
            "2010-07-30 Number of stocks in H and L: 63, 245\n",
            "2010-08-31 Number of stocks in H and L: 121, 244\n",
            "2010-09-30 Number of stocks in H and L: 61, 240\n",
            "2010-10-29 Number of stocks in H and L: 141, 176\n",
            "2010-11-30 Number of stocks in H and L: 225, 190\n",
            "2010-12-31 Number of stocks in H and L: 210, 165\n",
            "2011-01-31 Number of stocks in H and L: 167, 157\n",
            "2011-02-28 Number of stocks in H and L: 202, 178\n",
            "2011-03-31 Number of stocks in H and L: 144, 192\n",
            "2011-04-29 Number of stocks in H and L: 126, 198\n",
            "2011-05-31 Number of stocks in H and L: 126, 189\n",
            "2011-06-30 Number of stocks in H and L: 166, 173\n",
            "2011-07-29 Number of stocks in H and L: 140, 207\n",
            "2011-08-31 Number of stocks in H and L: 295, 167\n",
            "2011-09-30 Number of stocks in H and L: 488, 251\n",
            "2011-10-31 Number of stocks in H and L: 88, 379\n",
            "2011-11-30 Number of stocks in H and L: 165, 314\n",
            "2011-12-30 Number of stocks in H and L: 109, 519\n",
            "2012-01-31 Number of stocks in H and L: 49, 349\n",
            "2012-02-29 Number of stocks in H and L: 76, 369\n",
            "2012-03-30 Number of stocks in H and L: 190, 332\n",
            "2012-04-30 Number of stocks in H and L: 120, 342\n",
            "2012-05-31 Number of stocks in H and L: 109, 380\n",
            "2012-06-29 Number of stocks in H and L: 60, 333\n",
            "2012-07-31 Number of stocks in H and L: 76, 313\n",
            "2012-08-31 Number of stocks in H and L: 79, 240\n",
            "2012-09-28 Number of stocks in H and L: 88, 157\n",
            "2012-10-31 Number of stocks in H and L: 106, 221\n",
            "2012-11-30 Number of stocks in H and L: 73, 188\n",
            "2012-12-31 Number of stocks in H and L: 68, 167\n",
            "2013-01-31 Number of stocks in H and L: 84, 174\n",
            "2013-02-28 Number of stocks in H and L: 84, 199\n",
            "2013-03-28 Number of stocks in H and L: 107, 187\n",
            "2013-04-30 Number of stocks in H and L: 82, 170\n",
            "2013-05-31 Number of stocks in H and L: 103, 125\n",
            "2013-06-28 Number of stocks in H and L: 131, 131\n",
            "2013-07-31 Number of stocks in H and L: 100, 102\n",
            "2013-08-30 Number of stocks in H and L: 97, 100\n",
            "2013-09-30 Number of stocks in H and L: 92, 132\n",
            "2013-10-31 Number of stocks in H and L: 111, 108\n",
            "2013-11-29 Number of stocks in H and L: 135, 101\n",
            "2013-12-31 Number of stocks in H and L: 116, 99\n",
            "2014-01-31 Number of stocks in H and L: 110, 99\n",
            "2014-02-28 Number of stocks in H and L: 93, 96\n",
            "2014-03-31 Number of stocks in H and L: 99, 84\n",
            "2014-04-30 Number of stocks in H and L: 78, 87\n",
            "2014-05-30 Number of stocks in H and L: 66, 106\n",
            "2014-06-30 Number of stocks in H and L: 47, 93\n",
            "2014-07-31 Number of stocks in H and L: 64, 109\n",
            "2014-08-29 Number of stocks in H and L: 56, 108\n",
            "2014-09-30 Number of stocks in H and L: 85, 137\n",
            "2014-10-31 Number of stocks in H and L: 89, 145\n",
            "2014-11-28 Number of stocks in H and L: 103, 153\n",
            "2014-12-31 Number of stocks in H and L: 118, 171\n",
            "2015-01-30 Number of stocks in H and L: 102, 198\n",
            "2015-02-27 Number of stocks in H and L: 44, 232\n",
            "2015-03-31 Number of stocks in H and L: 59, 230\n",
            "2015-04-30 Number of stocks in H and L: 51, 181\n",
            "2015-05-29 Number of stocks in H and L: 54, 192\n",
            "2015-06-30 Number of stocks in H and L: 45, 198\n",
            "2015-07-31 Number of stocks in H and L: 76, 224\n",
            "2015-08-31 Number of stocks in H and L: 71, 267\n",
            "2015-09-30 Number of stocks in H and L: 113, 232\n",
            "2015-10-30 Number of stocks in H and L: 56, 273\n",
            "2015-11-30 Number of stocks in H and L: 64, 222\n",
            "2015-12-31 Number of stocks in H and L: 84, 270\n",
            "2016-01-29 Number of stocks in H and L: 123, 278\n",
            "2016-02-29 Number of stocks in H and L: 68, 309\n",
            "2016-03-31 Number of stocks in H and L: 41, 265\n",
            "2016-04-29 Number of stocks in H and L: 44, 268\n",
            "2016-05-31 Number of stocks in H and L: 82, 261\n",
            "2016-06-30 Number of stocks in H and L: 44, 240\n",
            "2016-07-29 Number of stocks in H and L: 32, 186\n",
            "2016-08-31 Number of stocks in H and L: 54, 164\n",
            "2016-09-30 Number of stocks in H and L: 81, 124\n",
            "2016-10-31 Number of stocks in H and L: 67, 152\n",
            "2016-11-30 Number of stocks in H and L: 37, 135\n",
            "2016-12-30 Number of stocks in H and L: 71, 116\n",
            "2017-01-31 Number of stocks in H and L: 84, 98\n",
            "2017-02-28 Number of stocks in H and L: 55, 85\n",
            "2017-03-31 Number of stocks in H and L: 59, 83\n",
            "2017-04-28 Number of stocks in H and L: 60, 79\n",
            "2017-05-31 Number of stocks in H and L: 75, 84\n",
            "2017-06-30 Number of stocks in H and L: 60, 82\n",
            "2017-07-31 Number of stocks in H and L: 57, 86\n",
            "2017-08-31 Number of stocks in H and L: 59, 116\n",
            "2017-09-29 Number of stocks in H and L: 35, 104\n",
            "2017-10-31 Number of stocks in H and L: 68, 93\n",
            "2017-11-30 Number of stocks in H and L: 64, 100\n",
            "2017-12-29 Number of stocks in H and L: 48, 99\n",
            "2018-01-31 Number of stocks in H and L: 54, 110\n",
            "2018-02-28 Number of stocks in H and L: 56, 123\n",
            "2018-03-29 Number of stocks in H and L: 47, 112\n",
            "2018-04-30 Number of stocks in H and L: 52, 115\n",
            "2018-05-31 Number of stocks in H and L: 57, 96\n",
            "2018-06-29 Number of stocks in H and L: 71, 90\n",
            "2018-07-31 Number of stocks in H and L: 68, 82\n",
            "2018-08-31 Number of stocks in H and L: 49, 93\n",
            "2018-09-28 Number of stocks in H and L: 50, 85\n",
            "2018-10-31 Number of stocks in H and L: 76, 144\n",
            "2018-11-30 Number of stocks in H and L: 53, 128\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_preds(y_true,y_preds):\n",
        "  accuracy=accuracy_score(y_true,y_preds)\n",
        "  precision=precision_score(y_true,y_preds)\n",
        "  recall=recall_score(y_true,y_preds)\n",
        "  f1=f1_score(y_true,y_preds)\n",
        "  metric_dict={\"accuracy\":round(accuracy,2),\"precision\":round(precision,2),\"recall\":round(recall,2),\"f1\":round(f1,2)}\n",
        "  print(f'Acc:{accuracy*100:.2f}%')\n",
        "  print(f'Precision:{precision:.2f}%')\n",
        "  print(f'Recall:{recall:.2f}%')\n",
        "  print(f'F1 score:{f1:.2f}%')\n",
        "  return metric_dict"
      ],
      "metadata": {
        "id": "lG5RoFVELSn5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(r_p):\n",
        "    cum_r = (1 + r_p).prod()-1\n",
        "    mean_r = 12 * np.mean(r_p)\n",
        "    std_r = np.sqrt(12) * np.std(r_p)\n",
        "    sharpe =  mean_r / std_r\n",
        "\n",
        "    return [cum_r, mean_r, std_r, sharpe]"
      ],
      "metadata": {
        "id": "SaxVTwBtQgWN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score,precision_score,recall_score,f1_score\n",
        "def evaluate_preds(y_true,y_preds):\n",
        "  accuracy=accuracy_score(y_true,y_preds)\n",
        "  precision=precision_score(y_true,y_preds)\n",
        "  recall=recall_score(y_true,y_preds)\n",
        "  f1=f1_score(y_true,y_preds)\n",
        "  metric_dict={\"accuracy\":round(accuracy,2),\"precision\":round(precision,2),\"recall\":round(recall,2),\"f1\":round(f1,2)}\n",
        "  print(f'Acc:{accuracy*100:.2f}%')\n",
        "  print(f'Precision:{precision:.2f}%')\n",
        "  print(f'Recall:{recall:.2f}%')\n",
        "  print(f'F1 score:{f1:.2f}%')\n",
        "  return metric_dict"
      ],
      "metadata": {
        "id": "b7jGoap0SB7n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## Result\n",
        "*   MLP(baseline model) performed better than Random Forest Classifier\n",
        "*   However, after performing hyper parameter tuning by Randomized Search CV and Grid Search CV, both models' accuracy score was almost similar. \n",
        "*  Result tells that with a proper hyper parameter tuning, Random Forest Classifier might perform similar as MLP\n",
        "\n"
      ],
      "metadata": {
        "id": "HtC1Qw-9ivQD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# dataframe to store evaluation results\n",
        "pftab = pd.DataFrame(0, index=['cum', 'mean', 'std', 'Sharpe'], columns=[])\n",
        "\n",
        "pftab['EW_H'] = evaluate(r_h_ew)\n",
        "pftab['EW_L'] = evaluate(r_l_ew)\n",
        "pftab['EW_HL'] = evaluate(r_hl_ew)\n",
        "\n",
        "pftab['VW_H'] = evaluate(r_h_vw)\n",
        "pftab['VW_L'] = evaluate(r_l_vw)\n",
        "pftab['VW_HL'] = evaluate(r_hl_vw)\n",
        "\n",
        "pftab"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 175
        },
        "id": "vjMGtVJlQlov",
        "outputId": "5c5141ab-fe79-47ca-9e89-d4096eeb3c88"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "            EW_H      EW_L     EW_HL      VW_H      VW_L     VW_HL\n",
              "cum     0.099846 -0.772335  3.403226  3.357052 -0.093710  2.768752\n",
              "mean    0.032842 -0.141019  0.173862  0.202523  0.025667  0.176856\n",
              "std     0.210209  0.215748  0.114444  0.271873  0.269887  0.236372\n",
              "Sharpe  0.156236 -0.653632  1.519182  0.744916  0.095103  0.748210"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-7a2f1725-e66c-4d1c-b055-457f6c448632\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>EW_H</th>\n",
              "      <th>EW_L</th>\n",
              "      <th>EW_HL</th>\n",
              "      <th>VW_H</th>\n",
              "      <th>VW_L</th>\n",
              "      <th>VW_HL</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>cum</th>\n",
              "      <td>0.099846</td>\n",
              "      <td>-0.772335</td>\n",
              "      <td>3.403226</td>\n",
              "      <td>3.357052</td>\n",
              "      <td>-0.093710</td>\n",
              "      <td>2.768752</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>0.032842</td>\n",
              "      <td>-0.141019</td>\n",
              "      <td>0.173862</td>\n",
              "      <td>0.202523</td>\n",
              "      <td>0.025667</td>\n",
              "      <td>0.176856</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>0.210209</td>\n",
              "      <td>0.215748</td>\n",
              "      <td>0.114444</td>\n",
              "      <td>0.271873</td>\n",
              "      <td>0.269887</td>\n",
              "      <td>0.236372</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Sharpe</th>\n",
              "      <td>0.156236</td>\n",
              "      <td>-0.653632</td>\n",
              "      <td>1.519182</td>\n",
              "      <td>0.744916</td>\n",
              "      <td>0.095103</td>\n",
              "      <td>0.748210</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-7a2f1725-e66c-4d1c-b055-457f6c448632')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-7a2f1725-e66c-4d1c-b055-457f6c448632 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-7a2f1725-e66c-4d1c-b055-457f6c448632');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cumret = pd.DataFrame(0, index=test.date.unique(), columns=[])\n",
        "cumret['H'] = (1 + r_h_ew).cumprod()\n",
        "cumret['L'] = (1 + r_l_ew).cumprod()\n",
        "cumret['H-L'] = (1 + r_hl_ew).cumprod()\n",
        "\n",
        "cumret.plot(title='Equal-Weight')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 299
        },
        "id": "CCBSq470QqxY",
        "outputId": "81a09e71-91bc-4f34-dc00-03d57d115033"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7f25db31e1d0>"
            ]
          },
          "metadata": {},
          "execution_count": 44
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cumret = pd.DataFrame(0, index=test.date.unique(), columns=[])\n",
        "cumret['H'] = (1 + r_h_vw).cumprod()\n",
        "cumret['L'] = (1 + r_l_vw).cumprod()\n",
        "cumret['H-L'] = (1 + r_hl_vw).cumprod()\n",
        "\n",
        "cumret.plot(title='Value-Weight')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 299
        },
        "id": "eMQaJzHWQtJX",
        "outputId": "a8223970-a962-4909-c9ae-9545d4fd1f8a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7f25db290710>"
            ]
          },
          "metadata": {},
          "execution_count": 45
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}